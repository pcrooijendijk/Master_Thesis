from ragas.metrics import RougeScore, BleuScore
from ragas import SingleTurnSample

test_data = {
    "user_input": "summarise given text\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.",
    "response": "The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.",
    "reference": "The company reported an 8% growth in Q3 2024, primarily driven by strong sales in the Asian market, attributed to strategic marketing and localized products, with continued growth anticipated in the next quarter."
}

question_list = [{'question': 'What is the top-1 accuracy of the Oracle KGLM on birthdate prediction?', 'ground_truth': 'The top-1 accuracy of the Oracle KGLM on birthdate prediction is 65%.', 'answer': 'Please refer to the provided context for the answer.', 'contexts': ['Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 465–476July 5 - 10, 2020. c⃝2020 Association for Computational Linguistics465Dice Loss for Data-imbalanced NLP TasksXiaoya Li♣, Xiaofei Sun♣, Yuxian Meng♣, Junjun Liang♣, Fei Wu♠and Jiwei Li♠♣♠Department of Computer Science and Technology, Zhejiang University♣Shannon.AI{xiaoya li, xiaofei sun, yuxian meng, jiwei li}@shannonai.com, wufei@cs.zju.edu.cnAbstractMany NLP tasks such as tagging and ma-chine reading comprehension (MRC) are facedwith the severe data imbalance issue: nega-tive examples signiﬁcantly outnumber positiveones, and the huge number of easy-negativeexamples overwhelms training.The mostcommonly used cross entropy criteria is ac-tually accuracy-oriented, which creates a dis-crepancy between training and test. At train-ing time, each training instance contributesequally to the objective function, while at testtime F1 score concerns more about positive ex-amples.In this paper, we propose to use dice loss inreplacement of the standard cross-entropy ob-jective for data-imbalanced NLP tasks. Diceloss is based on the Sørensen–Dice coefﬁcient(Sorensen, 1948) or Tversky index (Tversky,1977), which attaches similar importance tofalse positives and false negatives, and is moreimmune to the data-imbalance issue. To fur-ther alleviate the dominating inﬂuence fromeasy-negative examples in training, we pro-pose to associate training examples with dy-namically adjusted weights to deemphasizeeasy-negative examples. Experimental resultsshow that this strategy narrows down the gapbetween the F1 score in evaluation and the diceloss in training.With the proposed training objective, we ob-serve signiﬁcant performance boosts over awide range of data imbalanced NLP tasks. No-tably, we are able to achieve SOTA resultson CTB5, CTB6 and UD1.4 for the part ofspeech tagging task, and competitive or evenbetter results on CoNLL03, OntoNotes5.0,MSRA and OntoNotes4.0 for the named en-tity recognition task along with the machinereading comprehension and paraphrase iden-tiﬁcation tasks.The code can be foundat https://github.com/ShannonAI/dice_loss_for_NLP.Task# neg# posratioCoNLL03 NER170K34K4.98OntoNotes5.0 NER1.96M239K8.18SQuAD 1.1 (Rajpurkar et al., 2016)10.3M175K55.9SQuAD 2.0 (Rajpurkar et al., 2018)15.4M188K82.0QUOREF (Dasigi et al., 2019)6.52M38.6K169Table 1: Number of positive and negative examples andtheir ratios for different data-imbalanced NLP tasks.1IntroductionData imbalance is a common issue in a varietyof NLP tasks such as tagging and machine read-ing comprehension. Table 1 gives concrete exam-ples: for the Named Entity Recognition (NER)task (Sang and De Meulder, 2003; Nadeau andSekine, 2007), most tokens are backgrounds withtagging class O. Speciﬁcally, the number of to-kens with tagging class O is 5 times as many asthose with entity labels for the CoNLL03 datasetand 8 times for the OntoNotes5.0 dataset; Data-imbalanced issue is more severe for MRC tasks(Rajpurkar et al., 2016; Nguyen et al., 2016; Ra-jpurkar et al., 2018; Koˇcisk`y et al., 2018; Dasigiet al., 2019) with the value of negative-positive ra-tio being 50-200, which is due to the reason thatthe task of MRC is usually formalized as predictingthe starting and ending indexes conditioned on thequery and the context, and given a chunk of text ofan arbitrary length, only two tokens are positive (orof interest) with all the rest being background.Data imbalance results in the following two issues:(1) the training-test discrepancy: Without bal-ancing the labels, the learning process tends to con-verge to a point that strongly biases towards classwith the majority label. This actually creates a dis-crepancy between training and test: at training time,each training instance contributes equally to theobjective function, whereas at test time, F1 givesequal weight to positive and negative examples; (2)the overwhelming effect of easy-negative exam-ples. As pointed out by Meng et al. (2019), a sig-niﬁcantly large number of negative examples also\x0c466means that the number of easy-negative exampleis large. The huge number of easy examples tendsto overwhelm the training, making the model notsufﬁciently learn to distinguish between positiveexamples and hard-negative examples. The cross-entropy objective (CE for short) or maximum like-lihood (MLE) objective, which is widely adoptedas the training objective for data-imbalanced NLPtasks (Lample et al., 2016; Wu et al., 2019; Devlinet al., 2018; Yu et al., 2018a; McCann et al., 2018;Ma and Hovy, 2016; Chen et al., 2017), handlesneither of the issues.To handle the ﬁrst issue, we propose to replace CEor MLE with losses based on the Sørensen–Dice co-efﬁcient (Sorensen, 1948) or Tversky index (Tver-sky, 1977). The Sørensen–Dice coefﬁcient, diceloss for short, is the harmonic mean of precisionand recall. It attaches equal importance to false pos-itives (FPs) and false negatives (FNs) and is thusmore immune to data-imbalanced datasets. Tver-sky index extends dice loss by using a weight thattrades precision and recall, which can be thought asthe approximation of the Fβ score, and thus comeswith more ﬂexibility. Therefore, we use dice lossor Tversky index to replace CE loss to address theﬁrst issue.Only using dice loss or Tversky index is not enoughsince they are unable to address the dominatinginﬂuence of easy-negative examples. This is intrin-sically because dice loss is actually a soft versionof the F1 score. Taking the binary classiﬁcationtask as an example, at test time, an example willbe classiﬁed as negative as long as its probabilityis smaller than 0.5, but training will push the valueto 0 as much as possible. This gap isn’t a big issuefor balanced datasets, but is extremely detrimentalif a big proportion of training examples are easy-negative ones: easy-negative examples can easilydominate training since their probabilities can bepushed to 0 fairly easily. Meanwhile, the model canhardly distinguish between hard-negative examplesand positive ones. Inspired by the idea of focalloss (Lin et al., 2017) in computer vision, we pro-pose a dynamic weight adjusting strategy, whichassociates each training example with a weight inproportion to (1 −p), and this weight dynamicallychanges as training proceeds. This strategy helpsdeemphasize conﬁdent examples during training astheir probability p approaches 1, making the modelattentive to hard-negative examples, and thus alle-viates the dominating effect of easy-negative exam-ples. Combing both strategies, we observe signif-icant performance boosts on a wide range of dataimbalanced NLP tasks.The rest of this paper is organized as follows: re-lated work is presented in Section 2. We describedifferent proposed losses in Section 3. Experimen-tal results are presented in Section 4. We performablation studies in Section 5, followed by a briefconclusion in Section 6.2Related Work2.1Data ResamplingThe idea of weighting training examples has along history. Importance sampling (Kahn and Mar-shall, 1953) assigns weights to different samplesand changes the data distribution. Boosting algo-rithms such as AdaBoost (Kanduri et al., 2018)select harder examples to train subsequent classi-ﬁers. Similarly, hard example mining (Malisiewiczet al., 2011) downsamples the majority class andexploits the most difﬁcult examples. Oversampling(Chen et al., 2010; Chawla et al., 2002) is used tobalance the data distribution. Another line of dataresampling is to dynamically control the weights ofexamples as training proceeds. For example, focalloss (Lin et al., 2017) used a soft weighting schemethat emphasizes harder examples during training.In self-paced learning (Kumar et al., 2010), exam-ple weights are obtained through optimizing theweighted training loss which encourages learningeasier examples ﬁrst. At each training step, self-paced learning algorithm optimizes model param-eters and example weights jointly. Other works(Chang et al., 2017; Katharopoulos and Fleuret,2018) adjusted the weights of different training ex-amples based on training loss. Besides, recent work(Jiang et al., 2017; Fan et al., 2018) proposed tolearn a separate network to predict sample weights.2.2Data Imbalance Issue in ComputerVisionThe background-object label imbalance issue is se-vere and thus well studied in the ﬁeld of objectdetection (Li et al., 2015; Girshick, 2015; He et al.,2015; Girshick et al., 2013; Ren et al., 2015). Theidea of hard negative mining (HNM) (Girshicket al., 2013) has gained much attention recently.Pang et al. (2019) proposed a novel method calledIoU-balanced sampling and Chen et al. (2019) de-signed a ranking model to replace the conventionalclassiﬁcation task with an average-precision loss\x0c467to alleviate the class imbalance issue. The effortsmade on object detection have greatly inspired usto solve the data imbalance issue in NLP.Sudre et al. (2017) addressed the severe class imbal-ance issue for the image segmentation task. Theyproposed to use the class re-balancing propertyof the Generalized Dice Loss as the training ob-jective for unbalanced tasks. Shen et al. (2018)investigated the inﬂuence of Dice-based loss formulti-class organ segmentation using a dataset ofabdominal CT volumes. Kodym et al. (2018) pro-posed to use the batch soft Dice loss function totrain the CNN network for the task of segmentationof organs at risk (OAR) of medical images. Shamiret al. (2019) extended the deﬁnition of the classicalDice coefﬁcient to facilitate the direct comparisonof a ground truth binary image with a probabilis-tic map. In this paper, we introduce dice loss intoNLP tasks as the training objective and propose adynamic weight adjusting strategy to address thedominating inﬂuence of easy-negative examples.3Losses3.1NotationFor illustration purposes, we use the binary clas-siﬁcation task to demonstrate how different losseswork. The mechanism can be easily extended tomulti-class classiﬁcation. Let X denote a set oftraining instances and each instance xi ∈X is as-sociated with a golden binary label yi = [yi0, yi1]denoting the ground-truth class xi belongs to,and pi = [pi0, pi1] is the predicted probabilitiesof the two classes respectively, where yi0, yi1 ∈{0, 1}, pi0, pi1 ∈[0, 1] and pi1 + pi0 = 1.3.2Cross Entropy LossThe vanilla cross entropy (CE) loss is given by:CE = −1NXiXj∈{0,1}yij log pij(1)As can be seen from Eq.1, each xi contributesequally to the ﬁnal objective. Two strategies arenormally used to address the the case where wewish that not all xi are treated equally: associatingdifferent classes with different weighting factor αor resampling the datasets. For the former, Eq.1 isadjusted as follows:Weighted CE = −1NXiαiXj∈{0,1}yij log pij(2)where αi ∈[0, 1] may be set by the inverse classfrequency or treated as a hyperparameter to set bycross validation. In this work, we use lg(n−ntnt +K)to calculate the coefﬁcient α, where nt is the num-ber of samples with class t and n is the total numberof samples in the training set. K is a hyperparam-eter to tune. Intuitively, this equation assigns lessweight to the majority class and more weight to theminority class. The data resampling strategy con-structs a new dataset by sampling training examplesfrom the original dataset based on human-designedcriteria, e.g. extracting equal training samples fromeach class. Both strategies are equivalent to chang-ing the data distribution during training and thus areof the same nature. Empirically, these two meth-ods are not widely used due to the trickiness ofselecting α especially for multi-class classiﬁcationtasks and that inappropriate selection can easilybias towards rare classes (Valverde et al., 2017).3.3Dice Coefﬁcient and Tversky IndexSørensen–Dice coefﬁcient (Sorensen, 1948; Dice,1945), dice coefﬁcient (DSC) for short, is an F1-oriented statistic used to gauge the similarity oftwo sets. Given two sets A and B, the vanilla dicecoefﬁcient between them is given as follows:DSC(A, B) = 2|A ∩B||A| + |B|(3)In our case, A is the set that contains all positiveexamples predicted by a speciﬁc model, and B isthe set of all golden positive examples in the dataset.When applied to boolean data with the deﬁnitionof true positive (TP), false positive (FP), and falsenegative (FN), it can be then written as follows:DSC =2TP2TP + FN + FP =2TPTP+FNTPTP+FPTPTP+FN +TPTP+FP= 2Pre × RecPre+Rec= F1(4)For an individual example xi, its correspondingdice coefﬁcient is given as follows:DSC(xi) = 2pi1yi1pi1 + yi1(5)As can be seen, a negative example (yi1 = 0) doesnot contribute to the objective. For smoothing pur-poses, it is common to add a γ factor to both thenominator and the denominator, making the formto be as follows (we simply set γ = 1 in the rest of\x0c468LossFormula (one sample xi)CE−Pj∈{0,1} yij log pijWCE−αiPj∈{0,1} yij log pijDL1 −2pi1yi1+γp2i1+y2i1+γTL1 −pi1yi1+γpi1yi1+α pi1yi0+β pi0yi1+γDSC1 −2(1−pi1)pi1·yi1+γ(1−pi1)pi1+yi1+γFL−αiPj∈{0,1}(1 −pij)γ log pijTable 2: Different losses and their formulas. We add +1to DL, TL and DSC so that they are positive.this paper):DSC(xi) = 2pi1yi1 + γpi1 + yi1 + γ(6)As can be seen, negative examples whose DSC isγpi1+γ , also contribute to the training. Addition-ally, Milletari et al. (2016) proposed to change thedenominator to the square form for faster conver-gence, which leads to the following dice loss (DL):DL = 1NXi\x141 −2pi1yi1 + γp2i1 + y2i1 + γ\x15(7)Another version of DL is to directly compute set-level dice coefﬁcient instead of the sum of individ-ual dice coefﬁcient, which is easier for optimiza-tion:DL = 1 −2 Pi pi1yi1 + γPi p2i1 + Pi y2i1 + γ(8)Tversky index (TI), which can be thought as theapproximation of the Fβ score, extends dice coefﬁ-cient to a more general case. Given two sets A andB, tversky index is computed as follows:TI =|A ∩B||A ∩B| + α|A\\B| + β|B\\A|(9)Tversky index offers the ﬂexibility in controllingthe tradeoff between false-negatives and false-positives. It degenerates to DSC if α = β = 0.5.The Tversky loss (TL) is thus given as follows:TL = 1NXi\x141 −pi1yi1 + γpi1yi1 + α pi1yi0 + β pi0yi1 + γ\x15(10)3.4Self-adjusting Dice LossConsider a simple case where the dataset consistsof only one example xi, which is classiﬁed as posi-tive as long as pi1 is larger than 0.5. The computa-tion of F1 score is actually as follows:F1(xi) = 2I(pi1 > 0.5)yi1I(pi1 > 0.5) + yi1(11)00.10.20.30.40.50.60.70.80.91-2-1.5-1-0.500.511.52Derivatives FL( =1) DL( =1) TL( =0.5) DSCFigure 1: An illustration of derivatives of the fourlosses. The derivative of DSC approaches zero rightafter p exceeds 0.5, and for the other losses, the deriva-tives reach 0 only if the probability is exactly 1, whichmeans they will push p to 1 as much as possible.Comparing Eq.5 with Eq.11, we can see that Eq.5is actually a soft form of F1, using a continuous prather than the binary I(pi1 > 0.5). This gap isn’ta big issue for balanced datasets, but is extremelydetrimental if a big proportion of training examplesare easy-negative ones: easy-negative examplescan easily dominate training since their probabil-ities can be pushed to 0 fairly easily. Meanwhile,the model can hardly distinguish between hard-negative examples and positive ones, which has ahuge negative effect on the ﬁnal F1 performance.To address this issue, we propose to multiply thesoft probability p with a decaying factor (1 −p),changing Eq.11 to the following adaptive variantof DSC:DSC(xi) = 2(1 −pi1)pi1 · yi1 + γ(1 −pi1)pi1 + yi1 + γ(12)One can think (1−pi1) as a weight associated witheach example, which changes as training proceeds.The intuition of changing pi1 to (1 −pi1)pi1 is topush down the weight of easy examples. For easyexamples whose probability are approaching 0 or1, (1−pi1)pi1 makes the model attach signiﬁcantlyless focus to them.A close look at Eq.12 reveals that it actually mim-ics the idea of focal loss (FL for short) (Lin et al.,2017) for object detection in vision. Focal losswas proposed for one-stage object detector to han-dle foreground-background tradeoff encounteredduring training. It down-weights the loss assignedto well-classiﬁed examples by adding a (1 −p)γfactor, leading the ﬁnal loss to be −(1 −p)γ log p.\x0c469CTB5CTB6UD1.4ModelPrec.Rec.F1Prec.Rec.F1Prec.Rec.F1Joint-POS(Sig)(Shao et al., 2017)93.68 94.4794.07--90.8189.28 89.5489.41Joint-POS(Ens)(Shao et al., 2017)93.95 94.8194.38---89.67 89.8689.75Lattice-LSTM(Zhang and Yang, 2018)94.77 95.5195.1492.00 90.8691.4390.47 89.7090.09BERT-Tagger(Devlin et al., 2018)95.86 96.2696.0694.91 94.6394.7795.42 94.1794.79BERT+FL96.11 97.4296.7695.80 95.0895.4496.33 95.8596.81(+0.70)(+0.67)(+2.02)BERT+DL96.77 98.8797.8194.08 96.1295.0996.10 97.7996.94(+1.75)(+0.32)(+2.15)BERT+DSC97.10 98.7597.9296.29 96.8596.5796.24 97.7396.98(+1.86)(+1.80)(+2.19)Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4.English WSJModelPrec.Rec.F1Meta BiLSTM(Bohnet et al., 2018)--98.23BERT-Tagger (Devlin et al., 2018)99.21 98.3698.86BERT-Tagger+FL98.36 98.9798.88(+0.02)BERT-Tagger+DL99.34 98.2298.91(+0.05)BERT-Tagger+DSC99.41 98.9399.38(+0.52)English TweetsModelPrec.Rec.F1FastText+CNN+CRF(Godin, 2019)--91.78BERT-Tagger (Devlin et al., 2018)92.33 91.9892.34BERT-Tagger+FL91.24 93.2292.47(+0.13)BERT-Tagger+DL91.44 92.8892.52(+0.18)BERT-Tagger+DSC92.87 93.5492.58(+0.24)Table 4: Experimental results for English POS datasets.In Table 2, we summarize all the aforementionedlosses. Figure 1 gives an explanation from the per-spective in derivative: The derivative of DSC ap-proaches zero right after p exceeds 0.5, which sug-gests the model attends less to examples once theyare correctly classiﬁed. But for the other losses, thederivatives reach 0 only if the probability is exactly1, which means they will push p to 1 as much aspossible.4ExperimentsWe evaluated the proposed method on four NLPtasks, part-of-speech tagging, named entity recog-nition, machine reading comprehension and para-phrase identiﬁcation. Hyperparameters are tunedon the corresponding development set of eachdataset. More experiment details including datasetsand hyperparameters are shown in supplementarymaterial.4.1Part-of-Speech TaggingSettingsPart-of-speech tagging (POS) is the taskof assigning a part-of-speech label (e.g., noun, verb,adjective) to each word in a given text. In this paper,we choose BERT (Devlin et al., 2018) as the back-bone and conduct experiments on three widely usedChinese POS datasets including Chinese Treebank(Xue et al., 2005) 5.0/6.0 and UD1.4 and Englishdatasets including Wall Street Journal (WSJ) andthe dataset proposed by Ritter et al. (2011). We re-port the span-level micro-averaged precision, recalland F1 for evaluation.BaselinesWe used the following baselines:• Joint-POS: Shao et al. (2017) jointly learnsChinese word segmentation and POS.• Lattice-LSTM: Zhang and Yang (2018) con-structs a word-character lattice network.• Bert-Tagger: Devlin et al. (2018) treats part-of-speech as a tagging task.ResultsTable 3 presents the experimental resultson Chinese datasets. As can be seen, the proposedDSC loss outperforms the best baseline results bya large margin, i.e., outperforming BERT-taggerby +1.86 in terms of F1 score on CTB5, +1.80 onCTB6 and +2.19 on UD1.4. As far as we know,we are achieving SOTA performances on the threedatasets. Focal loss only obtains a little perfor-mance improvement on CTB5 and CTB6, and thedice loss obtains huge gain on CTB5 but not onCTB6, which indicates the three losses are not con-sistently robust in solving the data imbalance issue.Table 4 presents the experimental results for En-glish datasets.\x0c470English CoNLL 2003ModelPrec.Rec.F1ELMo(Peters et al., 2018)--92.22CVT(Clark et al., 2018)--92.6BERT-Tagger(Devlin et al., 2018)--92.8BERT-MRC(Li et al., 2019)92.33 94.6193.04BERT-MRC+FL93.13 93.0993.11(+0.06)BERT-MRC+DL93.22 93.1293.17(+0.12)BERT-MRC+DSC93.41 93.2593.33(+0.29)English OntoNotes 5.0ModelPrec.Rec.F1CVT (Clark et al., 2018)--88.8BERT-Tagger (Devlin et al., 2018)90.01 88.3589.16BERT-MRC(Li et al., 2019)92.98 89.9591.11BERT-MRC+FL90.13 92.3491.22(+0.11)BERT-MRC+DL91.70 92.0691.88(+0.77)BERT-MRC+DSC91.59 92.5692.07(+0.96)Chinese MSRAModelPrec.Rec.F1Lattice-LSTM (Zhang and Yang, 2018) 93.57 92.7993.18BERT-Tagger (Devlin et al., 2018)94.97 94.6294.80Glyce-BERT (Wu et al., 2019)95.57 95.5195.54BERT-MRC(Li et al., 2019)96.18 95.1295.75BERT-MRC+FL95.45 95.8995.67(-0.08)BERT-MRC+DL96.20 96.6896.44(+0.69)BERT-MRC+DSC96.67 96.7796.72(+0.97)Chinese OntoNotes 4.0ModelPrec.Rec.F1Lattice-LSTM (Zhang and Yang, 2018) 76.35 71.5673.88BERT-Tagger (Devlin et al., 2018)78.01 80.3579.16Glyce-BERT (Wu et al., 2019)81.87 81.4080.62BERT-MRC(Li et al., 2019)82.98 81.2582.11BERT-MRC+FL83.63 82.9783.30(+1.19)BERT-MRC+DL83.97 84.0584.01(+1.90)BERT-MRC+DSC84.22 84.7284.47(+2.36)Table 5: Experimental results for NER task.4.2Named Entity RecognitionSettingsNamed entity recognition (NER) is thetask of detecting the span and semantic category ofentities within a chunk of text. Our implementationuses the current state-of-the-art model proposed byLi et al. (2019) as the backbone, and changes theMLE loss to DSC loss. Datasets that we use in-clude OntoNotes4.0 (Pradhan et al., 2011), MSRA(Levow, 2006), CoNLL2003 (Sang and Meulder,2003) and OntoNotes5.0 (Pradhan et al., 2013). Wereport span-level micro-averaged precision, recalland F1.BaselinesWe use the following baselines:• ELMo: a tagging model with pretrainingfrom Peters et al. (2018).• Lattice-LSTM: Zhang and Yang (2018) con-structs a word-character lattice, only used inChinese datasets.• CVT: Clark et al. (2018) uses Cross-ViewTraining(CVT) to improve the representationsof a Bi-LSTM encoder.• Bert-Tagger: Devlin et al. (2018) treats NERas a tagging task.• Glyce-BERT: Wu et al. (2019) combines Chi-nese glyph information with BERT pretrain-ing.• BERT-MRC:Li et al. (2019) formulatesNER as a machine reading comprehensiontask and achieves SOTA results on Chineseand English NER benchmarks.ResultsTable 5 shows experimental results onNER datasets. DSC outperforms BERT-MRC(Liet al., 2019) by +0.29, +0.96, +0.97 and +2.36 re-spectively on CoNLL2003, OntoNotes5.0, MSRAand OntoNotes4.0. As far as we are concerned, weare setting new SOTA performances on all of thefour NER datasets.4.3Machine Reading ComprehensionSettingsThe task of machine reading compre-hension (MRC) (Seo et al., 2016; Wang et al., 2016;Wang and Jiang, 2016; Wang et al., 2016; Shenet al., 2017; Chen et al., 2017) predicts the an-swer span in the passage given a question and thepassage. We followed the standard protocols inSeo et al. (2016), in which the start and end in-dexes of answer are predicted. We report ExtractMatch (EM) as well as F1 score on validation set.We use three datasets on this task: SQuAD v1.1,SQuAD v2.0 (Rajpurkar et al., 2016, 2018) andQuoref (Dasigi et al., 2019).BaselinesWe used the following baselines:• QANet: Yu et al. (2018b) builds a modelbased on convolutions and self-attentions.Convolutions are used to model local inter-actions and self-attention are used to modelglobal interactions.• BERT: Devlin et al. (2018) scores each can-didate span and the maximum scoring span isused as a prediction.• XLNet: Yang et al. (2019) proposes a gener-alized autoregressive pretraining method that\x0c471SQuAD v1.1SQuAD v2.0QuoRefModelEMF1EMF1EMF1QANet (Yu et al., 2018b)73.682.7--34.4138.26BERT (Devlin et al., 2018)84.190.978.781.958.4464.95BERT+FL84.6791.2578.9282.2060.7866.19(+0.57)(+0.35)(+0.22)(+0.30)(+2.34)(+1.24)BERT+DL84.8391.8678.9982.8862.0366.88(+0.73)(+0.96)(+0.29)(+0.98)(+3.59)(+1.93)BERT+DSC85.3491.9779.0282.9562.4467.52(+1.24)(+1.07)(+0.32)(+1.05)(+4.00)(+2.57)XLNet (Yang et al., 2019)88.9594.5286.1288.7964.5271.49XLNet+FL88.9094.5587.0489.3265.1972.34(-0.05)(+0.03)(+0.92)(+0.53)(+0.67)(+0.85)XLNet+DL89.1395.3687.2289.4465.7772.85(+0.18)(+0.84)(+1.10)(+0.65)(+1.25)(+1.36)XLNet+DSC89.7995.7787.6589.5165.9872.90(+0.84)(+1.25)(+1.53)(+0.72)(+1.46)(+1.41)Table 6: Experimental results for MRC task.MRPCQQPModelF1F1BERT (Devlin et al., 2018)88.091.3BERT+FL88.4391.86(+0.43)(+0.56)BERT+DL88.7191.92(+0.71)(+0.62)BERT+DSC88.9292.11(+0.92)(+0.81)XLNet (Yang et al., 2019)89.291.8XLNet+FL89.2592.31(+0.05)(+0.51)XLNet+DL89.3392.39(+0.13)(+0.59)XLNet+DSC89.7892.60(+0.58)(+0.79)Table 7: Experimental results for PI task.enables learning bidirectional contexts.ResultsTable 6 shows the experimental resultsfor MRC task. With either BERT or XLNet, ourproposed DSC loss obtains signiﬁcant performanceboost on both EM and F1. For SQuADv1.1, ourproposed method outperforms XLNet by +1.25 interms of F1 score and +0.84 in terms of EM. ForSQuAD v2.0, the proposed method achieves 87.65on EM and 89.51 on F1. On QuoRef, the pro-posed method surpasses XLNet by +1.46 on EMand +1.41 on F1.4.4Paraphrase IdentiﬁcationSettingsParaphrase identiﬁcation (PI) is the taskof identifying whether two sentences have thesame meaning or not. We conduct experimentson the two widely-used datasets: MRPC (Dolanand Brockett, 2005) and QQP. F1 score is reportedfor comparison. We use BERT (Devlin et al., 2018)and XLNet (Yang et al., 2019) as baselines.ResultsTable 7 shows the results. We ﬁnd thatreplacing the training objective with DSC intro-duces performance boost for both settings, +0.58for MRPC and +0.73 for QQP.5Ablation Studies5.1Datasets imbalanced to different extentsIt is interesting to see how differently the pro-posed objectives affect datasets imbalanced to dif-ferent extents. We use the paraphrase identiﬁcationdataset QQP (37% positive and 63% negative) forstudies. To construct datasets with different imbal-ance degrees, we used the original QQP datasetto construct synthetic training sets with differentpositive-negative ratios. Models are trained onthese different synthetic sets and then test on thesame original test set.• Original training set (original) The originaldataset with 363,871 examples, with 37% be-ing positive and 63% being negative• Positive augmentation (+ positive)We created a balanced dataset by adding posi-tive examples. We ﬁrst randomly chose posi-tive training examples in the original trainingset as templates. Then we used Spacy1 to re-trieve entity mentions and replace them withnew ones by linking mentions to their corre-sponding entities in DBpedia. The augmentedset contains 458,477 examples, with 50% be-ing positive and 50% being negative.• Negative augmentation (+ negative)We created a more imbalanced dataset. Thesize of the newly constructed training set and1https://github.com/explosion/spaCy\x0c472original+ positive+ negative- negative+ positive & negativeBERT91.392.2790.0889.7393.14BERT+FL91.86(+0.56)92.64(+0.37)90.61(+0.53)90.79(+1.06)93.45(+0.31)BERT+DL91.92(+0.62)92.87(+0.60)90.22(+0.14)90.49(+0.76)93.52(+0.38)BERT+DSC92.11(+0.81)92.92(+0.65)90.78(+0.70)90.80(+1.07)93.63(+0.49)Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.the data augmented technique are exactly thesame as +negative, except that we chose neg-ative training examples as templates. The aug-mented training set contains 458,477 exam-ples, with 21% being positive and 79% beingnegative.• Negative downsampling (- negative)We down-sampled negative examples in theoriginal training set to get a balanced trainingset. The down-sampled set contains 269,165examples, with 50% being positive and 50%being negative.• Positive and negative augmentation (+ pos-itive & +negative)We augmented the original training data withadditional positive and negative exampleswith the data distribution staying the same.The augmented dataset contains 458,477 ex-amples, with 50% being positive and 50% be-ing negative.Results are shown in Table 8. We ﬁrst look at theﬁrst line, with all results obtained using the MLEobjective. We can see that + positive outperformsoriginal, and +negative underperforms original.This is in line with our expectation since + pos-itive creates a balanced dataset while +negativecreates a more imbalanced dataset. Despite the factthat -negative creates a balanced dataset, the num-ber of training data decreases, resulting in inferiorperformances.DSC achieves the highest F1 score across alldatasets. Specially, for +positive, DSC achievesminor improvements (+0.05 F1) over DL. In con-trast, it signiﬁcantly outperforms DL for +negativedataset. This is in line with our expectation sinceDSC helps more on more imbalanced datasets. Theperformance of FL and DL are not consistent acrossdifferent datasets, while DSC consistently performsthe best on all datasets.5.2Dice loss for accuracy-oriented tasks?We argue that the cross-entropy objective is ac-tually accuracy-oriented, whereas the proposedlosses perform as a soft version of F1 score. ToSST-2SST-5ModelAccAccBERT+CE94.9055.57BERT+DL94.3754.63BERT+DSC94.8455.19Table 9: The effect of DL and DSC on sentiment clas-siﬁcation tasks. BERT+CE refers to ﬁne-tuning BERTand setting cross-entropy as the training objective.explore the effect of the dice loss on accuracy-oriented tasks such as text classiﬁcation, we con-duct experiments on the Stanford Sentiment Tree-bank (SST) datasets including SST-2 and SST-5.We ﬁne-tuned BERTLarge with different training ob-jectives. Experimental results for SST are shownin Table 9. For SST-5, BERT with CE achieves55.57 in terms of accuracy, while DL and DSCperform slightly worse (54.63 and 55.19, respec-tively). Similar phenomenon is observed for SST-2.These results verify that the proposed dice loss isnot accuracy-oriented, and should not be used foraccuracy-oriented tasks.5.3Hyper-parameters in Tversky IndexAs mentioned in Section 3.3, Tversky index (TI)offers the ﬂexibility in controlling the tradeoff be-tween false-negatives and false-positives. In thissubsection, we explore the effect of hyperparame-ters (i.e., α and β) in TI to test how they manipu-late the tradeoff. We conduct experiments on theChinese OntoNotes4.0 NER dataset and EnglishQuoRef MRC dataset. Experimental results areshown in Table 10. The highest F1 on ChineseOntoNotes4.0 is 84.67 when α is set to 0.6 whilefor QuoRef, the highest F1 is 68.44 when α isset to 0.4. In addition, we can observe that theperformance varies a lot as α changes in distinctdatasets, which shows that the hyperparametersα, β acturally play an important role in TI.6ConclusionIn this paper, we propose the dice-based loss tonarrow down the gap between training objectiveand evaluation metrics (F1 score). Experimentalresults show that the proposed loss function help\x0c473αChinese Onto4.0English QuoRefα = 0.180.1363.23α = 0.281.1763.45α = 0.384.2265.88α = 0.484.5268.44α = 0.584.4767.52α = 0.684.6766.35α = 0.781.8165.09α = 0.880.9764.13α = 0.980.2164.84Table 10: The effect of hyperparameters in Tversky In-dex. We set β = 1 −α and thus we only list α here.to achieve signiﬁcant performance boost withoutchanging model architectures.AcknowledgementWe thank all anonymous reviewers, as well asQinghong Han, Wei Wu and Jiawei Wu for theircomments and suggestions. The work is supportedby the National Natural Science Foundation ofChina (NSFC No. 61625107 and 61751209).ReferencesBernd Bohnet, Ryan T. McDonald, Gonc¸alo Sim˜oes,Daniel Andor, Emily Pitler, and Joshua Maynez. 2018.Morphosyntactic tagging with a meta-bilstm modelover context sensitive token encodings.In Proceed-ings of the 56th Annual Meeting of the Associationfor Computational Linguistics, ACL 2018, Melbourne,Australia, July 15-20, 2018, Volume 1: Long Papers,pages 2642–2652.Haw-Shiuan Chang, Erik G. Learned-Miller, and An-drew McCallum. 2017. Active bias: Training more ac-curate neural networks by emphasizing high variancesamples. In NIPS.N. V. Chawla, K. W. Bowyer, Lawrence O. Hall, andW. P. Kegelmeyer. 2002. Smote: Synthetic minorityover-sampling technique. J. Artif. Intell. Res., 16:321–357.Danqi Chen, Adam Fisch, Jason Weston, and AntoineBordes. 2017.Reading wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051.Kean Chen, Jianguo Li, Weiyao Lin, John See, Ji Wang,Lingyu Duan, Zhibo Chen, Changwei He, and JunniZou. 2019. Towards accurate one-stage object detec-tion with ap-loss. In IEEE Conference on Computer Vi-sion and Pattern Recognition, CVPR 2019, Long Beach,CA, USA, June 16-20, 2019, pages 5119–5127.Shijuan Chen, Haibo He, and Edwardo A. Garcia. 2010.Ramoboost: Ranked minority oversampling in boost-ing. IEEE Transactions on Neural Networks, 21:1624–1642.Kevin Clark, Minh-Thang Luong, Christopher D. Man-ning, and Quoc V. Le. 2018. Semi-supervised sequencemodeling with cross-view training. In Proceedings ofthe 2018 Conference on Empirical Methods in NaturalLanguage Procfessing, Brussels, Belgium, October 31 -November 4, 2018, pages 1914–1925.Pradeep Dasigi, Nelson F Liu, Ana Marasovic, Noah ASmith, and Matt Gardner. 2019.Quoref: A readingcomprehension dataset with questions requiring coref-erential reasoning. arXiv preprint arXiv:1908.05803.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018. Bert: Pre-training of deepbidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805.Lee R Dice. 1945. Measures of the amount of ecologicassociation between species. Ecology, 26(3):297–302.William B. Dolan and Chris Brockett. 2005. Automat-ically constructing a corpus of sentential paraphrases.In Proceedings of the Third International Workshop onParaphrasing (IWP2005).Yang Fan, Fei Tian, Tao Qin, Xiuping Li, and Tie-YanLiu. 2018. Learning to teach. ArXiv, abs/1805.03643.Ross B. Girshick. 2015.Fast r-cnn.2015 IEEE In-ternational Conference on Computer Vision (ICCV),pages 1440–1448.Ross B. Girshick, Jeff Donahue, Trevor Darrell, andJitendra Malik. 2013.Rich feature hierarchies foraccurate object detection and semantic segmentation.2014 IEEE Conference on Computer Vision and Pat-tern Recognition, pages 580–587.Fr´ederic Godin. 2019. Improving and Interpreting Neu-ral Networks for Word-Level Prediction Tasks in Natu-ral Language Processing. Ph.D. thesis, Ghent Univer-sity, Belgium.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. 2015. Deep residual learning for image recogni-tion. 2016 IEEE Conference on Computer Vision andPattern Recognition (CVPR), pages 770–778.Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li,and Li Fei-Fei. 2017. Mentornet: Learning data-drivencurriculum for very deep neural networks on corruptedlabels. In ICML.H. Kahn and A. W. Marshall. 1953. Methods of reduc-ing sample size in monte carlo computations. Opera-tions Research, 1(5):263–278.AnilKanduri,MohammadHashemHaghbayan,AmirM.Rahmani,MuhammadShaﬁque,AxelJantsch, and Pasi Liljeberg. 2018. adboost: Thermalaware performance boosting through dark siliconpatterning. IEEE Trans. Computers, 67(8):1062–1077.Angelos Katharopoulos and Franc¸ois Fleuret. 2018.Not all samples are created equal: Deep learning withimportance sampling. In ICML.Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom,Chris Dyer, Karl Moritz Hermann, G´aabor Melis, andEdward Grefenstette. 2018.The narrativeqa reading\x0c474comprehension challenge. Transactions of the Associa-tion of Computational Linguistics, 6:317–328.Oldrich Kodym, Michal Spanel, and Adam Herout.2018. Segmentation of head and neck organs at risk us-ing CNN with batch dice loss. In Pattern Recognition -40th German Conference, GCPR 2018, Stuttgart, Ger-many, October 9-12, 2018, Proceedings, pages 105–114.M. Pawan Kumar, Benjamin Packer, and DaphneKoller. 2010.Self-paced learning for latent variablemodels. In Advances in Neural Information Process-ing Systems 23: 24th Annual Conference on Neural In-formation Processing Systems 2010. Proceedings of ameeting held 6-9 December 2010, Vancouver, BritishColumbia, Canada., pages 1189–1197.Guillaume Lample, Miguel Ballesteros, Sandeep Sub-ramanian, Kazuya Kawakami, and Chris Dyer. 2016.Neural architectures for named entity recognition.arXiv preprint arXiv:1603.01360.Gina-Anne Levow. 2006. The third international Chi-nese language processing bakeoff: Word segmentationand named entity recognition. In Proceedings of theFifth SIGHAN Workshop on Chinese Language Pro-cessing, pages 108–117, Sydney, Australia. Associa-tion for Computational Linguistics.H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua. 2015.A convolutional neural network cascade for face detec-tion. In 2015 IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 5325–5334.Xiaoya Li, Jingrong Feng, Yuxian Meng, QinghongHan, Fei Wu, and Jiwei Li. 2019.A uniﬁed MRCframework for named entity recognition.CoRR,abs/1910.11476.Tsung-Yi Lin, Priya Goyal, Ross Girshick, KaimingHe, and Piotr Doll´ar. 2017. Focal loss for dense objectdetection.In Proceedings of the IEEE internationalconference on computer vision, pages 2980–2988.Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-quence labeling via bi-directional lstm-cnns-crf. arXivpreprint arXiv:1603.01354.Tomasz Malisiewicz, Abhinav Gupta, and Alexei A.Efros. 2011. Ensemble of exemplar-svms for object de-tection and beyond. In IEEE International Conferenceon Computer Vision, ICCV 2011, Barcelona, Spain,November 6-13, 2011, pages 89–96.Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,and Richard Socher. 2018.The natural languagedecathlon: Multitask learning as question answering.arXiv preprint arXiv:1806.08730.Yuxian Meng, Muyu Li, Wei Wu, and Jiwei Li. 2019.Dsreg:Using distant supervision as a regularizer.arXiv preprint arXiv:1905.11658.Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ah-madi. 2016.V-net: Fully convolutional neural net-works for volumetric medical image segmentation. In2016 Fourth International Conference on 3D Vision(3DV), pages 565–571. IEEE.David Nadeau and Satoshi Sekine. 2007. A survey ofnamed entity recognition and classiﬁcation. Lingvisti-cae Investigationes, 30(1):3–26.Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,Saurabh Tiwary, Rangan Majumder, and Li Deng.2016.Ms marco:A human generated machinereading comprehension dataset.arXiv preprintarXiv:1611.09268.Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng,Wanli Ouyang, and Dahua Lin. 2019. Libra R-CNN: to-wards balanced learning for object detection. In IEEEConference on Computer Vision and Pattern Recogni-tion, CVPR 2019, Long Beach, CA, USA, June 16-20,2019, pages 821–830.Matthew E Peters, Mark Neumann, Mohit Iyyer, MattGardner, Christopher Clark, Kenton Lee, and LukeZettlemoyer. 2018.Deep contextualized word repre-sentations. arXiv preprint arXiv:1802.05365.Sameer Pradhan, Mitchell P. Marcus, Martha Palmer,Lance A. Ramshaw, Ralph M. Weischedel, and Ni-anwen Xue, editors. 2011.Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task. ACL.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina,Yuchen Zhang, and Zhi Zhong. 2013. Towards robustlinguistic analysis using OntoNotes.In Proceedingsof the Seventeenth Conference on Computational Nat-ural Language Learning, pages 143–152, Soﬁa, Bul-garia. Association for Computational Linguistics.Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.Know what you don’t know: Unanswerable questionsfor squad. arXiv preprint arXiv:1806.03822.Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,and Percy Liang. 2016.Squad: 100,000+ questionsfor machine comprehension of text.arXiv preprintarXiv:1606.05250.Shaoqing Ren, Kaiming He, Ross B. Girshick, and JianSun. 2015. Faster r-cnn: Towards real-time object de-tection with region proposal networks.IEEE Trans-actions on Pattern Analysis and Machine Intelligence,39:1137–1149.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011. Named entity recognition in tweets: An exper-imental study. In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1524–1534, Edinburgh, Scotland, UK. As-sociation for Computational Linguistics.Erik F Sang and Fien De Meulder. 2003. Introductionto the conll-2003 shared task: Language-independentnamed entity recognition. arXiv preprint cs/0306050.Erik F. Tjong Kim Sang and Fien De Meulder. 2003.Introduction to the conll-2003 shared task: Language-independent named entity recognition.In Proceed-\x0c475ings of the Seventh Conference on Natural LanguageLearning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003, Edmonton, Canada, May 31 - June 1,2003, pages 142–147.Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, andHannaneh Hajishirzi. 2016.Bidirectional attentionﬂow for machine comprehension.arXiv preprintarXiv:1611.01603.Reuben R. Shamir, Yuval Duchin, Jinyoung Kim,Guillermo Sapiro, and Noam Harel. 2019. Continuousdice coefﬁcient: a method for evaluating probabilisticsegmentations. CoRR, abs/1906.11031.Yan Shao, Christian Hardmeier, J¨org Tiedemann, andJoakim Nivre. 2017. Character-based joint segmenta-tion and pos tagging for chinese using bidirectional rnn-crf. arXiv preprint arXiv:1704.01314.Chen Shen, Holger R. Roth, Hirohisa Oda, MasahiroOda, Yuichiro Hayashi, Kazunari Misawa, and Ken-saku Mori. 2018. On the inﬂuence of dice loss func-tion in multi-class organ segmentation of abdominalCT using 3d fully convolutional networks.CoRR,abs/1801.05912.Yelong Shen, Po-Sen Huang, Jianfeng Gao, andWeizhu Chen. 2017. Reasonet: Learning to stop read-ing in machine comprehension.In Proceedings ofthe 23rd ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 1047–1055. ACM.Th A Sorensen. 1948. A method of establishing groupsof equal amplitude in plant sociology based on similar-ity of species content and its application to analyses ofthe vegetation on danish commons. Biol. Skar., 5:1–34.CaroleH.Sudre,WenqiLi,TomVercauteren,S´ebastien Ourselin, and M. Jorge Cardoso. 2017. Gen-eralised dice overlap as a deep learning loss functionfor highly unbalanced segmentations. In Deep Learn-ing in Medical Image Analysis and Multimodal Learn-ing for Clinical Decision Support - Third InternationalWorkshop, DLMIA 2017, and 7th International Work-shop, ML-CDS 2017, Held in Conjunction with MIC-CAI 2017, Qu´ebec City, QC, Canada, September 14,2017, Proceedings, pages 240–248.Amos Tversky. 1977. Features of similarity. Psycho-logical review, 84(4):327.Sergi Valverde, Mariano Cabezas, Eloy Roura, San-dra Gonz´alez-Vill`a, Deborah Pareto, Joan C Vilanova,Llu´ıs Rami´o-Torrent`a, `Alex Rovira, Arnau Oliver, andXavier Llad´o. 2017.Improving automated multiplesclerosis lesion segmentation with a cascaded 3d con-volutional neural network approach.NeuroImage,155:159–168.Shuohang Wang and Jing Jiang. 2016. Machine com-prehension using match-lstm and answer pointer. arXivpreprint arXiv:1608.07905.Zhiguo Wang, Haitao Mi, Wael Hamza, and RaduFlorian. 2016.Multi-perspective context match-ing for machine comprehension.arXiv preprintarXiv:1612.04211.Wei Wu, Yuxian Meng, Qinghong Han, Muyu Li, Xi-aoya Li, Jie Mei, Ping Nie, Xiaofei Sun, and Jiwei Li.2019. Glyce: Glyph-vectors for chinese character rep-resentations. arXiv preprint arXiv:1901.10125.Naiwen Xue, Fei Xia, Fudong Choiu, and MartaPalmer. 2005. The penn chinese treebank: Phrase struc-ture annotation of a large corpus. Natural LanguageEngineering, 11(2):207–238.Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.Xlnet: Generalized autoregressive pretraining for lan-guage understanding. CoRR, abs/1906.08237.Adams Wei Yu, David Dohan, Minh-Thang Luong, RuiZhao, Kai Chen, Mohammad Norouzi, and Quoc VLe. 2018a. Qanet: Combining local convolution withglobal self-attention for reading comprehension. arXivpreprint arXiv:1804.09541.Adams Wei Yu, David Dohan, Minh-Thang Luong, RuiZhao, Kai Chen, Mohammad Norouzi, and Quoc V.Le. 2018b. Qanet: Combining local convolution withglobal self-attention for reading comprehension. In 6thInternational Conference on Learning Representations,ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,2018, Conference Track Proceedings.Yue Zhang and Jie Yang. 2018. Chinese ner using lat-tice lstm. arXiv preprint arXiv:1805.02023.ADataset DetailsA.1Part-of-Speech TaggingDatasetsWe conduct experiments on threewidely used benchmark, i.e., Chinese Treebank5.02/6.03 and UD1.44.• CTB5 is a Chinese dataset for taggingand parsing, which contains 507,222 words,824,983 characters and 18,782 sentences ex-tracted from newswire sources, including 698articles from Xinhua (1994-1998), 55 articlesfrom Information Services Department of HK-SAR (1997) and 132 articles from SinoramaMagazine (1996-1998 & 2000-2001).• CTB6 is an extension of CTB5, contain-ing 781,351 words, 1,285,149 characters and28,295 sentences.• UD is the abbreviation of Universal Depen-dencies, which is a framework for consistent2https://catalog.ldc.upenn.edu/LDC2005T013https://catalog.ldc.upenn.edu/LDC2007T364https://universaldependencies.org/\x0c476annotation of grammar (parts of speech, mor-phological features, and syntactic dependen-cies) across different human languages. Inthis work, we use UD1.4 for Chinese POStagging.A.2Named Entity RecognitionDatasetsFor the NER task, we consider bothChinese datasets, i.e., OntoNotes4.05 and MSRA6, and English datasets, i.e., CoNLL2003 7 andOntoNotes5.08.• CoNLL2003 is an English dataset with 4 en-tity types: Location, Organization, Person andMiscellaneous. We followed data processingprotocols in (Ma and Hovy, 2016).• English OntoNotes5.0 consists of texts froma wide variety of sources and contains 18 en-tity types. We use the standard train/dev/testsplit of CoNLL2012 shared task.• Chinese MSRA performs as a Chinese bench-mark dataset containing 3 entity types. Data inMSRA is collected from news domain. Sincethe development set is not provided in theoriginal MSRA dataset, we randomly splitthe training set into training and developmentsplits by 9:1. We use the ofﬁcial test set forevaluation.• Chinese OntoNotes4.0 is a Chinese datasetand consists of texts from news domain, whichhas 18 entity types. In this paper, we take thesame data split as Wu et al. (2019) did.A.3Machine Reading ComprephensionDatasetsFor MRC task, we use three datasets:SQuADv1.1/v2.09 and Queref10 datasets.• SQuAD v1.1 and SQuAD v2.0 are the mostwidely used QA benchmarks.SQuAD1.1is a collectionof 100K crowdsourcedquestion-answer pairs, and SQuAD2.0 ex-tends SQuAD1.1 allowing no short answerexists in the provided passage.5https://catalog.ldc.upenn.edu/LDC2011T036http://sighan.cs.uchicago.edu/bakeoff2006/7https://www.clips.uantwerpen.be/conll2003/ner/8https://catalog.ldc.upenn.edu/LDC2013T199https://rajpurkar.github.io/SQuAD-explorer/10https://allennlp.org/quoref• Quoref is a QA dataset which tests the coref-erential reasoning capability of reading com-prehension systems, containing 24K questionsover 4.7K paragraphs from Wikipedia.A.4Paraphrase IdentiﬁcationDatasetsExperiments are conducted on two PIdatasets: MRPC11 and QQP12.• MRPC is a corpus of sentence pairs automati-cally extracted from online news sources, withhuman annotations of whether the sentencepairs are semantically equivalent. The MRPCdataset has imbalanced classes (6800 pairs intotal, and 68% for positive, 32% for negative).• QQP is a collection of question pairs fromthe community question-answering websiteQuora. The class distribution in QQP is alsounbalanced (over 400,000 question pairs intotal, and 37% for positive, 63% for negative).11https://www.microsoft.com/en-us/download/details.aspx?id=5239812https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs']}, {'question': 'How many categories of aggression were participants asked to classify texts into?', 'ground_truth': '3 categories: overt aggression, covert aggression, and non-aggression.', 'answer': '3', 'contexts': ['Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying, pages 1–11Santa Fe, USA, August 25, 2018.1Benchmarking Aggression Identiﬁcation in Social MediaRitesh Kumar1, Atul Kr. Ojha2, Shervin Malmasi3, Marcos Zampieri41Bhim Rao Ambedkar University, 2Jawaharlal Nehru University,3Harvard Medical School, 4University of Wolverhampton,AbstractIn this paper, we present the report and ﬁndings of the Shared Task on Aggression Identiﬁcationorganised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1)at COLING 2018. The task was to develop a classiﬁer that could discriminate between OvertlyAggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants wereprovided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each inHindi (in both Roman and Devanagari script) and English for training and validation. For testing,two different sets - one from Facebook and another from a different social media - were provided.A total of 130 teams registered to participate in the task, 30 teams submitted their test runs,and ﬁnally 20 teams also sent their system description paper which are included in the TRACworkshop proceedings. The best system obtained a weighted F-score of 0.64 for both Hindi andEnglish on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50for English and Hindi respectively. The results presented in this report depict how challengingthe task is. The positive response from the community and the great levels of participation in theﬁrst edition of this shared task also highlights the interest in this topic.1IntroductionIn the last decade, with the emergence of an interactive web and especially popular social networkingand social media platforms like Facebook and Twitter, there has been an exponential increase in theuser-generated content being made available over the web. Now any information online has the powerto reach billions of people within a matter of seconds. This has resulted in not only positive exchange ofideas but has also lead to a widespread dissemination of aggressive and potentially harmful content overthe web. While most of the potentially harmful incidents like bullying or hate speech have predated theInternet, the reach and extent of Internet has given these incidents an unprecedented power and inﬂuenceto affect the lives of billions of people. It has been reported that these incidents have not only createdmental and psychological agony to the users of the web but has in fact forced people to deactivate theiraccounts and in extreme cases also commit suicides (Hinduja and Patchin, 2010). Thus the incidents ofaggression and unratiﬁed verbal behaviour have not remained a minor nuisance, but have acquired theform of a major criminal activity that affects a large number of people. It is therefore important thatpreventive measures can be taken to cope with abusive behaviour aggression online.One of the strategies to cope with aggressive behaviour online is to manually monitor and moderateuser-generated content, however, the amount and pace at which new data is being created on the web hasrendered manual methods of moderation and intervention almost completely impractical. As such theuse (semi-) automatic methods to identify such behaviour has become important and has attracted moreattention from the research community in recent years (Davidson et al., 2017; Malmasi and Zampieri,2017).This work is licensed under a Creative Commons Attribution 4.0 International License.License details:http://creativecommons.org/licenses/by/4.0/\x0c2This paper reports the results of the ﬁrst Shared Task on Aggression Identiﬁcation which was organisedjointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018.2Related WorkVerbal aggression per se has been rarely explored within the ﬁeld of Natural Language Processing.However, previous research in the ﬁeld has been carried out to automatically recognise several relatedbehaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al.,2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014;Hee et al., 2015), ﬂaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen etal., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017;Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana,2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wie-gand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy andSmeaton, 2004; Greevy, 2004) and others. In addition to these, there have been some pragmatic studieson behaviour like trolling (Hardaker, 2010; Hardaker, 2013).This huge interest in the ﬁeld from different perspectives has created a conglomeration of terminolo-gies as well as understandings of the phenomenon. On the one hand, this provides us with a very richand extensive insight into the phenomena yet, on the other hand, it has also created a theoretical gapin the understanding of interrelationship among these. Moreover, it has also resulted in duplication ofresearch, to certain extent, and a certain kind of lack of focus and reusability of datasets across differentstrands of research. In order to make improvements towards solving a complex phenomenon like this,it is of utmost importance that some kind of uniform understanding of problem be achieved so that, atleast, standardised datasets and an understanding of different approaches to solving the problem may bedeveloped.While a large part of the research has focused on any one of these phenomena and their computationalprocessing, it seems there is a signiﬁcant overlap among these phenomenon in the way they are under-stood in these studies - and because of this underlying overlap, insights from different studies mightprove useful for solving these seemingly different phenomena. All of these behaviours are consideredundesirable, aggressive and detrimental for those on the receiving end. So, trolling is intended “to causedisruption and/or to trigger or exacerbate conﬂict for the purposes of their own amusement” (Hardaker,2010). Cyberbullying is “humiliating and slandering behavior towards other people” (Nitta et al., 2013).Flaming intends “to offend someone through e-mail, posting, commenting or any statement using insults,swearing and hostile, intense language, trolling, etc.” (Krol, 1992).Waseem et al. (2017) makes an attempt to unify these different trends of research in what may be con-sidered a signiﬁcantly overlapping ﬁeld and proposes a 2-way typology for understanding what they call’abusive language’ over the web. They propose 2 scales on which abusive language could be categorised- the target of the abuse (an individual or a group) and the nature of the language (explicit or implicit).Our classiﬁcation of aggression into overt and covert aggression is largely similar to the explicit-implicitdistinction. However, we make a more detailed distinction in relation to the target of the abuse (Kumaret al., 2018b) and it is not made along the axis of individual vs. group. This is so because we noticedin a large number of instances both individuals and groups are simultaneously targeted - in such casesindividuals are targeted as members of certain groups or the individuals’ actions were considered thoseof the group and became the locus of attack. As such it was not feasible to distinguish between theindividual and group attack in lot of instances while annotating the dataset. The distinction that we madewas related to the “locus” of attack and included such targets as gender, religion, caste, country of origin,race, etc. This classiﬁcation, on the one hand, gave scope for focusing on different kinds of attack (forexample, racial attacks or communal attacks) and, on the other hand, each of these targets may actuallybe attacked using a different set of vocabulary, thereby, making these more natural classes that could beclassiﬁed using the surface-level linguistic features. Of course, it cannot be denied that these targets arenot mutually exclusive and, as such, it makes the problem not just a multi-class classiﬁcation problembut also multi-label classiﬁcation problem. In addition to this, we also make use of a different terminol-\x0c3ogy taking into account its use within socio-pragmatics. This was done with an understanding that hugeamount of literature within the ﬁeld of aggression and impoliteness studies might be able to contributeand provide insights to understanding the phenomenon in a better way.The aim of this shared task was much simpler than the one discussed in the previous para. It onlyinvolved classiﬁcation of the texts into 3 categories - overt aggression, covert aggression and non-aggression. We wanted to use the dataset for experimenting with different approaches to make the mosttop-level classiﬁcation of aggression on social media.3Task Setup and ScheduleThe participants interested in competing in the shared task were required to register using a Google Form.The form gave them an option to participate for either English or Hindi or both the languages. All theregistered participants were sent the links to the annotated dataset in the language(s) of their choice, alongwith a description of the format of the dataset. The participants were allowed to use additional data fortraining the system, with the condition that the additional dataset should be either publicly available ormake available immediately after submission (and well before the submission of the system papers) andthis must be mentioned in the submission. Use of non-public additional data for training was not allowed.The participants were given around 6 weeks to experiment and develop the system. However, since morethan half of the participants registered after the ﬁrst release of the data, most of them got less time thanthis. Initially, the dataset was not released publicly but was emailed only to the registered participants.After the 6 weeks of release of train and dev sets, the test set was released and the participants had 5 daysto test and upload their system. The complete timeline of the shared task is given in Table 1. We madeuse of CodaLab 1 for the evaluation. Each team was allowed to submit up to 3 systems for evaluation.We used the best of the 3 runs for the ﬁnal ranking and evaluation of the systems.DateEvent1 February, 2018Shared Task Announcement and Start of Registration13 March, 2018Release of train and dev sets25 April, 2018Release of test set30 April, 2018Deadline for Submission of System2 May, 2018Declaration of Results28 May, 2018Deadline for Submission of System Description PaperTable 1: Timeline of the Aggression Identiﬁcation Shared Task at TRAC - 1.4DatasetThe participants of the shared task were provided with a dataset of 12,000 randomly sampled Face-book comments for training and 3,000 comments for development and in English and Hindi each, an-notated with 3 levels of aggression - Overtly Aggressive (OAG), Covertly Aggressive (CAG) and Non-Aggressive (NAG). For test, 916 English comments and 970 Hindi comments were provided. Addi-tionally, 1,257 English tweets and 1,194 Hindi tweets were given as the surprise test set 2. The datasetreleased for the task is a subset of a larger dataset discussed in Kumar et al. (2018b).4.1Issues with the DatasetWhile most of the participants considered the dataset to be of high quality, two major problems came upduring the task -• The language issue: Some of the comments in English dataset contained code-mixed Hindi-Englishdata as well as data from other languages like German. These formed a minuscule proportion of thedata but nevertheless these need to be ﬁltered out.1https://competitions.codalab.org/2The complete dataset used for the shared task can be downloaded here - http://trac1-dataset.kmiagra.org/\x0c4• The annotation issue: The second and more serious issue that was raised by some participantsis related to the the annotation itself. Several instances of supposedly inaccurate annotation werepointed out. Despite the fact that aggression is a highly subjective phenomenon and different anno-tators may have different judgments about the same comment, some of the annotation indeed lookedhighly implausible and consequently it needs further scrutiny and validation.5Participants and ApproachesThe shared task gave the participants an option to register for either one of the two languages - Englishor Hindi - or both. A total of 131 participants registered for the shared task, with 73 teams registeringto participate only in English track, 2 teams only in Hindi track and 56 teams registered to participatein both the tracks. Out of these, ﬁnally a total of 30 teams submitted their systems - 15 teams for bothEnglish and Hindi and 30 teams for only English track. All the systems who submitted their systemwere invited to submit the system description paper, describing the experiments conducted by them. 18participants submitted the ﬁnal description paper which are included in the workshop proceedings - itincluded papers by majority of the top 10 teams. Table 2, lists the participating teams and the languagethey took part in.TeamHindiEnglishSystem Description Papersaroyehun✓(Aroyehun and Gelbukh, 2018)EBSI-LIA-UNAM✓(Arroyo-Fern´andez et al., 2018)DA-LD-Hildesheim✓✓(Modha et al., 2018)TakeLab✓(Golem et al., 2018)sreeIN✓(Madisetty and Desarkar, 2018)Julian✓✓(Risch and Krestel, 2018)taraka rama✓✓uOttawa✓(Orabi et al., 2018)Isistanitos✓(Tommasel et al., 2018)hakuchumu✓DataGeeks✓✓na14✓✓(Samghabadi et al., 2018)dinel✓(Orasan, 2018)vista.ue✓✓(Raiyani et al., 2018)MANITBHOPALINDIA✓✓IRIT✓(Ramiandrisoa and Mothe, 2018)quine✓✓(Nikhil et al., 2018)IIIT-Delhi✓PMRS✓✓(Maitra and Sarkhel, 2018)resham✓✓IreneR✓Nestor✓✓UAEMex-UAPT1✓✓forest and trees✓(Galery et al., 2018)groutar✓(Fortuna et al., 2018)Shusrut✓✓(Roy et al., 2018)malaypramanick✓UAEMex-UAPT-TAC2✓✓Unito✓✓bhanodaig✓(Kumar et al., 2018a)Total153018Table 2: The teams that participated in the Aggression Identiﬁcation Shared Task at TRAC - 1.Next we give a short description of the approach taken by each team for building their system. Moredetails about the approaches could be found in the paper submitted by the respective teams.\x0c5• saroyehun system gives the best performance with LSTM and they resorted to translation as dataaugmentation strategy. With the surprise twitter set, a combination of the representations of theRNN and CNN as features, along with additional preprocessing like spelling correction, translationof emoji, and computation of sentiment score gave the best performance. In this case, the datasetwas also augmented using translation and pseudolabelled using an external dataset on hate speech.3This is the only approach in the competition that performs better on the Twitter dataset, despitebeing trained the Facebook dataset, thereby, depicting the ability of the approach to generaliseacross domain.• EBSI-LIA-UNAM system uses a combination of the Passive-Aggressive (PA) and SVM classiﬁerswith character based n-gram (1 - 5 grams) TF-IDF for feature representation.• DA-LD-Hildesheim uses LSTM with pretrained Fasttext vector for embeddings for classifyingEnglish Facebook texts. For all other datasets including Twitter data in English and both Facebookand Twitter dataset in Hindi, CNN performs better.• TakeLab uses a Bidirectional LSTM on Glove embeddings to give the best performance.• sreeIN system uses a voting-based ensemble method with 3 classiﬁers - CNN with 4 layers, LSTMand Bidirectional LSTM.• Julian team uses translation as data augmentation strategy and use an ensemble of TF-IDF basedapproaches, using character n-grams (2 - 6) and word n-grams (1 - 2) with a bi-directional RNN,using fasttext embeddings, to get the best performance in the task..• taraka rama uses different systems for different datasets. For English Facebook dataset and HindiTwitter dataset, the team uses a stacked ensemble classiﬁer that uses a SVM on top of the ensembleof SVM classiﬁers. The SVMs were trained on 1 - 6 character n-grams and word unigrams. ForHindi Facebook and English Twitter dataset, however, a plain SVM trained using character and wordbag-of-n-grams gave the best performance. In this case, the overlapping character and word n-gramfeatures are weigthed with sublinear tf-idf before being used for training and testing. The systemis tuned using 5-fold CV on the combined training and develpment sets for maximum numberof character and word n-grams included, case normalization, and SVM margin (regularization)parameter C.• uOttawa system is trained using a novel deep-learning architecture for text classiﬁcation based onMulti-task learning (MTL). The approach, MTL, is evaluated using three neural network models.MultiCNN, multiple convolution structure with a trainable embedding layer, gives the best perfor-mance.• Isistanitos system uses a soft voting (average the class probabilities of other models) of two models- a recurrent neural network, and an SVM. The recurrent neural network uses 3 preprocesed setof features. The ﬁrst set uses an ad-hoc glove model for representing the words, the second isa sentiwornet based model, and the third is a traditional TfIdf plus Vader Sentiment analysis andsentiments associated with the emojis. The SVM model is trained on a TF-IDF of the post stemmedterms, excluding stopwords, and 3 - 5 character n-grams.• hakuchumu system makes use of a Random Forest classiﬁer with some preprocessing includingremoval of urls and non letter characters and stop words. Along with the bag-of-word, the approachuses multiple occurrences of letters, exclamation marks and question marks in a row and emoticonsas binary features.• DataGeeks system uses Logistic Regression classiﬁer with some preprocessing on the data suchas removing non-ascii characters, replacing new line with ’.’, replacing n’t with not, removingstopwords and 1 - 3 word n-grams and 2 - 6 character n-grams for training the classiﬁer.3https://github.com/ZeerakW/hatespeech\x0c6• na14 also uses Logistic Regression classiﬁer with preprocessing involving replacing URLs, num-bers, email addresses and spelling correction. The classiﬁer is trained using word unigrams, tf-idfvectors of word unigram, character 4-gram, character 5-gram and Google news pre-trained wordembedding model. For the Hindi dataset, Devanagari texts were transliterated into Roman at thepreprocessing stage.• dinel achieves the best accuracy on the Facebook test set using a Random Forest classiﬁer whileSVMs performed better for the surprise Twiiter test set. Both the classiﬁers were trained using 300semantic features which represent the vector representation of the text, average scores of the topemojis for each of the classes and positive and negative sentiment scores.• vista.ue system is developed using dense neural networks.• MANITBHOPALINDIA system for English is developed using SVM while for English it is trainedusing deep neural networks.• IRIT system gets the best performance for the English Facebook test set by using a combination oftwo models - a doc2vec model and a logistic regression classiﬁer. For the Twitter test set, it uses acombination of CNN and LSTM to get the best performance.• quine system is trained using an LSTM with attention and simple embeddings (word to index)instead of pre-trained embeddings.• IIIT-Delhi system uses a Single channel CNN for this task. Bayesian Optimization is used fortuning the parameters.• PMRS system employs a winner-takes- all autoencoder, called Emoti-KATE for Twitter senti-ment classiﬁcation. Each input dimension of Emoti-KATE is a log-normalized, sentiwordnet-scoreweighted word-count vector. A binary cross-entropy loss function is used to train the network.• resham system for English has been made using an open vocabulary approach and ensemble modelof two predictors with soft voting. The ﬁrst predictor is a Naive Bayes model with CountVectorizerfor preprocessing. The second predictor is a recurrent neural network with one embedding layerand two LSTM layers. Pre-trained word vectors have been used for the embedding layer. For Hindidataset, a Naive Bayes classiﬁer is trained using the dataset augmented with English translations.• IreneR system is based on a Multinomial Naive Bayes classiﬁer that uses unigrams, bigrams,hedging bigrams and trigrams such as ’do you’, someone who is’,’to see that’, that potentiallysignal covert aggressivity, identiﬁed with chi-squared test as features. It also includes features fromLIWC2015 (list of anger and swear words).• Nestor uses an approach that combines Neural Networks and a new word representation model.The patterns obtained from the word model representation are used for training the back propa-gation neural network with ﬁx parameters. The length of the post was ﬁxed and the word modelrepresentation is language independent, so it was used for both the English and the Hindi tasks.• UAEMex-UAPT1 uses the same approach as used by the team Nestor.• forest and trees system uses a Pooled Recurrent Unit architecture combined with pre-trained En-glish and Hindi fasttext word embeddings as a representation of the sequence input. In this approach,Hindi and English vectors were aligned using pre-computed SVD matrices that pulls representationsfrom different languages into a single space. This enabled the same model to be used for both thelanguages, thereby, making data re-utilization and model deployability easier.• groutar system is trained using random forests. The dataset is augmented with an external toxicitydataset 4. The approach involved understanding the effects of new data on aggression identiﬁcation.4https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\x0c7• Shusrut system uses an ensemble of CNN 2D with MAXPOOL classiﬁer and a SVM classiﬁer. Theensemble model is passed through 3 dense layers to ﬁnally predict the output. Softmax activation isused in the outer layer for classiﬁcation.• malaypramanick system uses a random forest classiﬁer trained using a set of surface-level fea-tures including number of line,s uppercase and lowercase letters, digits, named entities, unicodecharacters, etc.• UAEMex-UAPT-TAC2 system is generated by combination of twelve distance measures, througha K Nearest Neighbors classiﬁcation algorithm and a canonical genetic algorithm.• Unito is the only unsupervised system submitted in the task. It is based only on a multilinguallexicon of aggressive words. The lexicon is obtained by automatic translation from an handmadelexicon of offensive words in Italian, with minimal human supervision. The original words areexpanded into a list of their senses. The senses are manually annotated to ﬁlter out senses that arenever used in an offensive context. Finally, all the lemmas of the remaining senses are generatedwith BabelNet in 50+ languages. The words in the lexicon are divided in those translating sense thatcan be used in an offensive context (but not necessarily are) and words translating senses that aredirectly offensive. This distinction is mapped to the Overtly Aggressive and Covertly Aggressiveclasses respectively. The classiﬁcation of sentences is straightforward: a sentence that does notcontain any word from the lexicon is tagged as NAG, a sentence containing more directly offensivewords than potentially offensive words is tagged as OAG, and the other cases are tagged as CAG.• bhanodaig system uses a bidirectional LSTM.6ResultsIn this section, we present the results of the experiments carried out by different teams during the sharedtask. The results of the top 15 teams on English dataset is given in Figure 1 and that on Hindi dataset isin Figure 2.Figure 1: Performance of top 15 teams on English Dataset\x0c8The participants were allowed to use other datasets, in addition to the one provided by the organizers ofthe task. However, because of the lack of similar alternative datasets, all the groups, except ’groutar’ and’saroyehun’ team, used only the dataset provided for the task. As we mentioned earlier, the participantswere given two kinds of test sets for the ﬁnal testing of the system - one from Facebook and a surprisetest set from Twitter.Figure 2: Performance of teams on Hindi Dataset7ConclusionIn this paper, we have presented the report of the First Shared task on Aggression Identiﬁcation organizedwith the TRAC workshop at COLING 2018. The shared task received a very encouraging response fromthe community which underlines the relevance and need of the task. More than 100 teams registered and30 teams ﬁnally submitted their system.The performance of the best systems in the task show that aggression identiﬁcation is a hard problem tosolve. Moreover, the performance of the neural networks-based systems as well as the other approachesdo not seem to differ much. If the features are carefully selected then classiﬁers like SVM and evenrandom forest and logistic regression perform at par with deep neural networks. On the other had, weﬁnd quite a few neural networks-based systems not performing quite well in the task. Nonetheless, 14systems were trained using one or the other architectures of deep neural networks - either solely or aspart of an ensemble. Moreover, 8 systems out of the top 15 are trained on neural networks, which showsthe efﬁcacy of the approach but at the same time does not rule out the usefulness and relevance of linearmodels for the task. There was only one system, Unito, that made use of a lexicon-based approach tosolve the task. A few participants of the task pointed out the apparent “inconsistencies” in the annotation.It points towards the need to get the annotations validated by multiple human annotators.AcknowledgementsWe would like to thank Microsoft Research India for providing grants to prepare the dataset and to ourannotators who worked very hard to ﬁnish the annotations within a strict deadline.\x0c9We would also like to thank the participants of the Shared Task for their participation and feedbackand the TRAC workshop PC members for thoroughly reviewing the shared task papers within a veryshort span of time.ReferencesSwati Agarwal and Ashish Sureka. 2015. Using knn and svm based one-class classiﬁer for detecting onlineradicalization on twitter. In International Conference on Distributed Computing and Internet Technology, pages431 – 442. Springer.Swati Agarwal and Ashish Sureka. 2017. Characterizing linguistic attributes for automatic classiﬁcation of intentbased racist/radicalized posts on tumblr micro-blogging website.Segun Taofeek Aroyehun and Alexander Gelbukh. 2018. Aggression detection in social media: Using deepneural networks, data augmentation, and pseudo labeling. In Proceedings of the First Workshop on Trolling,Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Ignacio Arroyo-Fern´andez, Dominic Forest, Juan-Manuel Torres-Moreno, Mauricio Carrasco-Ruiz, Thomas Leg-eleux, and Karen Joannette. 2018. Cyber-bullying detection task: the ebsi-lia-unam system (elu) at coling’18trac-1. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe,USA.Peter Burnap and Matthew L. Williams. 2014. Hate speech, machine classiﬁcation and statistical modelling ofinformation ﬂows on twitter: Interpretation and communication for policy decision making. In Proceedings ofInternet, Policy & Politics, pages 1 – 18.Erik Cambria, Praphul Chandra, Avinash Sharma, and Amir Hussain. 2010. Do not feel the trolls. In ISWC,Shanghai.Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu. 2012. Detecting offensive language in social media to pro-tect adolescent online safety. privacy, security, risk and trust (passat). In International Conference on SocialComputing (SocialCom), pages 71–80.Maral Dadvar, Dolf Trieschnigg, Roeland Ordelman, and Franciska de Jong. 2013. Improving cyberbullyingdetection with user context. In Advances in Information Retrieval, pages 693–696. Springer.Maral Dadvar, Dolf Trieschnigg, and Franciska de Jong. 2014. Experts and machines against bullies: a hybridapproach to detect cyberbullies. In Advances in Artiﬁcial Intelligence, pages 275–281. Springer, Berlin.Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detectionand the problem of offensive language. In Proceedings of ICWSM.Karthik Dinakar, Birago Jones, Catherine Havasi Henry Lieberman, and Rosalind Picard. 2012. Common sensereasoning for detection, prevention, and mitigation of cyberbullying. ACM Transactions on Interactive Intelli-gent Systems (TiiS), 2(3):18:1–18:30.Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan Radosavljevic, and Narayan Bhamidipati.2015. Hate speech detection with comment embeddings. In Proceedings of the 24th International Conferenceon World Wide Web, pages 29 – 30.Paula Fortana. 2017. Automatic detection of hate speech in text: an overview of the topic and dataset annotationwith hierarchical classes. Master’s thesis, Faculdade de Engenharia da Universidade do Porto.Paula Fortuna, Jos´e Ferreira, Luiz Pires, Guilherme Routar, and S´ergio Nunes. 2018. Merging datasets for aggres-sive text identiﬁcation. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC– 1), Santa Fe, USA.Thiago Galery, Efstathios Charitos, and Ye Tian. 2018. Aggression identiﬁcation and multi lingual word embed-dings. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe,USA.Njagi Dennis Gitari, Zhang Zuping, Hanyurwimfura Damien, and Jun Long. 2015. A lexicon- based approach forhate speech detection. International Journal of Multimedia and Ubiquitous Engineering, 10(4):215 – 230.\x0c10Viktor Golem, Mladen Karan, and Jan najder. 2018. Combining traditional machine learning models with deeplearning for aggressive text detection. In Proceedings of the First Workshop on Trolling, Aggression and Cyber-bullying (TRAC – 1), Santa Fe, USA.Edel Greevy and Alan F. Smeaton. 2004. Classifying racist texts using a support vector machine. In Proceedingsof the 27th annual international ACM SIGIR conference on Research and development in information retrieval,pages 468 – 469. ACM.Edel Greevy. 2004. Automatic text categorisation of racist webpages. Ph.D. thesis, Dublin City University.Claire Hardaker. 2010. Trolling in asynchronous computer-mediated communication: From user discussions toacademic deﬁnitions. Journal of Politeness Research. Language, Behaviour, Culture, 6(2):215–242.Claire Hardaker. 2013. uh. . . . not to be nitpicky,,,,,but...the past tense of drag is dragged, not drug. an overviewof trolling strategies. Journal of Language Aggression and Conﬂict, 1(1):58–86.Cynthia Van Hee, Els Lefever, Ben Verhoeven, Julie Mennes, Bart Desmet, Guy De Pauw, Walter Daelemans, andVronique Hoste. 2015. Detection and ﬁne-grained classiﬁcation of cyberbullying events. In Proceedings ofInternational Conference Recent Advances in Natural Language Processing (RANLP), pages 672–680.Sameer Hinduja and Justin W Patchin. 2010. Bullying, Cyberbullying, and Suicide. Archives of suicide research,14(3):206–221.E. Krol. 1992. The whole internet: User’s guide & catalog. O’Reilly & Associates, Inc., Sebastopol, CA.Sudhakar Kumar, Francesca Spezzano, and VS Subrahmanian. 2014. Accurately detecting trolls in slashdotzoo via decluttering. In Proceedings of IEEE/ACM International Conference on Advances in Social NetworksAnalysis and Mining (ASONAM), pages 188–195.Ritesh Kumar, Guggilla Bhanodai, Rajendra Pamula, and Maheshwar Reddy Chennuru. 2018a. Trac-1 shared taskon aggression identiﬁcation: Iit(ism)@coling18. In Proceedings of the First Workshop on Trolling, Aggressionand Cyberbullying (TRAC – 1), Santa Fe, USA.Ritesh Kumar, Aishwarya N. Reganti, Akshit Bhatia, and Tushar Maheshwari. 2018b. Aggression-annotated cor-pus of hindi-english code-mixed data. In Nicoletta Calzolari (Conference chair), Khalid Choukri, ChristopherCieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, HlneMazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of theEleventh International Conference on Language Resources and Evaluation (LREC 2018), Paris, France, may.European Language Resources Association (ELRA).Sreekanth Madisetty and Maunendra Sankar Desarkar. 2018. Aggression detection in social media using deepneural networks. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1),Santa Fe, USA.Promita Maitra and Ritesh Sarkhel. 2018. Emoti-kate: a k-competitive autoencoder for aggression detection insocial media text. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1),Santa Fe, USA.Shervin Malmasi and Marcos Zampieri. 2017. Detecting Hate Speech in Social Media. In Proceedings of theInternational Conference Recent Advances in Natural Language Processing (RANLP), pages 467–472.Shervin Malmasi and Marcos Zampieri. 2018. Challenges in discriminating profanity from hate speech. Journalof Experimental & Theoretical Artiﬁcial Intelligence, 30:1 – 16.Todor Mihaylov, Georgi D Georgiev, AD Ontotext, and Preslav Nakov. 2015. Finding opinion manipulation trollsin news community forums. In Proceedings of the Nineteenth Conference on Computational Natural LanguageLearning, CoNLL, pages 310–314.Sandip Modha, Prasenjit Majumder, and Thomas Mandl. 2018. Filtering aggression from multilingual socialmedia feed. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), SantaFe, USA.Luis G Mojica. 2016. Modeling trolling in social media conversations.Nishant Nikhil, Ramit Pahwa, Mehul Kumar Nirala, and Rohan Khilnani. 2018. Lstms with attention for aggres-sion detection. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1),Santa Fe, USA.\x0c11Nitin, Ankush Bansal, Siddhartha Mahadev Sharma, Kapil Kumar, Anuj Aggarwal, Sheenu Goyal, Kanika Choud-hary, Kunal Chawla, Kunal Jain, and Manav Bhasinar. 2012. Classiﬁcation of ﬂames in computer mediatedcommunications.Taisei Nitta, Fumito Masui, Michal Ptaszynski, Yasutomo Kimura, Rafal Rzepka, and Kenji Araki. 2013. Detect-ing cyberbullying entries on informal school websites based on category relevance maximization. In Proceed-ings of IJCNLP, pages 579–586.Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. 2016. Abusive Language Detec-tion in Online User Content. In Proceedings of the 25th International Conference on World Wide Web, pages145–153. International World Wide Web Conferences Steering Committee.Ahmed Husseini Orabi, Mahmoud Husseini Orabi, Qianjia Huang, Diana Inkpen, and David Van Bruwaene. 2018.Cyber-aggression detection using cross segment-and-concatenate multi-task learning from text. In Proceedingsof the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Constantin Orasan. 2018. Aggressive Language Identiﬁcation Using Word Embeddings and Sentiment Features.In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Manish Gupta Pinkesh Badjatiya, Shashank Gupta and Vasudeva Varma. 2017. Deep learning for hate speechdetection in tweets. In Proceedings of the 26th International Conference on World Wide Web Companion, pages759 – 760. International World Wide Web Conferences Steering Committee.Kashyap Raiyani, Teresa Gonc¸alves, Paulo Quaresma, and Vitor Beires Nogueira. 2018. Fully connected neuralnetwork with advance preprocessor to identify aggression over facebook and twitter. In Proceedings of the FirstWorkshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Faneva Ramiandrisoa and Josiane Mothe. 2018. Irit at trac 2018. In Proceedings of the First Workshop on Trolling,Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Julian Risch and Ralf Krestel. 2018. Aggression identiﬁcation using deep learning and data augmentation. InProceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Arjun Roy, Prashant Kapil, Kingshuk Basak, and Asif Ekbal. 2018. An ensemble approach for aggression identiﬁ-cation in english and hindi text. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying(TRAC – 1), Santa Fe, USA.Niloofar SaﬁSamghabadi, Deepthi Mave, Sudipta Kar, and Thamar Solorio. 2018. Ritual-uh at trac 2018 sharedtask: Aggression identiﬁcation. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbully-ing (TRAC – 1), Santa Fe, USA.Sasha Sax. 2016. Flame Wars: Automatic Insult Detection. Technical report, Stanford University.Anna Schmidt and Michael Wiegand. 2017. A Survey on Hate Speech Detection Using Natural Language Pro-cessing. In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media.Association for Computational Linguistics, pages 1–10, Valencia, Spain.Antonela Tommasel, Juan Manuel Rodriguez, and Daniela Godoy. 2018. Textual aggression detection throughdeep learning. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1),Santa Fe, USA.Fabio Del Vigna, Andrea Cimino, Felice DellOrletta, Marinella Petrocchi, and Maurizio Tesconi. 2017. Hate me,hate me not: Hate speech detection on facebook. In Proceedings of the First Italian Conference on Cybersecu-rity, pages 86 – 95.Zeerak Waseem and Dirk Hovy. 2016. Hateful symbols or hateful people? predictive features for hate speechdetection on twitter. In Proceedings of NAACL-HLT, pages 88 – 93.Zeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017. Understanding abuse: A typologyof abusive language detection subtasks. In Proceedings of the First Workshop on Abusive Language Online,pages 78–84. Association for Computational Linguistics.']}, {'question': 'Based on Figure 2, which language exhibited the greatest bias in accuracy between stereotypical and non-stereotypical gender roles when using Google Translate?', 'ground_truth': 'HE (Hebrew) exhibited the greatest bias in accuracy between stereotypical and non-stereotypical gender roles when using Google Translate, with an accuracy of 76% for stereotypical roles and 38% for non-stereotypical roles, resulting in a difference of 38 percentage points.', 'answer': 'B', 'contexts': ['Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics1679Evaluating Gender Bias in Machine TranslationGabriel Stanovsky1,2, Noah A. Smith1,2, and Luke Zettlemoyer11Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, USA2Allen Institute for Artiﬁcial Intelligence, Seattle, USA{gabis,nasmith,lsz}@cs.washington.eduAbstractWe present the ﬁrst challenge set and eval-uation protocol for the analysis of genderbias in machine translation (MT). Our ap-proach uses two recent coreference resolutiondatasets composed of English sentences whichcast participants into non-stereotypical genderroles (e.g., “The doctor asked the nurse to helpher in the operation”). We devise an automaticgender bias evaluation method for eight tar-get languages with grammatical gender, basedon morphological analysis (e.g., the use of fe-male inﬂection for the word “doctor”). Ouranalyses show that four popular industrial MTsystems and two recent state-of-the-art aca-demic MT models are signiﬁcantly prone togender-biased translation errors for all testedtarget languages. Our data and code are pub-licly available at https://github.com/gabrielStanovsky/mt_gender.1IntroductionLearned models exhibit social bias when theirtraining data encode stereotypes not relevant forthe task, but the correlations are picked up any-way. Notable examples include gender biases invisual SRL (cooking is stereotypically done bywomen, construction workers are stereotypicallymen; Zhao et al., 2017), lexical semantics (“manis to computer programmer as woman is to home-maker”; Bolukbasi et al., 2016), and natural lan-guage inference (associating women with gossip-ing and men with guitars; Rudinger et al., 2017).In this work, we conduct the ﬁrst large-scalemultilingual evaluation of gender-bias in machinetranslation (MT), following recent small-scalequalitative studies which observed that online MTservices, such as Google Translate or MicrosoftTranslator, also exhibit biases, e.g., translatingnurses as females and programmers as males, re-gardless of context (Alvarez-Melis and Jaakkola,The doctor asked the nurse to help her in the procedureEl doctor le pidio a la enfermera que le ayudara con el procedimientoFigure 1: An example of gender bias in machine trans-lation from English (top) to Spanish (bottom).Inthe English source sentence, the nurse’s gender is un-known, while the coreference link with “her” identi-ﬁes the “doctor” as a female. On the other hand, theSpanish target sentence uses morphological featuresfor gender: “el doctor” (male), versus “la enfermer-a” (female). Aligning between source and target sen-tences reveals that a stereotypical assignment of genderroles changed the meaning of the translated sentence bychanging the doctor’s gender.2017; Font and Costa-Juss`a, 2019). Google Trans-late recently tried to mitigate these biases by al-lowing users to sometimes choose between gen-dered translations (Kuczmarski, 2018).As shown in Figure 1, we use data introducedby two recent coreference gender-bias studies: theWinogender (Rudinger et al., 2018), and the Wino-Bias (Zhao et al., 2018) datasets. Following theWinograd schema (Levesque, 2011), each instancein these datasets is an English sentence which de-scribes a scenario with human entities, who areidentiﬁed by their role (e.g., “the doctor” and “thenurse” in Figure 1), and a pronoun (“her” in theexample), which needs to be correctly resolvedto one of the entities (“the doctor” in this case).Rudinger et al. (2018) and Zhao et al. (2018) foundthat while human agreement on the task was high(roughly 95%), coreference resolution models of-ten ignore context and make socially biased pre-dictions, e.g., associating the feminine pronoun“her” with the stereotypically female “nurse.”We observe that for many target languages, afaithful translation requires a similar form of (at\x0c1680least implicit) gender identiﬁcation. In addition,in the many languages which associate betweenbiological and grammatical gender (e.g., most Ro-mance, Germanic, Slavic, and Semitic languages;Craig, 1986; Mucchi-Faina, 2005; Corbett, 2007),the gender of an animate object can be identiﬁedvia morphological markers. For instance, whentranslating our running example in Figure 1 toSpanish, a valid translation may be: “La doc-tora le pidio a la enfermera que le ayudara conel procedimiento,” which indicates that the doctoris a woman, by using a feminine sufﬁx inﬂection(“doctora”) and the feminine deﬁnite gendered ar-ticle (“la”). However, a biased translation systemmay ignore the given context and stereotypicallytranslate the doctor as male, as shown at the bot-tom of the ﬁgure.Following these observations, we design a chal-lenge set approach for evaluating gender bias inMT using a concatenation of Winogender andWinoBias.We devise an automatic translationevaluation method for eight diverse target lan-guages, without requiring additional gold trans-lations, relying instead on automatic measuresfor alignment and morphological analysis (Sec-tion 2). We ﬁnd that four widely used commercialMT systems and two recent state-of-the-art aca-demic models are signiﬁcantly gender-biased onall tested languages (Section 3). Our method andbenchmarks are publicly available, and are easilyextensible with more languages and MT models.2Challenge Set for Gender Bias in MTWe compose a challenge set for gender bias in MT(which we dub “WinoMT”) by concatenating theWinogender and WinoBias coreference test sets.Overall, WinoMT contains 3,888 instances, and isequally balanced between male and female gen-ders, as well as between stereotypical and non-stereotypical gender-role assignments (e.g., a fe-male doctor versus a female nurse). Additionaldataset statistics are presented in Table 1.We use WinoMT to estimate the gender-bias ofan MT model, M, in target-language L by per-forming following steps (exempliﬁed in Figure 1):(1) Translate all of the sentences in WinoMT intoL using M, thus forming a bilingual corpus of En-glish and the target language L.(2) Align between the source and target transla-tions, using fast align (Dyer et al., 2013), trainedon the automatic translations from from step (1).WinogenderWinoBiasWinoMTMale24015821826Female24015861822Neutral2400240Total72031683888Table 1:The coreference test sets and resultingWinoMT corpus statistics (in number of instances).We then map the English entity annotated in thecoreference datasets to its translation (e.g., alignbetween “the doctor” and “el doctor” in Figure 1).(3) Finally, we extract the target-side entity’sgender using simple heuristics over language-speciﬁc morphological analysis, which we per-form using off-the-shelf tools for each target lan-guage, as discussed in the following section.This process extracts the translated genders, ac-cording to M, for all of the entities in WinoMT,which we can then evaluate against the gold anno-tations provided by the original English dataset.This process can introduce noise into our eval-uation in steps (2) and (3), via wrong alignmentsor erroneous morphological analysis. In Section 3,we will present a human evaluation showing theseerrors are infrequent.3EvaluationIn this section, we brieﬂy describe the MT systemsand the target languages we use, our main results,and their human validation.3.1Experimental SetupMT systemsWe test six widely used MT mod-els, representing the state of the art in bothcommercial and academic research: (1) GoogleTranslate,1 (2) Microsoft Translator,2 (3) AmazonTranslate,3 (4) SYSTRAN,4 (5) the model of Ottet al. (2018), which recently achieved the best per-formance on English-to-French translation on theWMT’14 test set, and (6) the model of Edunovet al. (2018), the WMT’18 winner on English-to-German translation. We query the online API forthe ﬁrst four commercial MT systems, while forthe latter two academic models we use the pre-trained models provided by the Fairseq toolkit.51https://translate.google.com2https://www.bing.com/translator3https://aws.amazon.com/translate4http://www.systransoft.com5https://github.com/pytorch/fairseq\x0c1681Google TranslateMicrosoft TranslatorAmazon Translate∗SYSTRANAcc∆G∆SAcc∆G∆SAcc∆G∆SAcc∆G∆SES53.123.421.347.336.823.259.415.422.345.646.315.0FR63.66.426.744.736.429.755.217.724.945.044.09.4IT39.632.921.539.839.817.042.427.818.538.947.59.4RU37.736.811.436.842.18.539.734.79.237.344.19.3UK38.443.610.841.346.911.8–––28.922.412.9HE53.77.937.848.114.932.950.510.347.346.620.524.5AR48.543.716.147.348.313.449.838.519.047.049.45.3DE59.412.512.574.10.030.262.412.016.748.634.510.3Table 2: Performance of commercial MT systems on the WinoMT corpus on all tested languages, categorized bytheir family: Spanish, French, Italian, Russian, Ukrainian, Hebrew, Arabic, and German. Acc indicates overallgender accuracy (% of instances the translation had the correct gender), ∆G denotes the difference in performance(F1 score) between masculine and feminine scores, and ∆S is the difference in performance (F1 score) betweenpro-stereotypical and anti-stereotypical gender role assignments (higher numbers in the two latter metrics indicatestronger biases). Numbers in bold indicate best accuracy for the language across MT systems (row), and underlinednumbers indicate best accuracy for the MT system across languages (column). ∗Amazon Translate does not havea trained model for English to Ukrainian.Acc∆G∆SFR (Ott et al., 2018)49.42.616.1DE (Edunov et al., 2018)52.57.38.4Table 3: Performance of recent state-of-the-art aca-demic translation models from English to French andGerman. Metrics are the same as those in Table 2.Target languages and morphological analysisWe selected a set of eight languages with gram-matical gender which exhibit a wide range ofother linguistic properties (e.g., in terms of al-phabet, word order, or grammar), while still al-lowing for highly accurate automatic morpholog-ical analysis. These languages belong to four dif-ferent families: (1) Romance languages: Span-ish, French, and Italian, all of which have gen-dered noun-determiner agreement and spaCy mor-phological analysis support (Honnibal and Mon-tani, 2017). (2) Slavic languages (Cyrillic alpha-bet): Russian and Ukrainian, for which we usethe morphological analyzer developed by Korobov(2015). (3) Semitic languages: Hebrew and Ara-bic, each with a unique alphabet.For Hebrew,we use the analyzer developed by Adler and El-hadad (2006), while gender inﬂection in Arabiccan be easily identiﬁed via the ta marbuta charac-ter, which uniquely indicates feminine inﬂection.(4) Germanic languages: German, for which weuse the morphological analyzer developed by Al-tinok (2018).3.2ResultsOur main ﬁndings are presented in Tables 2 and 3.For each tested MT system and target language wecompute three metrics with respect to their abil-ity to convey the correct gender in the target lan-guage. Ultimately, our analyses indicate that alltested MT systems are indeed gender biased.First, the overall system Accuracy is calculatedby the percentage of instances in which the trans-lation preserved the gender of the entity fromthe original English sentence. We ﬁnd that mosttested systems across eight tested languages per-form quite poorly on this metric. The best per-forming model on each language often does notdo much better than a random guess for the correctinﬂection. An exception to this rule is the transla-tion accuracies on German, where three out of foursystems acheive their best performance. This maybe explained by German’s similarity to the Englishsource language (Hawkins, 2015).In Table 2, ∆G denotes the difference in per-formance (F1 score) between male and femaletranslations. Interestingly, all systems, except Mi-crosoft Translator on German, perform signiﬁ-cantly better on male roles, which may stem fromthese being more frequent in the training set.Perhaps most tellingly, ∆S measures the differ-\x0c1682ESFRITRUUKHEARDE2040608010067805244467660694654303335384457Accuracy (%)StereotypicalNon-StereotypicalFigure 2: Google Translate’s performance on gender translation on our tested languages. The performance on thestereotypical portion of WinoMT is consistently better than that on the non-stereotypical portion. The other MTsystems we tested display similar trends.Original+Adj∆ES53.163.5+10.4RU37.748.9+11.2UK38.442.9+4.5Table 4: Performance of Google Translate on Spanish,Russian, and Ukranian gender prediction accuracy (%correct) on the original WinoMT corpus, versus a mod-iﬁed version of the dataset where we add sterotypicalgender adjectives (see Section 3.3).ence in performance (F1 score) between stereo-typical and non-stereotypical gender role assign-ments, as deﬁned by Zhao et al. (2018) whouse statistics provided by the US Department ofLabor.6This metric shows that all tested sys-tems have a signiﬁcant and consistently better per-formance when presented with pro-stereotypicalassignments (e.g., a female nurse), while theirperformance deteriorates when translating anti-stereotypical roles (e.g., a male receptionist).For instance, Figure 2 depicts Google Trans-late absolute accuracies on stereotypical and non-stereotypical gender roles across all tested lan-guages. Other tested systems show similar trends.3.3Fighting Bias with BiasFinally, we tested whether we can affect thetranslations by automatically creating a versionof WinoMT with the adjectives “handsome” and“pretty” prepended to male and female entities, re-spectively. For example, the sentence in Figure 1will be converted to: “The pretty doctor asked thenurse to help her in the operation”. We are inter-ested in evaluating whether this “corrects” the pro-fession bias by mixing signals, e.g., while “doc-6https://www.bls.gov/cps/cpsaat11.htmtor” biases towards a male translation, “pretty”tugs the translation towards a female inﬂection.Our results show that this improved performancein some languages, signiﬁcantly reducing bias inSpanish, Russian, and Ukrainian (see Table 4).Admittedly, this is impractical as a general debi-asing scheme, since it assumes oracle coreferenceresolution, yet it attests to the relation betweencoreference resolution and MT, and serves as a fur-ther indication of gender bias in MT.3.4Human ValidationWe estimate the accuracy of our gender bias evalu-ation method by randomly sampling 100 instancesof all translation systems and target languages, an-notating each sample by two target-language na-tive speakers (resulting in 9,600 human annota-tions). Each instance conformed to a format sim-ilar to that used by our automatic gender detec-tion algorithm: human annotators were asked tomark the gender of an entity within a given target-language sentence. (e.g., see “el doctor” as high-lighted in the Spanish sentence in Figure 1). Byannotating at the sentence-level, we can accountfor both types of possible errors, i.e., alignmentand gender extraction.We compare the sentence-level human anno-tations to the output of our automatic method,and ﬁnd that the levels of agreement for all lan-guages and systems were above 85%, with anaverage agreement on 87% of the annotations.In comparison, human inter-annotator agreementwas 90%, due to noise introduced by several inco-herent translations.Our errors occur when language-speciﬁc id-iosyncrasies introduce ambiguity to the morpho-logical analysis. For example, gender for certainwords in Hebrew cannot be distinguished without\x0c1683Source[Target lang.] Predicted translationPhenomenonThe janitor does not like the bakerbecause she always messes up thekitchen.[ES] Al conserje no le gusta el panaderoporque ella siempre desordena la cocina.Biased translation, giving “baker” amale inﬂection, with a mismatched pro-noun reference.The janitor does not like the prettybaker because she always messes upthe kitchen.[ES] Al conserje no le gusta la panaderabonita porque ella siempre desordena lacocina.Adding a stereotypically female adjec-tive “ﬁxes” the translation.The counselor asked the guard a fewquestions and praised her for the goodwork.[FR] Le conseiller a pos´e quelques ques-tions `a la garde et l’a lou´ee pour le bontravail.French uses “garde” for both male andfemale guards, allowing for a more di-rect translation from English.Table 5: Examples of Google Translate’s output for different sentences in the WinoMT corpus. Words in blue,red, and orange indicate male, female and neutral entities, respectively.diacritics (e.g., the male and female versions of theword “baker” are spelled identically), and the con-tracted determiner in French and Italian (l’) is usedfor both masculine and feminine nouns. In ad-dition, some languages have only male or femaleinﬂections for professions which were stereotypi-cally associated with one of the genders, for exam-ple “sastre” (tailor) in Spanish or “soldat” (soldier)in French, which do not have female inﬂections.See Table 5 for detailed examples.4DiscussionRelated workThis work is most related to sev-eral recent efforts which evaluate MT throughthe use of challenge sets.Similarly to our useWinoMT, these works evaluate MT systems (ei-ther manually or automatically) on test sets whichare specially created to exhibit certain linguis-tic phenomena, thus going beyond the traditionalBLEU metric (Papineni et al., 2002). These in-clude challenge sets for language-speciﬁc idiosyn-crasies (Isabelle et al., 2017), discourse phenom-ena (Bawden et al., 2018), pronoun translation(M¨uller et al., 2018; Webster et al., 2018), orcoreference and multiword expressions (Burchardtet al., 2017).Limitations and future workWhile our workpresents the ﬁrst large-scale evaluation of genderbias in MT, it still suffers from certain limitationswhich could be addressed in follow up work. First,like some of the challenge sets discussed above,WinoMT is composed of synthetic English source-side examples. On the one hand, this allows fora controlled experiment environment, while, onthe other hand, this might introduce some artiﬁ-cial biases in our data and evaluation.Ideally,WinoMT could be augmented with natural “in thewild” instances, with many source languages, allannotated with ground truth entity gender. Sec-ond, similar to any medium size test set, it is clearthat WinoMT serves only as a proxy estimation forthe phenomenon of gender bias, and would prob-ably be easy to overﬁt. A larger annotated cor-pus can perhaps provide a better signal for train-ing. Finally, even though in Section 3.3 we showa very rudimentary debiasing scheme which relieson oracle coreference system, it is clear that thisis not applicable in a real-world scenario. Whilerecent research has shown that getting rid of suchbiases may prove to be very challenging (Elazarand Goldberg, 2018; Gonen and Goldberg, 2019),we hope that this work will serve as a ﬁrst step fordeveloping more gender-balanced MT models.5ConclusionsWe presented the ﬁrst large-scale multilingualquantitative evidence for gender bias in MT,showing that on eight diverse target languages,all four tested popular commercial systems andtwo recent state-of-the-art academic MT mod-els are signiﬁcantly prone to translate basedon gender stereotypes rather than more mean-ingful context.Our data and code are pub-liclyavailableathttps://github.com/gabrielStanovsky/mt_gender.AcknowledgmentsWe would like to thank Mark Yatskar, Iz Beltagy,Tim Dettmers, Ronan Le Bras, Kyle Richardson,Ariel and Claudia Stanovsky, and Paola Virga formany insightful discussions about the role genderplays in the languages evaluated in this work, aswell as the reviewers for their helpful comments.\x0c1684ReferencesMeni Adler and Michael Elhadad. 2006. An unsuper-vised morpheme-based HMM for Hebrew morpho-logical disambiguation. In ACL.Duygu Altinok. 2018. DEMorphy, German languagemorphological analyzer. CoRR, abs/1803.00902.David Alvarez-Melis and Tommi S. Jaakkola. 2017.A causal framework for explaining the predictionsof black-box sequence-to-sequence models.InEMNLP.Rachel Bawden, Rico Sennrich, Alexandra Birch, andBarry Haddow. 2018.Evaluating discourse phe-nomena in neural machine translation. In NAACL-HLT.Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou,Venkatesh Saligrama, and Adam Tauman Kalai.2016. Man is to computer programmer as womanis to homemaker? debiasing word embeddings. InNIPS.Aljoscha Burchardt,Vivien Macketanz,Jon De-hdari, Georg Heigold, Jan-Thorsten Peter, andPhilip Williams. 2017.A linguistic evaluation ofrule-based, phrase-based, and neural mt engines.The Prague Bulletin of Mathematical Linguistics,108(1):159–170.Greville G Corbett. 2007. Gender and noun classes.Colette G Craig. 1986. Noun Classes and Categoriza-tion: Proceedings of a Symposium on Categoriza-tion and Noun Classiﬁcation, volume 7. John Ben-jamins Publishing Company.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013. A simple, fast, and effective reparameteriza-tion of ibm model 2. In HLT-NAACL.Sergey Edunov, Myle Ott, Michael Auli, and DavidGrangier. 2018. Understanding back-translation atscale. arXiv preprint arXiv:1808.09381.Yanai Elazar and Yoav Goldberg. 2018. Adversarialremoval of demographic attributes from text data. InEMNLP.Joel Escud´e Font and Marta R. Costa-Juss`a. 2019.Equalizing gender biases in neural machine trans-lation with word embeddings techniques.CoRR,abs/1901.03116.Hila Gonen and Yoav Goldberg. 2019. Lipstick on apig: Debiasing methods cover up systematic genderbiases in word embeddings but do not remove them.HLT-NAACL.John A Hawkins. 2015. A Comparative Typology ofEnglish and German: Unifying the Contrasts. Rout-ledge.Matthew Honnibal and Ines Montani. 2017. spaCy 2:Natural language understanding with Bloom embed-dings, convolutional neural networks and incremen-tal parsing. To appear.Pierre Isabelle, Colin Cherry, and George F. Foster.2017. A challenge set approach to evaluating ma-chine translation. In EMNLP.Mikhail Korobov. 2015. Morphological analyzer andgenerator for Russian and Ukrainian languages.In Mikhail Yu. Khachay, Natalia Konstantinova,Alexander Panchenko, Dmitry I. Ignatov, and Va-leri G. Labunets, editors, Analysis of Images, SocialNetworks and Texts, volume 542 of Communicationsin Computer and Information Science, pages 320–332. Springer International Publishing.James Kuczmarski. 2018.Reducing gender bias ingoogle translate.Hector J. Levesque. 2011. The Winograd schema chal-lenge. In AAAI Spring Symposium: Logical Formal-izations of Commonsense Reasoning.Angelica Mucchi-Faina. 2005. Visible or inﬂuential?language reforms and gender (in) equality. SocialScience Information, 44(1):189–215.Mathias M¨uller, Annette Rios, Elena Voita, and RicoSennrich. 2018. A large-scale test set for the evalu-ation of context-aware pronoun translation in neuralmachine translation. CoRR, abs/1810.02268.Myle Ott,Sergey Edunov,David Grangier,andMichael Auli. 2018. Scaling neural machine trans-lation. arXiv preprint arXiv:1806.00187.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic eval-uation of machine translation. In ACL.Rachel Rudinger, Chandler May, and Benjamin VanDurme. 2017.Social bias in elicited natural lan-guage inferences. In EthNLP@EACL.Rachel Rudinger, Jason Naradowsky, Brian Leonard,and Benjamin Van Durme. 2018.Gender bias incoreference resolution. In NAACL-HLT.Kellie Webster, Marta Recasens, Vera Axelrod, and Ja-son Baldridge. 2018.Mind the gap: A balancedcorpus of gendered ambiguous pronouns. Transac-tions of the Association for Computational Linguis-tics, 6:605–617.Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-donez, and Kai-Wei Chang. 2017.Men also likeshopping: Reducing gender bias ampliﬁcation usingcorpus-level constraints. In EMNLP.Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-donez, and Kai-Wei Chang. 2018. Gender bias incoreference resolution:Evaluation and debiasingmethods. In NAACL-HLT.']}, {'question': 'Which training data size from the Common Crawl resulted in the highest average accuracy across all GLUE tasks?', 'ground_truth': 'The 18,000 million tokens (M tok) training data size from the Common Crawl resulted in the highest average accuracy of 81.3 across all GLUE tasks.', 'answer': 'C', 'contexts': ['Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 62–90Florence, Italy, August 1-2, 2019. c⃝2019 Association for Computational Linguistics62Results of the WMT19 Metrics Shared Task:Segment-Level and Strong MT Systems Pose Big ChallengesQingsong MaTencent-CSIG, AI Evaluation Labqingsong.mqs@gmail.comJohnny Tian-Zheng WeiUMass Amherst, CICSjwei@umass.eduOndřej BojarCharles University, MFF ÚFALbojar@ufal.mff.cuni.czYvette GrahamDublin City University, ADAPTgraham.yvette@gmail.comAbstractThis paper presents the results of theWMT19 Metrics Shared Task.Par-ticipants were asked to score the out-puts of the translations systems compet-ing in the WMT19 News TranslationTask with automatic metrics. 13 researchgroups submitted 24 metrics, 10 of whichare reference-less “metrics” and constitutesubmissions to the joint task with WMT19Quality Estimation Task, “QE as a Met-ric”. In addition, we computed 11 baselinemetrics, with 8 commonly applied base-lines (BLEU, SentBLEU, NIST, WER,PER, TER, CDER, and chrF) and 3 reim-plementations (chrF+, sacreBLEU-BLEU,and sacreBLEU-chrF). Metrics were evalu-ated on the system level, how well a givenmetric correlates with the WMT19 offi-cial manual ranking, and segment level,how well the metric correlates with humanjudgements of segment quality. This year,we use direct assessment (DA) as our onlyform of manual evaluation.1IntroductionTo determine system performance in machinetranslation (MT), it is often more practicalto use an automatic evaluation, rather thana manual one. Manual/human evaluation canbe costly and time consuming, and so an au-tomatic evaluation metric, given that it suffi-ciently correlates with manual evaluation, canbe useful in developmental cycles. In studiesinvolving hyperparameter tuning or architec-ture search, automatic metrics are necessaryas the amount of human effort implicated inmanual evaluation is generally prohibitivelylarge.As objective, reproducible quantities,metrics can also facilitate cross-paper compar-isons. The WMT Metrics Shared Task1 annu-ally serves as a venue to validate the use ofexisting metrics (including baselines such asBLEU), and to develop new ones; see Koehnand Monz (2006) through Ma et al. (2018).In the setup of our Metrics Shared Task,an automatic metric compares an MT sys-tem’s output translations with manual ref-erence translations to produce:either (a)system-level score, i.e. a single overall scorefor the given MT system, or (b) segment-levelscores for each of the output translations, orboth.This year we teamed up with the organizersof the QE Task and hosted “QE as a Metric” asa joint task. In the setup of the Quality Esti-mation Task (Fonseca et al., 2019), no human-produced translations are provided to estimatethe quality of output translations. Quality es-timation (QE) methods are built to assess MToutput based on the source or based on thetranslation itself. In this task, QE developerswere invited to perform the same scoring asstandard metrics participants, with the excep-tion that they refrain from using a referencetranslation in production of their scores. Wethen evaluate the QE submissions in exactlythe same way as regular metrics are evalu-ated, see below.From the point of view ofcorrelation with manual judgements, there isno difference in metrics using or not using ref-erences.The source, reference texts, and MT sys-tem outputs for the Metrics task come fromthe News Translation Task (Barrault et al.,2019, which we denote as Findings 2019). Thetexts were drawn from the news domain andinvolve translations of English (en) to/from1http://www.statmt.org/wmt19/metrics-task.html\x0c63Czech (cs), German (de), Finnish (fi), Gu-jarati (gu), Kazakh (kk), Lithuanian (lt), Rus-sian (ru), and Chinese (zh), but excluding cs-en (15 language pairs). Three other languagepairs not including English were also manu-ally evaluated as part of the News TranslationTask: German→Czech and German↔French.In total, metrics could participate in 18 lan-guage pairs, with 10 target languages.In the following, we first give an overview ofthe task (Section 2) and summarize the base-line (Section 3) and submitted (Section 4) met-rics. The results for system- and segment-levelevaluation are provided in Sections 5.1 and5.2, respectively, followed by a joint discussionSection 6.2Task SetupThis year, we provided task participants withone test set for each examined language pair,i.e. a set of source texts (which are commonlyignored by MT metrics), corresponding MToutputs (these are the key inputs to be scored)and a reference translation (held out for theparticipants of “QE as a Metric” track).In the system-level, metrics aim to corre-late with a system’s score which is an aver-age over many human judgments of segmenttranslation quality produced by the given sys-tem.In the segment-level, metrics aim toproduce scores that correlate best with a hu-man ranking judgment of two output trans-lations for a given source segment (more onthe manual quality assessment in Section 2.3).Participants were free to choose which lan-guage pairs and tracks (system/segment andreference-based/reference-free) they wanted totake part in.2.1Source and Reference TextsThe source and reference texts we use arenewstest2019 from this year’s WMT NewsTranslation Task (see Findings 2019).Thisset contains approximately 2,000 sentences foreach translation direction (except Gujarati,Kazakh and Lithuanian which have approx-imately 1,000 sentences each, and Germanto/from French which has 1701 sentences).The reference translations provided in new-stest2019 were created in the same direc-tion as the MT systems were translating.The exceptions are German→Czech whereboth sides are translations from English andGerman↔French which followed last years’practice. Last year and the years before, thedataset consisted of two halves, one originat-ing in the source language and one in the tar-get language.This however lead to adverseartifacts in MT evaluation.2.2System OutputsThe results of the Metrics Task are affectedby the actual set of MT systems participatingin a given translation direction. On one hand,if all systems are very close in their transla-tion quality, then even humans will struggleto rank them. This in turn will make the taskfor MT metrics very hard. On the other hand,if the task includes a wide range of systemsof varying quality, correlating with humansshould be generally easier, see Section 6.1 fora discussion on this. One can also expect thatif the evaluated systems are of different types,they will exhibit different error patterns andvarious MT metrics can be differently sensi-tive to these patterns.This year, all MT systems included in theMetrics Task come from the News TranslationTask (see Findings 2019). There are howeverstill noticeable differences among the variouslanguage pairs.• UnsupervisedMTSystems.TheGerman→Czech research systems weretrained in an unsupervised fashion, i.e.without the access to parallel Czech-German texts (except for a couple ofthousand sentences used primarily for val-idation).We thus expect the researchGerman-Czech systems to be “more cre-ative” and depart further away from thereferences.The online systems in thislanguage directions are however standardMT systems so the German-Czech evalu-ation could be to some extent bimodal.• EU Election.The French↔Germantranslation was focused on a sub-domainof news, namely texts related EU Elec-tion. Various MT system developers mayhave invested more or less time to the do-main adaptation.• Regular News Tasks Systems. These\x0c64are all the other MT systems in theevaluation; differing in whether they aretrained only on WMT provided data(“Constrained”, or “Unconstrained”) asin the previous years. All the freely avail-able web services (online MT systems) aredeemed unconstrained.Overall, the results are based on 233 systemsacross 18 language pairs.22.3Manual Quality AssessmentDirect Assessment (DA, Graham et al., 2013,2014a, 2016) was employed as the source of the“golden truth” to evaluate metrics again thisyear.The details of this method of humanevaluation are provided in Findings 2019.The basis of DA is to collect a large numberof quality assessments (a number on a scale of1–100, i.e. effectively a continuous scale) forthe outputs of all MT systems. These scoresare then standardized per annotator.In the past years, the underlying man-ual scores were reference-based (human judgeshad access to the same reference translationas the MT quality metric). This year, the of-ficial WMT19 scores are reference-based (or“monolingual”) for some language pairs andreference-free (or “bilingual”) for others.3Due to these different types of golden truthcollection, reference-based language pairs arein a closer match with the standard reference-based metrics, while the reference-free lan-guage pairs are better fit for the “QE as ametric” subtask.Note that system-level manual scores aredifferent than those of the segment-level. Sincefor segment-level evaluation, collecting enoughDA judgements for each segment is infeasible,so we resort to converting DA judgements to2This year, we do not use the artificially constructed“hybrid systems” (Graham and Liu, 2016) because theconfidence on the ranking of system-level metrics is suf-ficient even without hybrids.3Specifically,the reference-based language pairswere those where the anticipated translation qualitywas lower or where the manual judgements were ob-tained with the help of anonymous crowdsourcing.Most of these cases were translations into English (fi-en, gu-en, kk-en, lt-en, ru-en and zh-en) and then thelanguage pairs not involving English (de-cs, de-fr andfr-de). The reference-less (bilingual) evaluations werethose where mainly MT researchers themselves wereinvolved in the annotations: en-cs, en-de, en-fi, en-gu,en-kk, en-lt, en-ru, en-zh.golden truth expressed as relative rankings, seeSection 2.3.2.The exact methods used to calculate corre-lations of participating metrics with the goldentruth are described below, in the two sectionsfor system-level evaluation (Section 5.1) andsegment-level evaluation (Section 5.2).2.3.1System-level Golden Truth: DAFor the system-level evaluation, the collectedcontinuous DA scores, standardized for eachannotator, are averaged across all assessed seg-ments for each MT system to produce a scalarrating for the system’s performance.The underlying set of assessed segments isdifferent for each system. Thanks to the factthat the system-level DA score is an averageover many judgments, mean scores are consis-tent and have been found to be reproducible(Graham et al., 2013). For more details seeFindings 2019.2.3.2Segment-level Golden Truth:daRRStarting from Bojar et al. (2017), when WMTfully switched to DA, we had to come up with asolid golden standard for segment-level judge-ments. Standard DA scores are reliable onlywhen averaged over sufficient number of judg-ments.4Fortunately, when we have at least two DAscores for translations of the same source in-put, it is possible to convert those DA scoresinto a relative ranking judgement, if the dif-ference in DA scores allows conclusion thatone translation is better than the other.Inthe following, we denote these re-interpretedDA judgements as “daRR”, to distinguishit clearly from the relative ranking (“RR”)golden truth used in the past years.54For segment-level evaluation, one would need tocollect many manual evaluations of the exact same seg-ment as produced by each MT system. Such a samplingwould be however wasteful for the evaluation needed byWMT, so only some MT systems happen to be evalu-ated for a given input sentence. In principle, we wouldlike to return to DA’s standard segment-level evalua-tion in future, where a minimum of 15 human judge-ments of translation quality are collected per transla-tion and combined to get highly accurate scores fortranslations, but this would increase annotation costs.5Since the analogue rating scale employed by DA ismarked at the 0-25-50-75-100 points, we use 25 pointsas the minimum required difference between two sys-tem scores to produce daRR judgements. Note that we\x0c65DA>1AveDA pairsdaRRde-en2,00016.0239,22085,365fi-en1,9969.583,16838,307gu-en1,01611.055,88031,139kk-en1,00011.055,00027,094lt-en1,00011.055,00021,862ru-en1,99911.9131,76646,172zh-en2,00010.195,17431,070en-cs1,9979.175,56027,178en-de1,99719.1347,10999,840en-fi1,9978.159,12931,820en-gu9986.921,85411,355en-kk9989.037,03218,172en-lt9989.036,43517,401en-ru1,9978.769,50324,334en-zh1,9979.887,50118,658de-cs1,9978.565,03935,793de-fr1,6054.112,0554,862fr-de1,2243.04,2581,369newstest2019Table 1: Number of judgements for DA convertedto daRR data; “DA>1” is the number of sourceinput sentences in the manual evaluation where atleast two translations of that same source inputsegment received a DA judgement; “Ave” is theaverage number of translations with at least oneDA judgement available for the same source inputsentence; “DA pairs” is the number of all possi-ble pairs of translations of the same source inputresulting from “DA>1”; and “daRR” is the num-ber of DA pairs with an absolute difference in DAscores greater than the 25 percentage point mar-gin.From the complete set of human assess-ments collected for the News Translation Task,all possible pairs of DA judgements attributedto distinct translations of the same source wereconverted into daRR better/worse judge-ments.Distinct translations of the samesource input whose DA scores fell within 25percentage points (which could have beendeemed equal quality) were omitted from theevaluation of segment-level metrics. Conver-sion of scores in this way produced a large setof daRR judgements for all language pairs,rely on judgements collected from known-reliable vol-unteers and crowd-sourced workers who passed DA’squality control mechanism.Any inconsistency thatcould arise from reliance on DA judgements collectedfrom low quality crowd-sourcing is thus prevented.shown in Table 1 due to combinatorial ad-vantage of extracting daRR judgements fromall possible pairs of translations of the samesource input. We see that only German-Frenchand esp. French-German can suffer from in-sufficient number of these simulated pairwisecomparisons.The daRR judgements serve as the goldenstandardforsegment-levelevaluationinWMT19.3Baseline MetricsIn addition to validating popular metrics, in-cluding baselines metrics serves as comparisonand prevents “loss of knowledge” as mentionedby Bojar et al. (2016).Moses scorer6 is one of the MT evaluationtools that aggregated several useful metricsover the time.Since Macháček and Bojar(2013), we have been using Moses scorer toprovide most of the baseline metrics and keptencouraging authors of well-performing MTmetrics to include them in Moses scorer.7The baselines we report are:BLEUandNIST The metrics BLEU(Papinenietal.,2002)andNIST(Doddington,2002)werecomputedusingmteval-v13a.pl8fromtheOpenMTEvaluationCampaign.Thetoolincludesitsowntokeniza-tion.We run mteval with the flag--international-tokenization.9TER, WER, PER and CDER. The met-rics TER (Snover et al., 2006), WER,PER and CDER (Leusch et al., 2006)were produced by the Moses scorer, whichis used in Moses model optimization.We used the standard tokenizer script asavailable in Moses toolkit for tokeniza-tion.sentBLEU. The metric sentBLEU is com-puted using the script sentence-bleu,a part of the Moses toolkit.It is a6https://github.com/moses-smt/mosesdecoder/blob/master/mert/evaluator.cpp7If you prefer standard BLEU, we recommend sacre-BLEU (Post, 2018a), found at https://github.com/mjpost/sacreBLEU.8http://www.itl.nist.gov/iad/mig/tools/9International tokenization is found to performslightly better (Macháček and Bojar, 2013).\x0c66Scoring LevelMetricFeaturesLearned?SegSysCitation/ParticipantAvailabilityBaselinessentBLEUn-grams•−(mosesdecoder) mert/sentence-bleuBLEUn-grams−•Papineni et al. (2002)(mosesdecoder) scripts/generic/mteval-v13a.plNISTn-grams−•Doddington (2002)(mosesdecoder) scripts/generic/mteval-v13a.plWERLevenshtein distance−•Leusch et al. (2006)(mosesdecoder) mert/evaluatorTERedit distance, edit types−•Snover et al. (2006)(mosesdecoder) mert/evaluatorPERedit distance, edit types−•Leusch et al. (2003)(mosesdecoder) mert/evaluatorCDERedit distance, edit types−•Leusch et al. (2006)(mosesdecoder) mert/evaluatorchrFcharacter n-grams•⊘Popović (2015)http://github.com/m-popovic/chrFchrF+character n-grams•⊘Popović (2017)http://github.com/m-popovic/chrFsacreBLEU-BLEUn-grams−•Post (2018a)http://github.com/mjpost/sacreBLEUsacreBLEU-chrFn-grams−•Post (2018a)http://github.com/mjpost/sacreBLEUMetricsBEERchar. n-grams, permutation treesyes•⊘Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015)http://github.com/stanojevic/beerBERTrcontextual word embeddings•⊘Univ. of Melbourne (Mathur et al., 2019)http://github.com/nitikam/mteval-in-contextcharacTERchar. edit distance, edit types•⊘RWTH Aachen Univ. (Wang et al., 2016a)http://github.com/rwth-i6/CharacTEREEDchar. edit distance, edit types•⊘RWTH Aachen Univ. (Stanchev et al., 2019)http://github.com/rwth-i6/ExtendedEditDistanceESIMlearned neural representationsyes•⊘Univ. of Melbourne (Mathur et al., 2019)http://github.com/nitikam/mteval-in-contextLEPORasurface linguistic features•⊘Dublin City University, ADAPT (Han et al., 2012, 2013)http://github.com/poethan/LEPORLEPORbsurface linguistic features•⊘Dublin City University, ADAPT (Han et al., 2012, 2013)http://github.com/poethan/LEPORMeteor++_2.0 (syntax)word alignments•⊘Peking University (Guo and Hu, 2019)−Meteor++_2.0 (syntax+copy)word alignments•⊘Peking University (Guo and Hu, 2019)−PRePpsuedo-references, paraphrases•⊘Tokyo Metropolitan Univ. (Yoshimura et al., 2019)http://github.com/kokeman/PRePWMDOword mover distance•⊘Imperial College London (Chow et al., 2019a)−YiSi-0semantic similarity•⊘NRC (Lo, 2019)http://github.com/chikiulo/YiSiYiSi-1semantic similarity•⊘NRC (Lo, 2019)http://github.com/chikiulo/YiSiYiSi-1_srlsemantic similarity•⊘NRC (Lo, 2019)http://github.com/chikiulo/YiSiQE SystemsIBM1-morphemeLM log probs., ibm1 lexicon•⊘Dublin City University, ADAPT (Popovic, 2012)−IBM1-pos4gramLM log probs., ibm1 lexicon•⊘Dublin City University, ADAPT (Popovic, 2012)−LPcontextual word emb., MT log prob.yes•⊘Univ. of Tartu (Yankovskaya et al., 2019)−LASIMcontextual word embeddingsyes•⊘Univ. of Tartu (Yankovskaya et al., 2019)−UNI??•⊘??UNI+??•⊘??USFD??•⊘Univ. of Sheffield?USFD-TL??•⊘Univ. of Sheffield?YiSi-2semantic similarity•⊘NRC (Lo, 2019)http://github.com/chikiulo/YiSiYiSi-2_srlsemantic similarity•⊘NRC (Lo, 2019)http://github.com/chikiulo/YiSiTable 2: Participants of WMT19 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-levelevaluation. “⊘” indicates that the system-level scores are implied, simply taking arithmetic (macro-)average of segment-level scores. “−” indicates that the metricdidn’t participate the track (Seg/Sys-level). A metric is learned if it is trained on a QE or metric evaluation dataset (i.e. pretraining or parsers don’t count, buttraining on WMT 2017 metrics task data does). For the baseline metrics available in the Moses toolkit, paths are relative to http://github.com/moses-smt/mosesdecoder/.\x0c67smoothed version of BLEU for scoringat the segment-level. We used the stan-dard tokenizer script as available in Mosestoolkit for tokenization.chrF and chrF+. The metrics chrF andchrF+ (Popović, 2015, 2017) are com-puted using their original Python im-plementation,see Table 2.We ranchrF++.py with the parameters -nw 0 -b3 to obtain the chrF score and with-nw 1 -b 3 to obtain the chrF+ score.Note that chrF intentionally removes allspaces before matching the n-grams, deto-kenizing the segments but also concate-nating words.10sacreBLEU-BLEUandsacreBLEU-chrF. The metrics sacreBLEU-BLEUand sacreBLEU-chrF (Post, 2018a)are re-implementation of BLEU and chrFrespectively. We ran sacreBLEU-chrFwith the same parameters as chrF, buttheir scores are slightly different. The sig-nature strings produced by sacreBLEUfor BLEU and chrF respectively areBLEU+case.lc+lang.de-en+numrefs.1+smooth.exp+tok.intl+version.1.3.6andchrF3+case.mixed+lang.de-en+numchars.6+numrefs.1+space.False+tok.13a+version.1.3.6.The baselines serve in system and segment-level evaluations as customary: BLEU, TER,WER, PER, CDER, sacreBLEU-BLEUand sacreBLEU-chrF for system-level only;sentBLEU for segment-level only and chrFfor both.Chinese word segmentation is unfortunatelynot supported by the tokenization scripts men-tioned above. For scoring Chinese with base-line metrics, we thus pre-processed MT out-puts and reference translations with the scripttokenizeChinese.py11 by Shujian Huang,which separates Chinese characters from eachother and also from non-Chinese parts.10We originally planned to use the chrF implemen-tation which was recently made available in MosesScorer but it mishandles Unicode characters for now.11http://hdl.handle.net/11346/WMT17-TVXH4Submitted MetricsTable 2 lists the participants of the WMT19Shared Metrics Task, along with their metricsand links to the source code where available.We have collected 24 metrics from a total of 13research groups, with 10 reference-less “met-rics” submitted to the joint task “QE as a Met-rich” with WMT19 Quality Estimation Task.The rest of this section provides a brief sum-mary of all the metrics that participated.4.1BEERBEER (Stanojević and Sima’an, 2015) is atrained evaluation metric with a linear modelthat combines sub-word feature indicators(character n-grams) and global word order fea-tures (skip bigrams) to achieve a language ag-nostic and fast to compute evaluation metric.BEER has participated in previous years ofthe evaluation task.4.2BERTrBERTr (Mathur et al., 2019) uses contextualword embeddings to compare the MT outputwith the reference translation.The BERTr score of a translation is theaveragerecallscoreoveralltokens,us-ing a relaxed version of token matchingbased on BERT embeddings:namely, com-puting the maximum cosine similarity be-tweentheembeddingofareferenceto-kenagainstanytokenintheMTout-put.BERTr uses bert_base_uncased em-beddings for the to-English language pairs,and bert_base_multilingual_cased embed-dings for all other language pairs.4.3CharacTERCharacTER (Wang et al., 2016b,a), identi-cal to the 2016 setup, is a character-level met-ric inspired by the commonly applied transla-tion edit rate (TER). It is defined as the mini-mum number of character edits required to ad-just a hypothesis, until it completely matchesthe reference, normalized by the length of thehypothesis sentence. CharacTER calculatesthe character-level edit distance while per-forming the shift edit on word level. Unlikethe strict matching criterion in TER, a hy-pothesis word is considered to match a refer-ence word and could be shifted, if the edit dis-\x0c68tance between them is below a threshold value.The Levenshtein distance between the refer-ence and the shifted hypothesis sequence iscomputed on the character level. In addition,the lengths of hypothesis sequences instead ofreference sequences are used for normalizingthe edit distance, which effectively countersthe issue that shorter translations normallyachieve lower TER.Similarly to other character-level metrics,CharacTER is generally applied to non-tokenized outputs and references, which alsoholds for this year’s submission with one ex-ception.This year tokenization was carriedout for en-ru hypotheses and references be-fore calculating the scores, since this results inlarge improvements in terms of correlations.For other language pairs, no tokenizer wasused for pre-processing.4.4EEDEED (Stanchev et al., 2019) is a character-based metric, which builds upon CDER. Itis defined as the minimum number of opera-tions of an extension to the conventional editdistance containing a “jump” operation. Theedit distance operations (insertions, deletionsand substitutions) are performed at the char-acter level and jumps are performed when ablank space is reached. Furthermore, the cov-erage of multiple characters in the hypothesisis penalised by the introduction of a coveragepenalty. The sum of the length of the refer-ence and the coverage penalty is used as thenormalisation term.4.5ESIMEnhanced Sequential Inference Model (ESIM;Chen et al., 2017; Mathur et al., 2019) is aneural model proposed for Natural LanguageInference that has been adapted for MT evalu-ation. It uses cross-sentence attention and sen-tence matching heuristics to generate a repre-sentation of the translation and the reference,which is fed to a feedforward regressor. Themetric is trained on singly-annotated DirectAssessment data that has been collected forevaluating WMT systems: all WMT 2018 to-English data for the to-English language pairs,and all WMT 2018 data for all other languagepairs.4.6hLEPORb_baseline,hLEPORa_baselineThe submitted metric hLEPOR_baseline isa metric based on the factor combination oflength penalty, precision, recall, and positiondifference penalty.The weighted harmonicmean is applied to group the factors togetherwith tunable weight parameters. The system-level score is calculated with the same formulabut with each factor weighted using weight es-timated at system-level and not at segment-level.In this submitted baseline version, hLE-POR_baseline was not tuned for each lan-guage pair separately but the default weightswere applied across all submitted languagepairs. Further improvements can be achievedby tuning the weights according to the devel-opment data, adding morphological informa-tion and applying n-gram factor scores intoit (e.g. part-of-speech, n-gram precision andn-gram recall that were added into LEPORin WMT13.).The basic model factors andfurther development with parameters settingwere described in the paper (Han et al., 2012)and (Han et al., 2013).Forsentence-levelscore,onlyhLE-PORa_baseline was submitted with scorescalculated as the weighted harmonic meanof all the designed factors using defaultparameters.Forsystem-levelscore,bothhLEPORa_baselineandhLE-PORb_baselineweresubmitted,wherehLEPORa_baselineisthetheaveragescore of all sentence-level scores, and hLE-PORb_baseline is calculated via the samesentence-level hLEPOR equation but replac-ing each factor value with its system-levelcounterpart.4.7Meteor++_2.0 (syntax),Meteor++_2.0 (syntax+copy)Meteor++2.0 (Guo and Hu, 2019) isa metric based on Meteor (Denkowski andLavie, 2014) that takes syntactic-level para-phrase knowledge into consideration, whereparaphrases may sometimes be skip-grams.i.e.(protect...from, protect...against).Asthe original Meteor-based metrics only payattentiontoconsecutivestringmatching,\x0c69they perform badly when reference-hypothesispairs contain skip n-gram paraphrases. Me-teor++ 2.0 extracts the knowledge from theParaphrase Database (PPDB; Bannard andCallison-Burch, 2005) and integrates it intoMeteor-based metrics.4.8PRePPReP (Yoshimura et al., 2019) is a method forfiltering pseudo-references to achieve a goodmatch with a gold reference.At the beginning, the source sentence istranslated with some off-the-shelf MT sys-tems to create a set of pseudo-references.(Here the MT systems were Google Translateand Microsoft Bing Translator.) The pseudo-references are then filtered using BERT (De-vlin et al., 2019) fine-tuned on the MPRCcorpus (Dolan and Brockett, 2005), estimat-ing the probability of the paraphrase betweengold reference and pseudo-references. Thanksto the high quality of the underlying MT sys-tems, a large portion of their outputs is indeedconsidered as a valid paraphrase.The final metric score is calculated sim-ply with SentBLEU with these multiple ref-erences.4.9WMDOWMDO (Chow et al., 2019b) is a metric basedon distance between distributions in the se-mantic vector space. Matching in the seman-tic space has been investigated for translationevaluation, but the constraints of a transla-tion’s word order have not been fully explored.Building on the Word Mover’s Distance metricand various word embeddings, WMDO intro-duces a fragmentation penalty to account forfluency of a translation. This word order ex-tension is shown to perform better than stan-dard WMD, with promising results againstother types of metrics.4.10YiSi-0, YiSi-1, YiSi-1_srl, YiSi-2,YiSi-2_srlYiSi (Lo, 2019) is a unified semantic MT qual-ity evaluation and estimation metric for lan-guages with different levels of available re-sources.YiSi-1 is a MT evaluation metric that mea-sures the semantic similarity between a ma-chine translation and human references byaggregating the idf-weighted lexical semanticsimilarities based on the contextual embed-dings extracted from BERT and optionally in-corporating shallow semantic structures (de-noted as YiSi-1_srl).YiSi-0 is the degenerate version of YiSi-1that is ready-to-deploy to any language.Ituses longest common character substring tomeasure the lexical similarity.YiSi-2 is the bilingual, reference-less versionfor MT quality estimation, which uses the con-textual embeddings extracted from BERT toevaluate the crosslingual lexical semantic simi-larity between the input and MT output. LikeYiSi-1, YiSi-2 can exploit shallow semanticstructures as well (denoted as YiSi-2_srl).4.11QE SystemsIn addition to the submitted standard metrics,10 quality estimation systems were submittedto the “QE as a Metric” track. The submittedQE systems are evaluated in the same settingsas metrics to facilitate comparison. Their de-scriptions can be found in the Findings of theWMT 2019 Shared Task on Quality Estima-tion (Fonseca et al., 2019).5ResultsWe discuss system-level results for news tasksystems in Section 5.1. The segment-level re-sults are in Section 5.2.5.1System-Level EvaluationAs in previous years, we employ the Pearsoncorrelation (r) as the main evaluation measurefor system-level metrics. The Pearson correla-tion is as follows:r =∑ni=1(Hi −H)(Mi −M)√∑ni=1(Hi −H)2√∑ni=1(Mi −M)2(1)where Hi are human assessment scores of allsystems in a given translation direction, Miare the corresponding scores as predicted bya given metric.H and M are their means,respectively.Since some metrics, such as BLEU, aim toachieve a strong positive correlation with hu-man assessment, while error metrics, such asTER, aim for a strong negative correlation wecompare metrics via the absolute value |r| of a\x0c70de-enfi-engu-enkk-enlt-enru-enzh-enn16121111111415Correlation|r||r||r||r||r||r||r|BEER0.9060.9930.9520.9860.9470.9150.942BERTr0.9260.9840.9380.9900.9480.9710.974BLEU0.8490.9820.8340.9460.9610.8790.899CDER0.8900.9880.8760.9670.9750.8920.917CharacTER0.8980.9900.9220.9530.9550.9230.943chrF0.9170.9920.9550.9780.9400.9450.956chrF+0.9160.9920.9470.9760.9400.9450.956EED0.9030.9940.9760.9800.9290.9500.949ESIM0.9410.9710.8850.9860.9890.9680.988hLEPORa_baseline−−−0.975−−0.947hLEPORb_baseline−−−0.9750.906−0.947Meteor++_2.0(syntax)0.8870.9950.9090.9740.9280.9500.948Meteor++_2.0(syntax+copy)0.8960.9950.9000.9710.9270.9520.952NIST0.8130.9860.9300.9420.9440.9250.921PER0.8830.9910.9100.7370.9470.9220.952PReP0.5750.6140.7730.7760.4940.7820.592sacreBLEU.BLEU0.8130.9850.8340.9460.9550.8730.903sacreBLEU.chrF0.9100.9900.9520.9690.9350.9190.955TER0.8740.9840.8900.7990.9600.9170.840WER0.8630.9830.8610.7930.9610.9110.820WMDO0.8720.9870.9830.9980.9000.9420.943YiSi-00.9020.9930.9930.9910.9270.9580.937YiSi-10.9490.9890.9240.9940.9810.9790.979YiSi-1_srl0.9500.9890.9180.9940.9830.9780.977QE as a Metric:ibm1-morpheme0.3450.740−−0.487−−ibm1-pos4gram0.339−−−−−−LASIM0.247−−−−0.310−LP0.474−−−−0.488−UNI0.8460.930−−−0.805−UNI+0.8500.924−−−0.808−YiSi-20.7960.6420.5660.3240.4420.3390.940YiSi-2_srl0.804−−−−−0.947newstest2019Table 3: Absolute Pearson correlation of to-English system-level metrics with DA human assessment innewstest2019; correlations of metrics not significantly outperformed by any other for that language pairare highlighted in bold.\x0c71en-csen-deen-fien-guen-kken-lten-ruen-zhn1122121111121212Correlation|r||r||r||r||r||r||r||r|BEER0.9900.9830.9890.8290.9710.9820.9770.803BLEU0.8970.9210.9690.7370.8520.9890.9860.901CDER0.9850.9730.9780.8400.9270.9850.9930.905CharacTER0.9940.9860.9680.9100.9360.9540.9850.862chrF0.9900.9790.9860.8410.9720.9810.9430.880chrF+0.9910.9810.9860.8480.9740.9820.9500.879EED0.9930.9850.9870.8970.9790.9750.9670.856ESIM−0.9910.957−0.9800.9890.9890.931hLEPORa_baseline−−−0.8410.968−−−hLEPORb_baseline−−−0.8410.9680.980−−NIST0.8960.3210.9710.7860.9300.9930.9880.884PER0.9760.9700.9820.8390.9210.9850.9810.895sacreBLEU.BLEU0.9940.9690.9660.7360.8520.9860.9770.801sacreBLEU.chrF0.9830.9760.9800.8410.9670.9660.9850.796TER0.9800.9690.9810.8650.9400.9940.9950.856WER0.9820.9660.9800.8610.9390.9910.9940.875YiSi-00.9920.9850.9870.8630.9740.9740.9530.861YiSi-10.9620.9910.9710.9090.9850.9630.9920.951YiSi-1_srl−0.991−−−−−0.948QE as a Metric:ibm1-morpheme0.8710.8700.084−−0.810−−ibm1-pos4gram−0.393−−−−−−LASIM−0.871−−−−0.823−LP−0.569−−−−0.661−UNI0.0280.8410.907−−−0.919−UNI+−−−−−−0.918−USFD−0.224−−−−0.857−USFD-TL−0.091−−−−0.771−YiSi-20.3240.9240.6960.3140.3390.0550.7660.097YiSi-2_srl−0.936−−−−−0.118newstest2019Table 4: Absolute Pearson correlation of out-of-English system-level metrics with DA human assessmentin newstest2019; correlations of metrics not significantly outperformed by any other for that languagepair are highlighted in bold.\x0c72de-enfi-engu-enYiSi.1_srlYiSi.1ESIMBERTrchrFchrF.sacreBLEU.chrFBEEREEDYiSi.0CharacTERMeteor.._2.0.syntax.copy.CDERMeteor.._2.0.syntax.PERTERWMDOWERUNI.BLEUUNIsacreBLEU.BLEUNISTYiSi.2_srlYiSi.2PRePLP.1ibm1.morphemeibm1.pos4gramLASIMLASIMibm1.pos4gramibm1.morphemeLP.1PRePYiSi.2YiSi.2_srlNISTsacreBLEU.BLEUUNIBLEUUNI.WERWMDOTERPERMeteor.._2.0.syntax.CDERMeteor.._2.0.syntax.copy.CharacTERYiSi.0EEDBEERsacreBLEU.chrFchrF.chrFBERTrESIMYiSi.1YiSi.1_srlMeteor.._2.0.syntax.copy.Meteor.._2.0.syntax.EEDBEERYiSi.0chrF.chrFPERCharacTERsacreBLEU.chrFYiSi.1_srlYiSi.1CDERWMDONISTsacreBLEU.BLEUTERBERTrWERBLEUESIMUNIUNI.ibm1.morphemeYiSi.2PRePPRePYiSi.2ibm1.morphemeUNI.UNIESIMBLEUWERBERTrTERsacreBLEU.BLEUNISTWMDOCDERYiSi.1YiSi.1_srlsacreBLEU.chrFCharacTERPERchrFchrF.YiSi.0BEEREEDMeteor.._2.0.syntax.Meteor.._2.0.syntax.copy.YiSi.0WMDOEEDchrFsacreBLEU.chrFBEERchrF.BERTrNISTYiSi.1CharacTERYiSi.1_srlPERMeteor.._2.0.syntax.Meteor.._2.0.syntax.copy.TERESIMCDERWERBLEUsacreBLEU.BLEUPRePYiSi.2YiSi.2PRePsacreBLEU.BLEUBLEUWERCDERESIMTERMeteor.._2.0.syntax.copy.Meteor.._2.0.syntax.PERYiSi.1_srlCharacTERYiSi.1NISTBERTrchrF.BEERsacreBLEU.chrFchrFEEDWMDOYiSi.0kk-enlt-enru-enWMDOYiSi.1YiSi.1_srlYiSi.0BERTrESIMBEEREEDchrFchrF.hLEPORa_baselinehLEPORb_baselineMeteor.._2.0.syntax.Meteor.._2.0.syntax.copy.sacreBLEU.chrFCDERCharacTERBLEUsacreBLEU.BLEUNISTTERWERPRePPERYiSi.2YiSi.2PERPRePWERTERNISTsacreBLEU.BLEUBLEUCharacTERCDERsacreBLEU.chrFMeteor.._2.0.syntax.copy.Meteor.._2.0.syntax.hLEPORb_baselinehLEPORa_baselinechrF.chrFEEDBEERESIMBERTrYiSi.0YiSi.1_srlYiSi.1WMDOESIMYiSi.1_srlYiSi.1CDERBLEUWERTERsacreBLEU.BLEUCharacTERBERTrPERBEERNISTchrF.chrFsacreBLEU.chrFEEDMeteor.._2.0.syntax.Meteor.._2.0.syntax.copy.YiSi.0hLEPORb_baselineWMDOPRePibm1.morphemeYiSi.2YiSi.2ibm1.morphemePRePWMDOhLEPORb_baselineYiSi.0Meteor.._2.0.syntax.copy.Meteor.._2.0.syntax.EEDsacreBLEU.chrFchrFchrF.NISTBEERPERBERTrCharacTERsacreBLEU.BLEUTERWERBLEUCDERYiSi.1YiSi.1_srlESIMYiSi.1YiSi.1_srlBERTrESIMYiSi.0Meteor.._2.0.syntax.copy.Meteor.._2.0.syntax.EEDchrFchrF.WMDONISTCharacTERPERsacreBLEU.chrFTERBEERWERCDERBLEUsacreBLEU.BLEUUNI.UNIPRePLP.1YiSi.2LASIMLASIMYiSi.2LP.1PRePUNIUNI.sacreBLEU.BLEUBLEUCDERWERBEERTERsacreBLEU.chrFPERCharacTERNISTWMDOchrF.chrFEEDMeteor.._2.0.syntax.Meteor.._2.0.syntax.copy.YiSi.0ESIMBERTrYiSi.1_srlYiSi.1zh-enen-csen-deESIMYiSi.1YiSi.1_srlBERTrchrF.chrFsacreBLEU.chrFPERMeteor.._2.0.syntax.copy.EEDMeteor.._2.0.syntax.hLEPORb_baselinehLEPORa_baselineYiSi.2_srlWMDOCharacTERBEERYiSi.2YiSi.0NISTCDERsacreBLEU.BLEUBLEUTERWERPRePPRePWERTERBLEUsacreBLEU.BLEUCDERNISTYiSi.0YiSi.2BEERCharacTERWMDOYiSi.2_srlhLEPORa_baselinehLEPORb_baselineMeteor.._2.0.syntax.EEDMeteor.._2.0.syntax.copy.PERsacreBLEU.chrFchrFchrF.BERTrYiSi.1_srlYiSi.1ESIMsacreBLEU.BLEUCharacTEREEDYiSi.0chrF.BEERchrFCDERsacreBLEU.chrFWERTERPERYiSi.1BLEUNISTibm1.morphemeYiSi.2UNIUNIYiSi.2ibm1.morphemeNISTBLEUYiSi.1PERTERWERsacreBLEU.chrFCDERchrFBEERchrF.YiSi.0EEDCharacTERsacreBLEU.BLEUESIMYiSi.1YiSi.1_srlCharacTEREEDYiSi.0BEERchrF.chrFsacreBLEU.chrFCDERPERsacreBLEU.BLEUTERWERYiSi.2_srlYiSi.2BLEULASIMibm1.morphemeUNILP.1ibm1.pos4gramNISTUSFDUSFD.TLUSFD.TLUSFDNISTibm1.pos4gramLP.1UNIibm1.morphemeLASIMBLEUYiSi.2YiSi.2_srlWERTERsacreBLEU.BLEUPERCDERsacreBLEU.chrFchrFchrF.BEERYiSi.0EEDCharacTERYiSi.1_srlYiSi.1ESIMen-fien-guen-kkBEEREEDYiSi.0chrFchrF.PERTERsacreBLEU.chrFWERCDERYiSi.1NISTBLEUCharacTERsacreBLEU.BLEUESIMUNIYiSi.2ibm1.morphemeibm1.morphemeYiSi.2UNIESIMsacreBLEU.BLEUCharacTERBLEUNISTYiSi.1CDERWERsacreBLEU.chrFTERPERchrF.chrFYiSi.0EEDBEERCharacTERYiSi.1EEDTERYiSi.0WERchrF.sacreBLEU.chrFchrFhLEPORa_baselinehLEPORb_baselineCDERPERBEERNISTBLEUsacreBLEU.BLEUYiSi.2YiSi.2sacreBLEU.BLEUBLEUNISTBEERPERCDERhLEPORb_baselinehLEPORa_baselinechrFsacreBLEU.chrFchrF.WERYiSi.0TEREEDYiSi.1CharacTERYiSi.1ESIMEEDchrF.YiSi.0chrFBEERhLEPORa_baselinehLEPORb_baselinesacreBLEU.chrFTERWERCharacTERNISTCDERPERsacreBLEU.BLEUBLEUYiSi.2YiSi.2BLEUsacreBLEU.BLEUPERCDERNISTCharacTERWERTERsacreBLEU.chrFhLEPORb_baselinehLEPORa_baselineBEERchrFYiSi.0chrF.EEDESIMYiSi.1en-lten-ruen-zhTERNISTWERESIMBLEUsacreBLEU.BLEUCDERPERBEERchrF.chrFhLEPORb_baselineEEDYiSi.0sacreBLEU.chrFYiSi.1CharacTERibm1.morphemeYiSi.2YiSi.2ibm1.morphemeCharacTERYiSi.1sacreBLEU.chrFYiSi.0EEDhLEPORb_baselinechrFchrF.BEERPERCDERsacreBLEU.BLEUBLEUESIMWERNISTTERTERWERCDERYiSi.1ESIMNISTBLEUCharacTERsacreBLEU.chrFPERsacreBLEU.BLEUBEEREEDYiSi.0chrF.chrFUNIUNI.USFDLASIMUSFD.TLYiSi.2LP.1LP.1YiSi.2USFD.TLLASIMUSFDUNI.UNIchrFchrF.YiSi.0EEDBEERsacreBLEU.BLEUPERsacreBLEU.chrFCharacTERBLEUNISTESIMYiSi.1CDERWERTERYiSi.1YiSi.1_srlESIMCDERBLEUPERNISTchrFchrF.WERCharacTERYiSi.0EEDTERBEERsacreBLEU.BLEUsacreBLEU.chrFYiSi.2_srlYiSi.2YiSi.2YiSi.2_srlsacreBLEU.chrFsacreBLEU.BLEUBEERTEREEDYiSi.0CharacTERWERchrF.chrFNISTPERBLEUCDERESIMYiSi.1_srlYiSi.1Figure 1: System-level metric significance test results for DA human assessment for into English andout-of English language pairs (newstest2019): Green cells denote a statistically significant increase incorrelation with human assessment for the metric in a given row over the metric in a given columnaccording to Williams test.\x0c73given metric’s correlation with human assess-ment.5.1.1System-Level ResultsTables 3, 4 and 5 provide the system-level cor-relations of metrics evaluating translation ofnewstest2019. The underlying texts are partof the WMT19 News Translation test set (new-stest2019) and the underlying MT systems areall MT systems participating in the WMT19News Translation Task.As recommended by Graham and Bald-win (2014), we employ Williams significancetest (Williams, 1959) to identify differencesin correlation that are statistically significant.Williams test is a test of significance of a dif-ference in dependent correlations and there-fore suitable for evaluation of metrics. Corre-lations not significantly outperformed by anyother metric for the given language pair arehighlighted in bold in Tables 3, 4 and 5.Since pairwise comparisons of metrics maybe also of interest, e.g. to learn which metricssignificantly outperform the most widely em-ployed metric BLEU, we include significancetest results for every competing pair of metricsincluding our baseline metrics in Figure 1 andFigure 2.This year, the increased number of systemsparticipating in the news tasks has provided alarger sample of system scores for testing met-rics.Since we already have sufficiently con-clusive results on genuine MT systems, we donot need to generate hybrid system results asin Graham and Liu (2016) and past metricstasks.5.2Segment-Level EvaluationSegment-level evaluation relies on the man-ual judgements collected in the News Trans-lation Task evaluation.This year, again wewere unable to follow the methodology out-lined in Graham et al. (2015) for evaluation ofsegment-level metrics because the sampling ofsentences did not provide sufficient number ofassessments of the same segment. We there-fore convert pairs of DA scores for compet-ing translations to daRR better/worse prefer-ences as described in Section 2.3.2.We measure the quality of metrics’ segment-level scores against the daRR golden truth us-ing a Kendall’s Tau-like formulation, which isan adaptation of the conventional Kendall’sTau coefficient.Since we do not have a to-tal order ranking of all translations, it is notpossible to apply conventional Kendall’s Tau(Graham et al., 2015).Our Kendall’s Tau-like formulation, τ, is asfollows:τ = |Concordant| −|Discordant||Concordant| + |Discordant|(2)where Concordant is the set of all human com-parisons for which a given metric suggests thesame order and Discordant is the set of allhuman comparisons for which a given metricdisagrees. The formula is not specific with re-spect to ties, i.e. cases where the annotationsays that the two outputs are equally good.The way in which ties (both in human andmetric judgement) were incorporated in com-puting Kendall τ has changed across the yearsof WMT Metrics Tasks. Here we adopt theversion used in WMT17 daRR evaluation.For a detailed discussion on other options, seealso Macháček and Bojar (2014).Whether or not a given comparison of a pairof distinct translations of the same source in-put, s1 and s2, is counted as a concordant(Conc) or disconcordant (Disc) pair is definedby the following matrix:Metrics1 < s2s1 = s2s1 > s2Humans1 < s2ConcDiscDiscs1 = s2−−−s1 > s2DiscDiscConcIn the notation of Macháček and Bojar(2014), this corresponds to the setup used inWMT12 (with a different underlying methodof manual judgements, RR):MetricWMT12<=>Human<1-1-1=XXX>-1-11The key differences between the evaluationused in WMT14–WMT16 and evaluation usedin WMT17–WMT19 were (1) the move fromRR to daRR and (2) the treatment of ties. Inthe years 2014-2016, ties in metrics scores werenot penalized. With the move to daRR, wherethe quality of the two candidate translations\x0c74de-csde-frfr-den111110Correlation|r||r||r|BEER0.9780.9410.848BLEU0.9410.8910.864CDER0.8640.9490.852CharacTER0.9650.9280.849chrF0.9740.9310.864chrF+0.9720.9360.848EED0.9820.9400.851ESIM0.9800.9500.942hLEPORa_baseline0.9410.814−hLEPORb_baseline0.9590.814−NIST0.9540.9160.862PER0.8750.8570.899sacreBLEU-BLEU0.8690.8910.869sacreBLEU-chrF0.9750.9520.882TER0.8900.9560.895WER0.8720.9560.894YiSi-00.9780.9520.820YiSi-10.9730.9690.908YiSi-1_srl−−0.912QE as a Metric:ibm1-morpheme0.3550.5090.625ibm1-pos4gram−0.0850.478YiSi-20.6060.7210.530newstest2019Table 5: Absolute Pearson correlation of system-level metrics for language pairs not involving Englishwith DA human assessment in newstest2019; correlations of metrics not significantly outperformed byany other for that language pair are highlighted in bold.de-csde-frfr-deEEDESIMYiSi.0BEERsacreBLEU.chrFchrFYiSi.1chrF.CharacTERhLEPORb_baselineNISTBLEUhLEPORa_baselineTERPERWERsacreBLEU.BLEUCDERYiSi.2ibm1.morphemeibm1.morphemeYiSi.2CDERsacreBLEU.BLEUWERPERTERhLEPORa_baselineBLEUNISThLEPORb_baselineCharacTERchrF.YiSi.1chrFsacreBLEU.chrFBEERYiSi.0ESIMEEDYiSi.1WERTERYiSi.0sacreBLEU.chrFESIMCDERBEEREEDchrF.chrFCharacTERNISTBLEUsacreBLEU.BLEUPERhLEPORb_baselinehLEPORa_baselineYiSi.2ibm1.morphemeibm1.pos4gramibm1.pos4gramibm1.morphemeYiSi.2hLEPORa_baselinehLEPORb_baselinePERsacreBLEU.BLEUBLEUNISTCharacTERchrFchrF.EEDBEERCDERESIMsacreBLEU.chrFYiSi.0TERWERYiSi.1ESIMYiSi.1_srlYiSi.1PERTERWERsacreBLEU.chrFsacreBLEU.BLEUchrFBLEUNISTCDEREEDCharacTERBEERchrF.YiSi.0ibm1.morphemeYiSi.2ibm1.pos4gramibm1.pos4gramYiSi.2ibm1.morphemeYiSi.0chrF.BEERCharacTEREEDCDERNISTBLEUchrFsacreBLEU.BLEUsacreBLEU.chrFWERTERPERYiSi.1YiSi.1_srlESIMFigure 2: System-level metric significance test results for DA human assessment in newstest2019 forGerman to Czech, German to French and French to German; green cells denote a statistically significantincrease in correlation with human assessment for the metric in a given row over the metric in a givencolumn according to Williams test.\x0c75de-enfi-engu-enkk-enlt-enru-enzh-enHuman EvaluationdaRRdaRRdaRRdaRRdaRRdaRRdaRRn85,36538,30731,13927,09421,86246,17231,070BEER0.1280.2830.2600.4210.3150.1890.371BERTr0.1420.3310.2910.4210.3530.1950.399CharacTER0.1010.2530.1900.3400.2540.1550.337chrF0.1220.2860.2560.3890.3010.1800.371chrF+0.1250.2890.2570.3940.3030.1820.374EED0.1200.2810.2640.3920.2980.1760.376ESIM0.1670.3370.3030.4350.3590.2010.396hLEPORa_baseline−−−0.372−−0.339Meteor++_2.0(syntax)0.0840.2740.2370.3950.2910.1560.370Meteor++_2.0(syntax+copy)0.0940.2730.2440.4020.2870.1630.367PReP0.0300.1970.1920.3860.1930.1240.267sentBLEU0.0560.2330.1880.3770.2620.1250.323WMDO0.0960.2810.2600.4200.3000.1620.362YiSi-00.1170.2710.2630.4020.2890.1780.355YiSi-10.1640.3470.3120.4400.3760.2170.426YiSi-1_srl0.1990.3460.3060.4420.3800.2220.431QE as a Metric:ibm1-morpheme−0.0740.009−−0.069−−ibm1-pos4gram−0.153−−−−−−LASIM−0.024−−−−0.022−LP−0.096−−−−−0.035−UNI0.0220.202−−−0.084−UNI+0.0150.211−−−0.089−YiSi-20.0680.126−0.0010.0960.0750.0530.253YiSi-2_srl0.068−−−−−0.246newstest2019Table 6: Segment-level metric results for to-English language pairs in newstest2019: absolute Kendall’sTau formulation of segment-level metric scores with DA scores; correlations of metrics not significantlyoutperformed by any other for that language pair are highlighted in bold.\x0c76en-csen-deen-fien-guen-kken-lten-ruen-zhHuman EvaluationdaRRdaRRdaRRdaRRdaRRdaRRdaRRdaRRn27,17899,84031,82011,35518,17217,40124,33418,658BEER0.4430.3160.5140.5370.5160.4410.5420.232CharacTER0.3490.2640.4040.5000.3510.3110.4320.094chrF0.4550.3260.5140.5340.4790.4460.5390.301chrF+0.4580.3270.5140.5380.4910.4480.5430.296EED0.4310.3150.5080.5680.5180.4250.5460.257ESIM−0.3290.511−0.5100.4280.5720.339hLEPORa_baseline−−−0.4630.390−−−sentBLEU0.3670.2480.3960.4650.3920.3340.4690.270YiSi-00.4060.3040.4830.5390.4940.4020.5350.266YiSi-10.4750.3510.5370.5510.5460.4700.5850.355YiSi-1_srl−0.368−−−−−0.361QE as a Metric:ibm1-morpheme−0.135−0.003−0.005−−−0.165−−ibm1-pos4gram−−0.123−−−−−−LASIM−0.147−−−−−0.24−LP−−0.119−−−−−0.158−UNI0.0600.1290.351−−−0.226−UNI+−−−−−−0.222−USFD−−0.029−−−−0.136−USFD-TL−−0.037−−−−0.191−YiSi-20.0690.2120.2390.1470.1870.003−0.1550.044YiSi-2_srl−0.236−−−−−0.034newstest2019Table 7:Segment-level metric results for out-of-English language pairs in newstest2019:absoluteKendall’s Tau formulation of segment-level metric scores with DA scores; correlations of metrics notsignificantly outperformed by any other for that language pair are highlighted in bold.de-csde-frfr-deHuman EvaluationdaRRdaRRdaRRn35,7934,8621,369BEER0.3370.2930.265CharacTER0.2320.2510.224chrF0.3260.2840.275chrF+0.3260.2840.278EED0.3450.3010.267ESIM0.3310.2900.289hLEPORa_baseline0.2070.239−sentBLEU0.2030.2350.179YiSi-00.3310.2960.277YiSi-10.3760.3490.310YiSi-1_srl−−0.299QE as a Metric:ibm1-morpheme0.048−0.013−0.053ibm1-pos4gram−−0.074−0.097YiSi-20.1990.1860.066newstest2019Table 8: Segment-level metric results for languagepairs not involving English in newstest2019: ab-solute Kendall’s Tau formulation of segment-levelmetric scores with DA scores; correlations of met-rics not significantly outperformed by any other forthat language pair are highlighted in bold.is deemed substantially different and no tiesin human judgements arise, it makes sense topenalize ties in metrics’ predictions in order topromote discerning metrics.Note that the penalization of ties makes ourevaluation asymmetric, dependent on whetherthe metric predicted the tie for a pair wherehumans predicted <, or >. It is now impor-tant to interpret the meaning of the compar-ison identically for humans and metrics. Forerror metrics, we thus reverse the sign of themetric score prior to the comparison with hu-man scores: higher scores have to indicate bet-ter translation quality. In WMT19, the origi-nal authors did this for CharacTER.To summarize, the WMT19 Metrics Taskfor segment-level evaluation:• ensures that error metrics are first con-verted to the same orientation as the hu-man judgements, i.e.higher score indi-cating higher translation quality,• excludes all human ties (this is alreadyimplied by the construction of daRRfrom DA judgements),\x0c77de-enfi-engu-enYiSi.1_srlESIMYiSi.1BERTrBEERchrF.chrFEEDYiSi.0CharacTERWMDOMeteor.._2.0.syntax.copy.Meteor.._2.0.syntax.YiSi.2YiSi.2_srlsentBLEUPRePUNIUNI.LASIMibm1.morphemeLPibm1.pos4gramibm1−pos4gramLPibm1−morphemeLASIMUNI+UNIPRePsentBLEUYiSi−2_srlYiSi−2Meteor++_2.0(syntax)Meteor++_2.0(syntax+copy)WMDOCharacTERYiSi−0EEDchrFchrF+BEERBERTrYiSi−1ESIMYiSi−1_srlYiSi.1YiSi.1_srlESIMBERTrchrF.chrFBEERWMDOEEDMeteor.._2.0.syntax.Meteor.._2.0.syntax.copy.YiSi.0CharacTERsentBLEUUNI.UNIPRePYiSi.2ibm1.morphemeibm1−morphemeYiSi−2PRePUNIUNI+sentBLEUCharacTERYiSi−0Meteor++_2.0(syntax+copy)Meteor++_2.0(syntax)EEDWMDOBEERchrFchrF+BERTrESIMYiSi−1_srlYiSi−1YiSi.1YiSi.1_srlESIMBERTrEEDYiSi.0WMDOBEERchrF.chrFMeteor.._2.0.syntax.copy.Meteor.._2.0.syntax.PRePCharacTERsentBLEUYiSi.2YiSi−2sentBLEUCharacTERPRePMeteor++_2.0(syntax)Meteor++_2.0(syntax+copy)chrFchrF+BEERWMDOYiSi−0EEDBERTrESIMYiSi−1_srlYiSi−1kk-enlt-enru-enYiSi.1_srlYiSi.1ESIMBERTrBEERWMDOYiSi.0Meteor.._2.0.syntax.copy.Meteor.._2.0.syntax.chrF.EEDchrFPRePsentBLEUhLEPORa_baselineCharacTERYiSi.2YiSi−2CharacTERhLEPORa_baselinesentBLEUPRePchrFEEDchrF+Meteor++_2.0(syntax)Meteor++_2.0(syntax+copy)YiSi−0WMDOBEERBERTrESIMYiSi−1YiSi−1_srlYiSi.1_srlYiSi.1ESIMBERTrBEERchrF.chrFWMDOEEDMeteor.._2.0.syntax.YiSi.0Meteor.._2.0.syntax.copy.sentBLEUCharacTERPRePYiSi.2ibm1.morphemeibm1−morphemeYiSi−2PRePCharacTERsentBLEUMeteor++_2.0(syntax+copy)YiSi−0Meteor++_2.0(syntax)EEDWMDOchrFchrF+BEERBERTrESIMYiSi−1YiSi−1_srlYiSi.1_srlYiSi.1ESIMBERTrBEERchrF.chrFYiSi.0EEDMeteor.._2.0.syntax.copy.WMDOMeteor.._2.0.syntax.CharacTERsentBLEUPRePUNI.UNIYiSi.2LASIMLPLPLASIMYiSi−2UNIUNI+PRePsentBLEUCharacTERMeteor++_2.0(syntax)WMDOMeteor++_2.0(syntax+copy)EEDYiSi−0chrFchrF+BEERBERTrESIMYiSi−1YiSi−1_srlzh-enen-csen-deYiSi.1_srlYiSi.1BERTrESIMEEDchrF.BEERchrFMeteor.._2.0.syntax.Meteor.._2.0.syntax.copy.WMDOYiSi.0hLEPORa_baselineCharacTERsentBLEUPRePYiSi.2YiSi.2_srlYiSi−2_srlYiSi−2PRePsentBLEUCharacTERhLEPORa_baselineYiSi−0WMDOMeteor++_2.0(syntax+copy)Meteor++_2.0(syntax)chrFBEERchrF+EEDESIMBERTrYiSi−1YiSi−1_srlYiSi.1chrF.chrFBEEREEDYiSi.0sentBLEUCharacTERYiSi.2UNIibm1.morphemeibm1−morphemeUNIYiSi−2CharacTERsentBLEUYiSi−0EEDBEERchrFchrF+YiSi−1YiSi.1_srlYiSi.1ESIMchrF.chrFBEEREEDYiSi.0CharacTERsentBLEUYiSi.2_srlYiSi.2LASIMUNIibm1.morphemeUSFDUSFD.TLLPibm1.pos4gramibm1−pos4gramLPUSFD−TLUSFDibm1−morphemeUNILASIMYiSi−2YiSi−2_srlsentBLEUCharacTERYiSi−0EEDBEERchrFchrF+ESIMYiSi−1YiSi−1_srlen-fien-guen-kkYiSi.1chrF.BEERchrFESIMEEDYiSi.0CharacTERsentBLEUUNIYiSi.2ibm1.morphemeibm1−morphemeYiSi−2UNIsentBLEUCharacTERYiSi−0EEDESIMchrFBEERchrF+YiSi−1EEDYiSi.1YiSi.0chrF.BEERchrFCharacTERsentBLEUhLEPORa_baselineYiSi.2YiSi−2hLEPORa_baselinesentBLEUCharacTERchrFBEERchrF+YiSi−0YiSi−1EEDYiSi.1EEDBEERESIMYiSi.0chrF.chrFsentBLEUhLEPORa_baselineCharacTERYiSi.2YiSi−2CharacTERhLEPORa_baselinesentBLEUchrFchrF+YiSi−0ESIMBEEREEDYiSi−1en-lten-ruen-zhYiSi.1chrF.chrFBEERESIMEEDYiSi.0sentBLEUCharacTERYiSi.2ibm1.morphemeibm1−morphemeYiSi−2CharacTERsentBLEUYiSi−0EEDESIMBEERchrFchrF+YiSi−1YiSi.1ESIMEEDchrF.BEERchrFYiSi.0sentBLEUCharacTERUNIUNI.USFD.TLUSFDYiSi.2LPLASIMLASIMLPYiSi−2USFDUSFD−TLUNI+UNICharacTERsentBLEUYiSi−0chrFBEERchrF+EEDESIMYiSi−1YiSi.1_srlYiSi.1ESIMchrFchrF.sentBLEUYiSi.0EEDBEERCharacTERYiSi.2YiSi.2_srlYiSi−2_srlYiSi−2CharacTERBEEREEDYiSi−0sentBLEUchrF+chrFESIMYiSi−1YiSi−1_srlFigure 3: daRR segment-level metric significance test results for into English and out-of English languagepairs (newstest2019): Green cells denote a significant win for the metric in a given row over the metricin a given column according bootstrap resampling.\x0c78de-csde-frfr-deYiSi.1EEDBEERESIMYiSi.0chrF.chrFCharacTERhLEPORa_baselinesentBLEUYiSi.2ibm1.morphemeibm1−morphemeYiSi−2sentBLEUhLEPORa_baselineCharacTERchrFchrF+YiSi−0ESIMBEEREEDYiSi−1YiSi.1EEDYiSi.0BEERESIMchrF.chrFCharacTERhLEPORa_baselinesentBLEUYiSi.2ibm1.morphemeibm1.pos4gramibm1−pos4gramibm1−morphemeYiSi−2sentBLEUhLEPORa_baselineCharacTERchrFchrF+ESIMBEERYiSi−0EEDYiSi−1YiSi.1YiSi.1_srlESIMchrF.YiSi.0chrFEEDBEERCharacTERsentBLEUYiSi.2ibm1.morphemeibm1.pos4gramibm1−pos4gramibm1−morphemeYiSi−2sentBLEUCharacTERBEEREEDchrFYiSi−0chrF+ESIMYiSi−1_srlYiSi−1Figure 4: daRR segment-level metric significance test results for German to Czech, German to Frenchand French to German (newstest2019): Green cells denote a significant win for the metric in a given rowover the metric in a given column according bootstrap resampling.• counts metric’s ties as a Discordant pairs.We employ bootstrap resampling (Koehn,2004; Graham et al., 2014b) to estimate con-fidence intervals for our Kendall’s Tau for-mulation, and metrics with non-overlapping95% confidence intervals are identified as hav-ing statistically significant difference in perfor-mance.5.2.1Segment-Level ResultsResults of the segment-level human evaluationfor translations sampled from the News Trans-lation Task are shown in Tables 6, 7 and 8,where metric correlations not significantly out-performed by any other metric are highlightedin bold. Head-to-head significance test resultsfor differences in metric performance are in-cluded in Figures 3 and 4.6DiscussionThis year, human data was collected fromreference-basedevaluations(or“monolin-gual”)andreference-freeevaluations(or“bilingual”).The reference-based (mono-lingual) evaluations were obtained with thehelpofanonymouscrowdsourcing,whilethe reference-less (bilingual) evaluations weremainly from MT researchers who committedtheir time contribution to the manual evalua-tion for each submitted system.6.1Stability across MT SystemsThe observed performance of metrics dependson the underlying texts and systems that par-ticipate in the News Translation Task (see Sec-tion 2). For the strongest MT systems, distin-guishing which system outputs are better is468101214161820sacreBLEU-BLEUFigure 5: Pearson correlations of sacreBLEU-BLEU for English-German system-level evalua-tion for all systems (left) down to only top 4 sys-tems (right). The y-axis spans from -1 to +1, base-line metrics for the language pair in grey.hard, even for human assessors. On the otherhand, if the systems are spread across a wideperformance range, it will be easier for metricsto correlate with human judgements.To provide a more reliable view, we createdplots of Pearson correlation when the under-lying set of MT systems is reduced to top nones. One sample such plot is in Figure 5, alllanguage pairs and most of the metrics are inAppendix A.As the plot documents, the official correla-tions reported in Tables 3 to 5 can lead towrong conclusions. sacreBLEU-BLEU cor-relates at .969 when all systems are considered,but as we start considering only the top n sys-tems, the correlation falls relatively quickly.With 10 systems, we are below .5 and whenonly the top 6 or 4 systems are considered,the correlation falls even to the negave val-ues.Note that correlations point estimates(the value in the y-axis) become noiser withthe decreasing number of the underlying MTsystems.Figure 6 explains the situation and illus-\x0c79 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5-2-1.5-1-0.5 0 0.5SacreBLEU-BLEUDATop 4Top 6Top 8Top 10Top 12Top 15All systemsFigure 6trates the sensitivity of the observed correla-tions to the exact set of systems. On the fullset of systems, the single outlier (the worst-performing system called en_de_task) helpsto achieve a great positive correlation.Themajority of MT systems however form a cloudwith Pearson correlation around .5 and the top4 systems actually exhibit a negative corre-lation of the human score and sacreBLEU-BLEU.In Appendix A, baseline metrics are plottedin grey in all the plots, so that their trends canbe observed jointly. In general, most baselineshave similar correlations, as most baselines usesimilar features (n-gram or word-level features,with the exception of chrF). In a number oflanguage pairs (de-en, de-fr, en-de, en-kk, lt-en, ru-en, zh-en), baseline correlations tend to-wards 0 (no correlation) or even negative Pear-son correlation. For a widely applied metricsuch as sacreBLEU-BLEU, our analysis re-veals weak correlation in comparing top state-of-the-art systems in these language pairs, es-pecially in en-de, de-en, ru-en, and zh-en.We will restrict our analysis to those lan-guage pairs where the baseline metrics have anobvious downward trend (de-en, de-fr, en-de,en-kk, lt-en, ru-en, zh-en). Examining the top-n correlation in the submitted metrics (not in-cluding QE systems), most metrics show thesame degredation in correlation as the base-lines. We note BERTr as the one exceptionconsistently degrading less and retaining pos-itive correlation compared to other submittedmetrics and baselines, in the language pairswhere it participated.For QE systems, we noticed that in some in-stances, QE systems have upward correlationtrends when other metrics and baselines havedownward trends. For instance, LP, UNI, andUNI+ in the de-en language pair, YiSi-2 inen-kk, and UNI and UNI+ in ru-en. Theseresults suggest that QE systems such as UNIand UNI+ perform worse on judging systemsof wide ranging quality, but better for top per-forming systems, or perhaps for systems closerin quality.If our method of human assessment is sound,we should believe that BLEU, a widely ap-plied metric, is no longer a reliable metric forjudging our best systems.Future investiga-tions are needed to understand when BLEUapplies well, and why BLEU is not effectivefor output from our state of the art models.Metrics and QE systems such as BERTr,ESIM, YiSi that perform well at judgingour best systems often use more semanticfeatures compared to our n-gram/char-grambased baselines. Future metrics may want toexplore a) whether semantic features such ascontextual word embeddings are achieving se-mantic understanding and b) whether seman-tic understanding is the true source of a met-ric’s performance gains.It should be noted that some language pairsdo not show the strong degrading pattern withtop-n systems this year, for instance en-cs, en-gu, en-ru, or kk-en. English-Chinese is partic-ularly interesting because we see a clear trendtowards better correlations as we reduce theset of underlying systems to the top scoringones.6.2Overall Metric Performance6.2.1System-Level EvaluationIn system-level evaluation, the series of YiSimetrics achieve the highest correlations in sev-eral language pairs and it is not significantlyoutperformed by any other metrics (denotedas a “win” in the following) for almost all lan-guage pairs.The new metric ESIM performs best on 5language languages (18 language pairs) andobtains 11 “wins” out of 16 language pairs inwhich ESIM participated.The metric EED performs better for lan-guage pairs out-of English and excluding En-\x0c80glish compared to into-English language pairs,achieving 7 out of 11 “wins” there.6.2.2Segment-Level EvaluationFor segment-level evaluation, most languagepairs are quite discerning, with only one ortwo metrics taking the “winner” position (ofnot being significantly surpassed by others).Only French-German differs, with all metricsperforming similarly except the significantlyworse sentBLEU.YiSi-1_srl stands out as the “winner” forall language pairs in which it participated.The excluded language pairs were probablydue to the lack of semantic information re-quired by YiSi-1_srl.YiSi-1 participatedall language pairs and its correlations are com-parable with those of YiSi-1_srl.ESIM obtain 6 “winners” out of all 18 lan-guages pairs.Both YiSi and ESIM are based on neu-ral networks (YiSi via word and phrase em-beddings, as well as other types of availableresources, ESIM via sentence embeddings).This is a confirmation of a trend observed lastyear.6.2.3QE Systems as MetricsGenerally,correlationsforthestandardreference-based metrics are obviously betterthan those in “QE as a Metric” track, bothwhen using monolingual and bilingual goldentruth.In system-level evaluation, correlations for“QE as a Metric” range from 0.028 to 0.947across all language pairs and all metrics butthey are very unstable.Even for a singlemetric, take UNI for example, the correla-tions range from 0.028 to 0.930 across languagepairs.In segment-level evaluation, correlations forQE metrics range from -0.153 to 0.351 acrossall language pairs and show the same instabil-ity across language pairs for a given metric.In either case, we do not see any patternthat could explain the behaviour, e.g. whetherthe manual evaluation was monolingual orbilingual, or the characteristics of the givenlanguage pair.6.3Dependence on ImplementationAs it already happened in the past, we hadmultiple implementations for some metrics,BLEU and chrF in particular.The detailed configuration of BLEU andsacreBLEU-BLEU differ and hence theirscores and correlation results are different.chrF and sacreBLEU-chrF use the sameparameters and should thus deliver the samescores but we still observe some differences,leading to different correlations. For instancefor German-French Pearson correlation, chrFobtains 0.931 (no win) but sacreBLEU-chrF reaches 0.952, tying for a win with othermetrics.We thus fully support the call for clarity byPost (2018b) and invite authors of metrics toinclude their implementations either in Mosesscorer or sacreBLEU to achieve a long-termassessment of their metric.7ConclusionThis paper summarizes the results of WMT19shared task in machine translation evaluation,the Metrics Shared Task. Participating met-rics were evaluated in terms of their correla-tion with human judgement at the level ofthe whole test set (system-level evaluation),as well as at the level of individual sentences(segment-level evaluation).We reported scores for standard metrics re-quiring the reference as well as quality estima-tion systems which took part in the track “QEas a metric”, joint with the Quality Estimationtask.For system-level, best metrics reach over0.95 Pearson correlation or better across sev-eral language pairs.As expected, QE sys-tems are visibly in all language pairs but theycan also reach high system-level correlations,up to .947 (Chinese-English) or .936 (English-German) by YiSi-1_srl or over .9 for multi-ple language pairs by UNI.An important caveat is that the correlationsare heavily affected by the underlying set ofMT systems.We explored this by reducingthe set of systems to top-n ones for various nsand found out that for many language pairs,system-level correlations are much worse whenbased on only the better performing systems.With both good and bad MT systems partic-\x0c81ipating in the news task, the metrics resultscan be overly optimistic compared to what weget when evaluating state-of-the-art systems.In terms of segment-level Kendall’s τ re-sults, the standard metrics correlations variedbetween 0.03 and 0.59, and QE systems ob-tained even negative correlations.The results confirm the observation from thelast year, namely metrics based on word orsentence-level embeddings (YiSi and ESIM),achieve the highest performance.AcknowledgmentsResults in this shared task would not be pos-sible without tight collaboration with organiz-ers of the WMT News Translation Task. Wewould like to thank Marcin Junczys-Dowmuntfor the suggestion to examine metrics perfor-mance across varying subsets of MT systems,as we did in Appendix A.This study was supported in parts by thegrants 19-26934X (NEUREM3) of the CzechScience Foundation, ADAPT Centre for Dig-ital Content Technology (www.adaptcentre.ie) at Dublin City University funded un-der the SFI Research Centres Programme(Grant 13/RC/2106) co-funded under theEuropean Regional Development Fund, andCharles University Research Programme “Pro-gres” Q18+Q48.ReferencesColin Bannard and Chris Callison-Burch. 2005.Paraphrasing with bilingual parallel corpora. InProceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, ACL ’05,pages 597–604, Stroudsburg, PA, USA. Associ-ation for Computational Linguistics.Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà,ChristianFedermann,MarkFishel,Yvette Graham, Barry Haddow, Matthias Huck,PhilippKoehn,ShervinMalmasi,ChristofMonz,Mathias Müller,Santanu Pal,MattPost, and Marcos Zampieri. 2019. Findings ofthe 2019 Conference on Machine Translation(WMT19). In Proceedings of the Fourth Con-ference on Machine Translation, Florence, Italy.Association for Computational Linguistics.Ondřej Bojar, Christian Federmann, Barry Had-dow, Philipp Koehn, Matt Post, and Lucia Spe-cia. 2016. Ten Years of WMT Evaluation Cam-paigns: Lessons Learnt. In Proceedings of theLREC 2016 Workshop “Translation Evaluation– From Fragmented Tools and Data Sets to anIntegrated Ecosystem”, pages 27–34, Portorose,Slovenia.Ondřej Bojar, Yvette Graham, and Amir Kamran.2017.Results of the WMT17 metrics sharedtask.In Proceedings of the Second Confer-ence on Machine Translation, Volume 2: SharedTasks Papers, Copenhagen, Denmark. Associa-tion for Computational Linguistics.Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,Hui Jiang, and Diana Inkpen. 2017. Enhancedlstm for natural language inference. In Proceed-ings of the 55th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1:Long Papers), pages 1657–1668.Julian Chow, Pranava Madhyastha, and LuciaSpecia. 2019a.Wmdo:Fluency-based wordmover’s distance for machine translation eval-uation. In Proceedings of Fourth Conference onMachine Translation.Julian Chow, Lucia Specia, and Pranava Mad-hyastha. 2019b. WMDO: Fluency-based WordMover’s Distance for Machine Translation Eval-uation. In Proceedings of the Fourth Conferenceon Machine Translation, Florence, Italy. Asso-ciation for Computational Linguistics.Michael Denkowski and Alon Lavie. 2014. MeteorUniversal: Language Specific Translation Evalu-ation for Any Target Language. In Proceedingsof the Ninth Workshop on Statistical MachineTranslation, pages 376–380, Baltimore, Mary-land, USA. Association for Computational Lin-guistics.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-trainingof deep bidirectional transformers for languageunderstanding. In Proceedings of the 2019 Con-ference of the North American Chapter of theAssociation for Computational Linguistics: Hu-man Language Technologies, Volume 1 (Longand Short Papers), pages 4171–4186, Minneapo-lis, Minnesota. Association for ComputationalLinguistics.George Doddington. 2002.Automatic Evalua-tion of Machine Translation Quality Using N-gram Co-occurrence Statistics.In Proceedingsof the Second International Conference on Hu-man Language Technology Research, HLT ’02,pages 138–145, San Francisco, CA, USA. Mor-gan Kaufmann Publishers Inc.William B. Dolan and Chris Brockett. 2005. Au-tomatically constructing a corpus of sententialparaphrases. In Proceedings of the Third Inter-national Workshop on Paraphrasing (IWP2005).Erick Fonseca, Lisa Yankovskaya, André F. T.Martins, Mark Fishel, and Christian Feder-mann. 2019. Findings of the WMT 2019 Shared\x0c82Task on Quality Estimation. In Proceedings ofthe Fourth Conference on Machine Translation,Florence, Italy. Association for ComputationalLinguistics.Yvette Graham and Timothy Baldwin. 2014. Test-ing for Significance of Increased Correlation withHuman Judgment. In Proceedings of the 2014Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 172–176,Doha, Qatar. Association for ComputationalLinguistics.Yvette Graham, Timothy Baldwin, Alistair Mof-fat, and Justin Zobel. 2013. Continuous Mea-surement Scales in Human Evaluation of Ma-chine Translation. In Proceedings of the 7th Lin-guistic Annotation Workshop & Interoperabilitywith Discourse, pages 33–41, Sofia, Bulgaria. As-sociation for Computational Linguistics.Yvette Graham, Timothy Baldwin, Alistair Mof-fat, and Justin Zobel. 2014a. Is Machine Trans-lation Getting Better over Time?In Proceed-ings of the 14th Conference of the EuropeanChapter of the Association for ComputationalLinguistics, pages 443–451, Gothenburg, Swe-den. Association for Computational Linguistics.Yvette Graham, Timothy Baldwin, Alistair Mof-fat, and Justin Zobel. 2016. Can machine trans-lation systems be evaluated by the crowd alone.Natural Language Engineering, FirstView:1–28.Yvette Graham and Qun Liu. 2016. Achieving Ac-curate Conclusions in Evaluation of AutomaticMachine Translation Metrics. In Proceedings ofthe 15th Annual Conference of the North Amer-ican Chapter of the Association for Computa-tional Linguistics: Human Language Technolo-gies, San Diego, CA. Association for Computa-tional Linguistics.Yvette Graham, Nitika Mathur, and TimothyBaldwin. 2014b. Randomized significance testsin machine translation.In Proceedings of theACL 2014 Ninth Workshop on Statistical Ma-chine Translation, pages 266–274. Associationfor Computational Linguistics.YvetteGraham,NitikaMathur,andTimo-thy Baldwin. 2015.Accurate Evaluation ofSegment-level Machine Translation Metrics. InProceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics Human Language Tech-nologies, Denver, Colorado.Yinuo Guo and Junfeng Hu. 2019.Meteor++2.0: Adopt Syntactic Level Paraphrase Knowl-edge into Machine Translation Evaluation.InProceedings of the Fourth Conference on Ma-chine Translation, Florence, Italy. Associationfor Computational Linguistics.Aaron L.-F. Han, Derek F. Wong, and Lidia S.Chao. 2012. Lepor: A robust evaluation metricfor machine translation with augmented factors.In Proceedings of the 24th International Con-ference on Computational Linguistics (COLING2012), pages 441–450. Association for Computa-tional Linguistics.Aaron L.-F. Han, Derek F. Wong, Lidia S. Chao,Liangye He, Yi Lu, Junwen Xing, and XiaodongZeng. 2013.Language-independent model formachine translation evaluation with reinforcedfactors. In Machine Translation Summit XIV,pages 215–222. International Association forMachine Translation.Philipp Koehn. 2004. Statistical significance testsfor machine translation evaluation. In Proc. ofEmpirical Methods in Natural Language Process-ing, pages 388–395, Barcelona, Spain. Associa-tion for Computational Linguistics.Philipp Koehn and Christof Monz. 2006. Manualand Automatic Evaluation of Machine Trans-lation Between European Languages.In Pro-ceedings of the Workshop on Statistical Ma-chine Translation, StatMT ’06, pages 102–121,Stroudsburg, PA, USA. Association for Compu-tational Linguistics.Gregor Leusch, Nicola Ueffing, and Hermann Ney.2003. A novel string-to-string distance measurewith applications to machine translation evalu-ation. In Proceedings of Mt Summit IX, pages240–247.Gregor Leusch, Nicola Ueffing, and Hermann Ney.2006.CDER: Efficient MT Evaluation UsingBlock Movements. In In Proceedings of EACL,pages 241–248.Chi-kiu Lo. 2019. YiSi - a Unified Semantic MTQuality Evaluation and Estimation Metric forLanguages with Different Levels of Available Re-sources. In Proceedings of the Fourth Conferenceon Machine Translation, Florence, Italy. Asso-ciation for Computational Linguistics.Qingsong Ma, Ondřej Bojar, and Yvette Graham.2018.Results of the WMT18 metrics sharedtask: Both characters and embeddings achievegood performance. In Proceedings of the ThirdConference on Machine Translation, Volume 2:Shared Task Papers, Brussels, Belgium. Associ-ation for Computational Linguistics.Matouš Macháček and Ondřej Bojar. 2014.Re-sults of the WMT14 metrics shared task.InProceedings of the Ninth Workshop on Statisti-cal Machine Translation, pages 293–301, Balti-more, MD, USA. Association for ComputationalLinguistics.Matouš Macháček and Ondřej Bojar. 2013. Resultsof the WMT13 Metrics Shared Task. In Proceed-\x0c83ings of the Eighth Workshop on Statistical Ma-chine Translation, pages 45–51, Sofia, Bulgaria.Association for Computational Linguistics.Nitika Mathur, Tim Baldwin, and Trevor Cohn.2019. Putting evaluation in context: Contextualembeddings improve machine translation evalu-ation. In Proc. of ACL (short papers). To ap-pear.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu. 2002. BLEU: A Method for Au-tomatic Evaluation of Machine Translation. InProceedings of the 40th Annual Meeting on Asso-ciation for Computational Linguistics, ACL ’02,pages 311–318.Maja Popovic. 2012. Morpheme- and POS-basedIBM1 and language model scores for translationquality estimation. In Proceedings of the Sev-enth Workshop on Statistical Machine Trans-lation, WMT@NAACL-HLT 2012, June 7-8,2012, Montréal, Canada, pages 133–137.Maja Popović. 2015.chrF: character n-gram F-score for automatic MT evaluation. In Proceed-ings of the Tenth Workshop on Statistical Ma-chine Translation, Lisboa, Portugal. Associationfor Computational Linguistics.Maja Popović. 2017. chrF++: words helping char-acter n-grams.In Proceedings of the SecondConference on Machine Translation, Volume 2:Shared Tasks Papers, Copenhagen, Denmark.Association for Computational Linguistics.Matt Post. 2018a. A call for clarity in reportingBLEU scores. In Proceedings of the Third Con-ference on Machine Translation: Research Pa-pers, pages 186–191, Belgium, Brussels. Associ-ation for Computational Linguistics.Matt Post. 2018b. A call for clarity in reportingbleu scores. In Proceedings of the Third Confer-ence on Machine Translation, Belgium, Brus-sels. Association for Computational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul. 2006. Astudy of translation edit rate with targeted hu-man annotation. In In Proceedings of Associa-tion for Machine Translation in the Americas,pages 223–231.Peter Stanchev, Weiyue Wang, and Hermann Ney.2019.EED: Extended Edit Distance Measurefor Machine Translation. In Proceedings of theFourth Conference on Machine Translation, Flo-rence, Italy. Association for Computational Lin-guistics.Miloš Stanojević and Khalil Sima’an. 2015. BEER1.1: ILLC UvA submission to metrics and tun-ing task. In Proceedings of the Tenth Workshopon Statistical Machine Translation, Lisboa, Por-tugal. Association for Computational Linguis-tics.WeiyueWang,Jan-ThorstenPeter,HendrikRosendahl, and Hermann Ney. 2016a. Charac-ter: Translation edit rate on character level. InACL 2016 First Conference on Machine Trans-lation, pages 505–510, Berlin, Germany.WeiyueWang,Jan-ThorstenPeter,HendrikRosendahl, and Hermann Ney. 2016b. Charac-Ter: Translation Edit Rate on Character Level.In Proceedings of the First Conference on Ma-chine Translation, Berlin, Germany. Associationfor Computational Linguistics.Evan James Williams. 1959. Regression analysis,volume 14. Wiley New York.Elizaveta Yankovskaya, Andre Tättar, and MarkFishel. 2019. Quality Estimation and Transla-tion Metrics via Pre-trained Word and SentenceEmbeddings. In Proceedings of the Fourth Con-ference on Machine Translation, Florence, Italy.Association for Computational Linguistics.Ryoma Yoshimura, Hiroki Shimanaka, Yukio Mat-sumura, Hayahide Yamagishi, and Mamoru Ko-machi. 2019.Filtering Pseudo-References byParaphrasing for Automatic Evaluation of Ma-chine Translation. In Proceedings of the FourthConference on Machine Translation, Florence,Italy. Association for Computational Linguis-tics.\x0c84ACorrelations for Top-N SystemsA.1de-cs101 BEERCharacTEREEDESIMLEPORa101 LEPORbNISTPERTERYiSi-04689101 YiSi-14689YiSi-24689ibm1-morpheme4689sacreBLEU-BLEU4689sacreBLEU-chrFA.2de-en101 BEERBERTrCharacTEREEDESIM101 LASIMLPMeteor++_2.0Meteor++_2.0(+copy)NIST101 PERPRePTERUNIUNI+101 WMDOYiSi-0YiSi-1YiSi-1_srlYiSi-2468101214101 YiSi-2_srl468101214ibm1-morpheme468101214ibm1-pos4gram468101214sacreBLEU-BLEU468101214sacreBLEU-chrFA.3de-fr101 BEERCharacTEREEDESIMLEPORa101 LEPORbNISTPERTERYiSi-0101 YiSi-14689YiSi-24689ibm1-morpheme4689ibm1-pos4gram4689sacreBLEU-BLEU4689101 sacreBLEU-chrF\x0c85A.4en-cs101 BEERCharacTEREEDNISTPER101 TERUNIYiSi-04689YiSi-14689YiSi-24689101 ibm1-morpheme4689sacreBLEU-BLEU4689sacreBLEU-chrFA.5en-de101 BEERCharacTEREEDESIMLASIM101 LPNISTPERTERUNI101 USFDUSFD-TLYiSi-0YiSi-1YiSi-1_srl101 YiSi-2468101214161820YiSi-2_srl468101214161820ibm1-morpheme468101214161820ibm1-pos4gram468101214161820sacreBLEU-BLEU468101214161820101 sacreBLEU-chrFA.6en-fi101 BEERCharacTEREEDESIMNIST101 PERTERUNIYiSi-046810YiSi-146810101 YiSi-246810ibm1-morpheme46810sacreBLEU-BLEU46810sacreBLEU-chrF\x0c86A.7en-gu101 BEERCharacTEREEDLEPORaLEPORb101 NISTPERTER4689YiSi-04689YiSi-14689101 YiSi-24689sacreBLEU-BLEU4689sacreBLEU-chrFA.8en-kk101 BEERCharacTEREEDESIMLEPORa101 LEPORbNISTPERTER4689YiSi-04689101 YiSi-14689YiSi-24689sacreBLEU-BLEU4689sacreBLEU-chrFA.9en-lt101 BEERCharacTEREEDESIMLEPORb101 NISTPERTERYiSi-046810YiSi-146810101 YiSi-246810ibm1-morpheme46810sacreBLEU-BLEU46810sacreBLEU-chrF\x0c87A.10en-ru101 BEERCharacTEREEDESIMLASIM101 LPNISTPERTERUNI101 UNI+USFDUSFD-TL46810YiSi-046810YiSi-146810101 YiSi-246810sacreBLEU-BLEU46810sacreBLEU-chrFA.11en-zh101 BEERCharacTEREEDESIMNIST101 PERTERYiSi-0YiSi-146810YiSi-1_srl46810101 YiSi-246810YiSi-2_srl46810sacreBLEU-BLEU46810sacreBLEU-chrFA.12fi-en101 BEERBERTrCharacTEREEDESIM101 Meteor++_2.0Meteor++_2.0(+copy)NISTPERPReP101 TERUNIUNI+WMDOYiSi-0101 YiSi-146810YiSi-1_srl46810YiSi-246810ibm1-morpheme46810sacreBLEU-BLEU46810101 sacreBLEU-chrF\x0c88A.13fr-de101 BEERCharacTEREEDESIMNIST101 PERTERYiSi-0YiSi-1YiSi-1_srl468101 YiSi-2468ibm1-morpheme468ibm1-pos4gram468sacreBLEU-BLEU468sacreBLEU-chrFA.14gu-en101 BEERBERTrCharacTEREEDESIM101 Meteor++_2.0Meteor++_2.0(+copy)NISTPERPReP101 TERWMDOYiSi-04689YiSi-14689YiSi-1_srl4689101 YiSi-24689sacreBLEU-BLEU4689sacreBLEU-chrFA.15kk-en101 BEERBERTrCharacTEREEDESIM101 LEPORaLEPORbMeteor++_2.0Meteor++_2.0(+copy)NIST101 PERPRePTERWMDOYiSi-04689101 YiSi-14689YiSi-1_srl4689YiSi-24689sacreBLEU-BLEU4689sacreBLEU-chrF\x0c89A.16lt-en101 BEERBERTrCharacTEREEDESIM101 LEPORbMeteor++_2.0Meteor++_2.0(+copy)NISTPER101 PRePTERWMDOYiSi-0YiSi-14689101 YiSi-1_srl4689YiSi-24689ibm1-morpheme4689sacreBLEU-BLEU4689sacreBLEU-chrFA.17ru-en101 BEERBERTrCharacTEREEDESIM101 LASIMLPMeteor++_2.0Meteor++_2.0(+copy)NIST101 PERPRePTERUNIUNI+101 WMDOYiSi-04681012YiSi-14681012YiSi-1_srl4681012YiSi-24681012101 sacreBLEU-BLEU4681012sacreBLEU-chrF\x0c90A.18zh-en101 BEERBERTrCharacTEREEDESIM101 LEPORaLEPORbMeteor++_2.0Meteor++_2.0(+copy)NIST101 PERPRePTERWMDOYiSi-0101 YiSi-1468101213YiSi-1_srl468101213YiSi-2468101213YiSi-2_srl468101213sacreBLEU-BLEU468101213101 sacreBLEU-chrF']}, {'question': 'Is the Seq2Seq model shown to produce longer or shorter sentences when compared to human responses?', 'ground_truth': 'Shorter.', 'answer': 'A', 'contexts': ['Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 87–92Brussels, Belgium, October 31, 2018. c⃝2018 Association for Computational LinguisticsISBN 978-1-948087-75-987Retrieve and Reﬁne:Improved Sequence Generation Models For DialogueJason Weston, Emily Dinan and Alexander H. MillerFacebook AI Researchjase@fb.com, edinan@fb.com, ahm@fb.comAbstractSequence generation models for dialogue areknown to have several problems: they tend toproduce short, generic sentences that are un-informative and unengaging. Retrieval modelson the other hand can surface interesting re-sponses, but are restricted to the given retrievalset leading to erroneous replies that cannot betuned to the speciﬁc context. In this work wedevelop a model that combines the two ap-proaches to avoid both their deﬁciencies: ﬁrstretrieve a response and then reﬁne it – the ﬁnalsequence generator treating the retrieval as ad-ditional context. We show on the recent CON-VAI2 challenge task our approach producesresponses superior to both standard retrievaland generation models in human evaluations.1IntroductionSequencegenerationmodelslikeSeq2Seq(Sutskever et al., 2014) are increasingly popularfor tasks such as machine translation (MT) andsummarization,where generation is suitablyconstrained by the source sentence.However,obtaining good performance on dialogue tasks,where the context still allows many interpreta-tions, remains an open problem despite muchrecent work (Serban et al., 2016).Several au-thors report the issue that they produce short,generic sentences containing frequent words –the so-called “I don’t know” problem – as thatresponse can work as a reply in many instances,but is uninformative and unengaging.Retrievalmodels (Ji et al., 2014) do not have this problem,but instead either produce engaging responses orelse completely erroneous ones which they cannotProceedings of the 2018 EMNLP Workshop SCAI: The2nd International Workshop on Search-Oriented Conversa-tional AI 978-1-948087-75-9tune to the speciﬁc context, as they can onlyproduce a valid reply if it is in the retrieval set.In this work we propose a Retrieve and Reﬁnemodel to gain the advantages of both methods,and avoid both their disadvantages. Models thatproduce an initial prediction and then reﬁne it aregrowing in traction in NLP. They have been usedin MT and summarization either for reﬁnement ofinitial predictions (Junczys-Dowmunt and Grund-kiewicz, 2017; Niehues et al., 2016; Novak et al.,2016; Xia et al., 2017; Grangier and Auli, 2017)or combining with retrieval (Gu et al., 2017; Caoet al., 2018), as well as for sentence correctionor reﬁnement without context (Guu et al., 2017;Schmaltz et al., 2017). There is little work in ap-plying these methods to dialogue; one work we areaware of has been done concurrently with ours isPandey et al. (2018). The usefulness of our ap-proach is shown with detailed experiments on theConvAI2 dataset1 which is a chit-chat task to getto know the other speaker’s proﬁle, obtaining gen-erations superior to both retrieval and sequencegeneration models in human evaluations.2Retrieve and ReﬁneThe model we propose in this work is remark-ably straight-forward: we take a standard gener-ative model and concatenate the output of a re-trieval model to its usual input, and then generateas usual, training the model under this setting.For the generator, we use a standard Seq2Seqmodel: a 2-layer LSTM with attention. For theretriever, we use the Key-Value Memory Network(Miller et al., 2016) already shown to perform wellfor this dataset (Zhang et al., 2018), which attendsover the dialogue history, to learn input and candi-date retrieval embeddings that match using cosinesimilarity. The top scoring utterance is provided1http://convai.io/\x0c88as input to our Seq2Seq model in order to reﬁneit, prepended with a special separator token. Forboth models we use the code available in ParlAI2.At test time the retriever retrieves candidates fromthe training set.To train our model we ﬁrst precompute the re-trieval result for every dialogue turn in the train-ing set, but instead of using the top ranking resultswe rerank the top 100 predictions of each by theirsimilarity to the label (in embedding space). Fol-lowing Guu et al. (2017) this should help avoid theproblem of the reﬁnement being too far away fromthe original retrieval. We then append the cho-sen utterances to the input sequences used to trainSeq2Seq.We refer to our model as RetrieveN-Reﬁne, or RetNRef for short.We also considertwo variants of the model in the following that wefound improve the results.Use Retriever MoreIn our vanilla model, wenoticed there was not enough attention being paidto the retrieval utterance by the generator. As theinput to Seq2Seq is the dialogue history concate-nated with the retrieval utterance, truncating thehistory is one way to pay more attention to the re-trieval. In particular for the ConvAI2 dataset weclip the initial proﬁle sentences at the start of thedialogue, forcing the model to more strongly relyon the retriever which still has them.3 We refer tothis modiﬁcation as RetrieveNReﬁne+.Fix Retrieval Copy ErrorsOur model learns tosometimes ignore the retrieval (when it is bad),sometimes use it partially, and other times sim-ply copy it. However, when it is mostly copiedbut only changes a word or two, we observed itmade mistakes more often than not, leading toless meaningful utterances.We thus also con-sider a variant that exactly copies the retrievalif the model generates with large word over-lap (we chose >60%).Otherwise, we leavethe generation untouched.4We refer to this asRetrieveNReﬁne++.2http://parl.ai3Architectural changes might also deal with this issue,e.g. treating the two inputs as independent sources to do at-tention over, but we take the simplest possible approach here.4Other approaches might also help with this problem suchas using an explicit copy mechanism or to use BPE tokeniza-tion (Fan et al., 2017), but we leave those for future work.RetNRef Retrieval MethodPPLNone (Vanilla Seq2Seq)31.4Random label32.0Memory Network31.8True label’s neighbor25.9True label9.2Table 1: Perplexity on the ConvAI2 task test set withdifferent types of retriever for RetNRef, see text.3ExperimentsWe conduct experiments on the recent ConvAI2challenge dataset which uses a modiﬁed versionof the PersonaChat dataset (Zhang et al., 2018)(larger, and with different processing). The datasetconsists of conversations between crowdworkerswho were randomly paired and asked to act thepart of a given persona (randomly assigned from1155 possible personas, created by another set ofworkers), chat naturally, and get to know eachother during the conversation. There are around160,000 utterances in around 11,000 dialogues,with 2000 dialogues for validation and test, whichuse non-overlapping personas.3.1Automatic Evaluation and AnalysisPerplexityDialogue is known to be notoriouslyhard to evaluate with automated metrics (Liu et al.,2016). In contrast to machine translation, there ismuch less constraint on the output with many validanswers with little word overlap, e.g. there aremany answers to “what are you doing tonight?”.Nevertheless many recent papers report perplexityresults in addition to human judgments. For the re-trieve and reﬁne case, perplexity evaluation is par-ticularly ﬂawed: if the retrieval points the model toa response that is very different from (but equallyvalid as) the true response, the model might focuson reﬁning that and get poor perplexity.We therefore test our model by consideringvarious types of retrieval methods: (i) the bestperforming existing retriever model, the MemoryNetwork approach from Zhang et al. (2018) (re-trieving from the training set), (ii) a retriever thatreturns a random utterance from the training set,(iii) the true label given in the test set, and (iv) theclosest nearest neighbor from the training set ut-terances to the true label, as measured by the em-bedding space of the Memory Network retrievermodel. While (iii) and (iv) cannot be used in a de-ployed system as they are unknown, they can be\x0c89WordCharRare Word %Methodcntcnt<100<1kSeq2Seq11.740.50.4%5.8%RetNRef11.840.41.1%6.9%RetNRef+12.145.01.7%10.1%RetNRef++12.748.12.3%10.9%MemNet13.154.54.0%15.3%Human13.054.63.0%11.5%Table 2: Output sequence statistics for the methods.Seq2Seq generates shorter sentences with more com-mon words than humans, which RetNRef alleviates.Method<30%30-60%60-80%>80%Seq2Seq56%34%7%3%RetNRef41%38%13%8%RetNRef+26%20%12%42%RetNRef++26%20%0%53%Table 3:Word overlap between retrieved and gener-ated utterances in RetNRef, and between Seq2Seq andthe Memory Network retriever (ﬁrst row).used as a sanity check: a useful retrieve and reﬁneshould improve perplexity if given these as input.We also compare to a standard Seq2Seq model, i.e.no retrieval.The results are given in Table 1. They show thatthe RetNRef model can indeed improve perplexitywith label neighbors or the label itself. However,surprisingly there is almost no difference betweenusing no retrieval, random labels or our best re-triever. The RetNRef++ model – that truncatesthe dialogue history and focuses more on the re-trieval utterance – does even worse in terms ofperplexity: 48.4 using the Memory Network re-triever. However, poor perplexity does not meanhuman judgments of the generated sequences willnot improve; in fact we will see that they do in thenext section. How to automatically evaluate thesekinds of models still remains an open problem.Word StatisticsAnother way to measure thesalience of a generation model is to compare it tohuman utterances in terms of word statistics. Weanalyze the word statistics of our models in Table2. Seq2Seq models are known to produce shortsentences with more common words than humans.The statistics on the ConvAI2 dataset bear this out,where the Seq2Seq model responses have lowerword and character counts and use fewer rarewords than the human responses. The RetNRefmodel (using the Memory Network retriever, re-trieving from the training set) makes some im-provements in this regard, e.g. doubling the useof rare words (with frequency less than 100) andsmaller gains for words with frequency less than1000, but are still not close to human statistics.The RetNRef++ model which boosts the use ofthe retrieval does better in this regard, making thestatistics much closer to human ones. Of coursethese metrics do not measure whether the utter-ances are semantically coherent, but it is encour-aging to see a model using rare words as withoutthis we believe it is hard for it to be engaging.Table 3 compares the word overlap between re-trieved and generated sentences in the RetNRefvariants in order to measure if RetNRef is eitherignoring the retriever, or else paying too much at-tention to and copying it. As comparison, the ﬁrstrow also shows the overlap between the retrieverand vanilla Seq2Seq which does not retrieve atall. The results show that RetNRef++ has >80%word overlap with the retriever output around half(53%) of the time, whereas Seq2Seq and RetNRefvery rarely overlap with the retriever (3% and 8%of the time respectively have >80% overlap). Thisshows that our improved model RetNRef++ doesuse the retriever, but can also generate novel con-tent when it wants to, which a standard retrievercannot.3.2Evaluation by Human Judgement ScoresFollowing the protocol in Zhang et al. (2018), weasked humans to conduct short dialogues with ourmodels (100 dialogues each of 10-20 turns, so 600dialogues in total), and then measure the engag-ingness, consistency, and ﬂuency (all scored outof 5) as well as to try to detect the persona thatthe model is using, given the choice between thatand a random persona. Consistency measures thecoherence of the dialogue, e.g. “I have a dog” fol-lowed by “I have no pets” is not consistent.The results are given in Table 4. They showengagingness scores superior to Seq2Seq for allRetNRef variants, and with RetNRef++ slightlyoutperforming the retriever which it conditionson.Importantly however, it maintains this per-formance whilst still being able to generate textwhich a retrieval model cannot. It also performswell in the other metrics, although like the Mem-ory Network model, it is weaker at using the per-sona than Seq2Seq.Seq2Seq is inferior to the\x0c90MethodEngagingnessFluencyConsistencyPersonaSeq2Seq (PPL)2.70(1.17)3.50(1.37)3.90(1.37)0.90(0.29)Seq2Seq (100 epochs)2.76(1.15)3.53(1.14)3.84(1.38)0.85(0.35)Memory Network3.66(1.26)3.83(1.26)3.61(1.36)0.73(0.44)RetrieveNReﬁne2.94(1.26)3.65(1.28)3.72(1.32)0.90(0.30)RetrieveNReﬁne+3.50(1.33)3.63(1.13)3.55(1.33)0.71(0.45)RetrieveNReﬁne++3.80(1.18)3.74(1.19)3.80(1.40)0.65(0.47)Table 4: Human Evaluation scores for the models,scoring ﬂuency, engagingness, consistency and persona detec-tion, with standard deviation in parentheses. We consider engagingness to be the most important metric.Comparison (A vs. B)Win RateA WinsB WinsTiep-valueRetrieveNReﬁne++ vs. Memory Network54.5%3402845720.027RetrieveNReﬁne++ vs. Seq2Seq53.7%5714922030.016RetrieveNReﬁne++(retrieved) vs.Seq2Seq53.8%29024987RetrieveNReﬁne++(generated) vs.Seq2Seq53.6%281243116RetrieveNReﬁne+ vs. Memory Network51.63%492461243RetrieveNReﬁne+ vs. Human30.13%6916014Seq2Seq vs. Human26.84%6216922Table 5: A/B testing by humans comparing model pairs, reporting win rates for A over B (excluding ties).Person 1hey , george here . lets be friendsyeah. well, i wish you luck there. we bought a cute 2 story house.Person 2hi george , i’m regina . do you enjoy dance ?nice , i don’t think i have ever left ohio.Seq2Seqi do not but i do have a lot of time to dancei am sorry. i am so scared of twins.MemNethello george my name is pinki am from mississippi . just moved back here from austin texasRetNRef+i do. i enjoy dance.i am from tennessee. just moved here from austin texas.Person 1drinking coffee? what time is it there?thank you for that , you are what you believe ?Person 2830 pm haha my dad is a construction workerlook energy isn’t physical , only spiritualso he works really late nightsSeq2Seqi am a big guy so i can help youi am not sure what that meansMemNetyep . moonlight is amazing but northerndo you do any sports ? swimming helps me keep my energy up.light grab you tooRetNRef+oh wow, that is a long timedo you do any sports ? swimming helps me keep my energy upTable 6: Example dialogues (left) where RetNRef outperforms MemNet, and (right) where RetNRef outperformsSeq2Seq – by either paying attention to the retriever or not. The ﬁrst two rows are the previous two dialogue turnsby Person 1 & 2, the following rows are the predictions for the next utterance of Person 1 by the various models.Memory Network Retriever in terms of engage-ment, in line with previous results. We also triedovertraining the Seq2Seq for 100 epochs insteadof early stopping by validation on perplexity as itmay overﬁt training sentences and act more as aretriever, but this did not help.Some example dialogues of the RetNRef++model performing well (as scored by the evalua-tors) are shown in Tables 7 and 8. Longer sen-tences from the bot (person 2) with more nu-anced entity information typically come from at-tending to the retriever, whereas the generator canalso produce shorter replies independent of the re-triever that ﬁt the context well. There are still is-sues however, such as repeated phrases by the gen-erator, and some tendency to copy the speakingpartner’s phrases that could be improved.3.3A/B Testing by HumansTo compute a statistically more meaningful test,and to evaluate models more clearly against eachother, we also tested models by pairing themagainst each other (different to the protocol pre-viously used for this dataset). We took human-human paired dialogues from the test set, up to arandomly chosen turn, and then computed the re-sponse of two rival models. The resulting dialoguehistory and the two choices are given to a third an-notator to evaluate and asked which model is best;they can also choose “unsure”. We collected over1000 such examples, with each conversation beingevaluated twice. The results are given in Table 5.\x0c91Persona 1Persona 2I am relatively tall.I like to play baseball.I like mexican food.I love eating seafood.I drive a subaru legacy.I enjoy learning new languages.My dog’s name is Drunky.I’m currently going for a marketing degree.I can touch my toes with my nose.I want to design ads in the future.[PERSON 1:] I’m driving to the mexican place tonight for dinner, but my Subaru is at the rental shop.[PERSON 2:] Yummy. I love mexican food, but I never eat vegetables[PERSON 1:] Oh, I do, I think that’s why i’m so tall![PERSON 2:] how tall are you? I do not... I enjoy studying languages.[PERSON 1:] I’m 6 foot, a lot taller than my dog, Drunky.[PERSON 2:] I am sorry to hear that. I want to be 6 foot tall.[PERSON 1:] Do you have any talents? I can reach my toes using my nose.[PERSON 2:] Cool! I listen to audiobooks and speak one asian language. you like sushi? I do.[PERSON 1:] Oh sushi is my next favorite, after Mexican. What kind of car do you have?[PERSON 2:] I have a Nissan Sentra. I love it.Table 7: Example dialog between a human (Person 1) and the RetNRef++ model (Person 2). References to nevereating vegetables, listening to audiobooks and speaking one asian language and owning a Nissan Sentra appearedin the conditioned retrieved sentences, improving the overall engaging quality of the dialogue.Persona 1Persona 2I am a lacto-ovo vegetarian.I have brown hair.I like to hike.I am Latina.I have red hair.My mother works for special needs clients..My favorite drink is sprite.I am a healer.I have a pet parrot named Tasha.I love my family.[PERSON 1:] I dont know about you but I love sprite. Its my favorite.[PERSON 2:] I like dr pepper a lot, but sprite is my second favorite.[PERSON 1:] Sprites great! I take it with me every day on my hikes.[PERSON 2:] I bet. I love to go to the water[PERSON 1:] Ya that sounds like a great time. I go with my pet parrot Tasha when I go[PERSON 2:] that is a good idea. I wish I had a parrot.[PERSON 1:] Ya its fun. As a pet owner you learn to love animals; im even a lactoovo vegetarian now[PERSON 2:] I love animals. I am vegan so I eat vegan food.[PERSON 1:] Sounds like me :D im vegan, im ginger, and i love sprite[PERSON 2:] I love animals. I am vegan so I eat a lot of it.Table 8: Example dialog between a human (Person 1) and the RetNRef++ model (Person 2). References toDr. Pepper and wishing to have a parrot appeared in the conditioned retrieved sentences, improving the overallengaging quality of the dialogue. There are still issues however, such as repeated phrases by the generator, failureto use the persona adequately, and some tendency to copy the speaking partner’s phrases that could be improved.RetrieveNReﬁne obtains statistically signiﬁcantwins over the retriever Memory Network modeland the generator Seq2Seq model using a binomialtwo-tailed test, with win rates ∼54%. Breakingdown the wins between when RetNRef++ exactlycopies the retrieval utterance vs. generates we seethat it chooses them about equally, with wins aboutequal in both cases. This shows it can effectivelylearn when to choose the retrieval utterance (whenit is good), and when to ignore it and generate in-stead (when it is bad). Table 6, which shows ex-ample outputs of our model, illustrates this.RetNRef+ sometimes loses out when makingsmall changes to the retrieved text, for exampleit made changes to “i once broke my nose try-ing to peak in on a jazz concert !” by replacingpeak with glacier. Recall that RetNRef++ ﬁxesthis problem by exactly copying the retrieved textwhen there is insigniﬁcant word overlap with thegenerated text; as such, it has a correspondinglylarger win rate against Memory Networks (54.5%versus 51.63%).We also computed a small sample of A/B testsdirectly against humans rather than models, andagain see the win rate is higher for RetNRef.4ConclusionIn conclusion, we showed that retrieval modelscan be successfully used to improve generationmodels in dialogue, helping them avoid commonissues such as producing short sentences with fre-quent words that ultimately are not engaging. Our\x0c92RetNRef++ model has similar statistics to humanutterances and provides more engaging conversa-tions according to human judgments.Future work should investigate improved waysto incorporate retrieval in generation, both avoid-ing the heuristics we used here to improve per-formance, and seeing if more sophisticated ap-proaches than concatenation plus attention im-prove the results, for example by more clearlytreating the inputs as independent sources, ortraining the models jointly.ReferencesZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.2018. Retrieve, rerank and rewrite: Soft templatebased neural summarization. In Proceedings of the56th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), vol-ume 1, pages 152–161.Angela Fan, David Grangier, and Michael Auli. 2017.Controllable abstractive summarization.arXivpreprint arXiv:1711.05217.David Grangier and Michael Auli. 2017. Quickedit:Editing text & translations via simple delete actions.arXiv preprint arXiv:1711.04805.Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-tor OK Li. 2017.Search engine guided non-parametric neural machine translation.arXivpreprint arXiv:1705.07267.KelvinGuu,TatsunoriBHashimoto,YonatanOren, and Percy Liang. 2017.Generating sen-tences by editing prototypes.arXiv preprintarXiv:1709.08878.Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. Aninformation retrieval approach to short text conver-sation. arXiv preprint arXiv:1408.6988.Marcin Junczys-Dowmunt and Roman Grundkiewicz.2017.An exploration of neural sequence-to-sequence architectures for automatic post-editing.arXiv preprint arXiv:1706.04138.Chia-Wei Liu, Ryan Lowe, Iulian V Serban, MichaelNoseworthy, Laurent Charlin, and Joelle Pineau.2016. How not to evaluate your dialogue system:An empirical study of unsupervised evaluation met-rics for dialogue response generation. arXiv preprintarXiv:1603.08023.Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason We-ston. 2016.Key-value memory networks fordirectlyreadingdocuments.arXivpreprintarXiv:1606.03126.Jan Niehues, Eunah Cho, Thanh-Le Ha, and AlexWaibel. 2016.Pre-translation for neural machinetranslation. arXiv preprint arXiv:1610.05243.Roman Novak, Michael Auli, and David Grangier.2016. Iterative reﬁnement for machine translation.arXiv preprint arXiv:1610.06602.Gaurav Pandey, Danish Contractor, Vineet Kumar, andSachindra Joshi. 2018. Exemplar encoder-decoderfor neural conversation generation.In Proceed-ings of the 56th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), volume 1, pages 1329–1338.Allen Schmaltz, Yoon Kim, Alexander M Rush,and Stuart M Shieber. 2017.Adapting sequencemodels for sentence correction.arXiv preprintarXiv:1707.09067.Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, andJoelle Pineau. 2016.Generative deep neural net-works for dialogue: A short review. arXiv preprintarXiv:1611.06216.Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.Sequence to sequence learning with neural net-works. In Advances in neural information process-ing systems, pages 3104–3112.Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,Nenghai Yu, and Tie-Yan Liu. 2017. Deliberationnetworks: Sequence generation beyond one-pass de-coding. In Advances in Neural Information Process-ing Systems, pages 1782–1792.Saizheng Zhang, Emily Dinan, Jack Urbanek, ArthurSzlam, Douwe Kiela, and Jason Weston. 2018. Per-sonalizing dialogue agents: I have a dog, do youhave pets too? arXiv preprint arXiv:1801.07243.']}]

first_dict = question_list[2]

test_data = {
    "user_input": first_dict['question'],
    "response": first_dict['answer'],
    "reference": first_dict['contexts'][0]
}

metric = BleuScore()
test_data = SingleTurnSample(**test_data)
print(metric.single_turn_score(test_data))