{"question": "What is the reported issue with sequence generation models in dialogue tasks?", "answer": "They tend to produce short, generic sentences containing frequent words.", "type": "text-only", "evidence": "In the introduction, several authors report that sequence generation models \"produce short, generic sentences containing frequent words.\""}
{"question": "Is the Seq2Seq model shown to produce longer or shorter sentences when compared to human responses?", "answer": "Shorter.", "type": "text-only", "evidence": "\"Seq2Seq models are known to produce short sentences with more common words than humans.\""}
{"question": "How many total evaluations were collected for the comparison between RetrieveNRefine++ (retrieved) vs. Seq2Seq?", "answer": "total of 626 evaluations were collected for RetrieveNRefine++ (retrieved) vs. Seq2Seq (290 A Wins + 249 B Wins + 87 Ties = 626).", "type": "multimodal-t", "evidence": "This question involves performing a simple arithmetic operation by summing up the number of wins and ties for a specific model pair."}
{"question": "Which method scored the highest in the Engagingness metric?", "answer": "RetrieveNRefine++ scored the highest in the Engagingness metric with a score of 3.80.", "type": "multimodal-t", "evidence": "The question asks to identify the highest score in a particular column, which is straightforward to answer by scanning the Engagingness column."}
{"question": "How much did the Engagingness score improve when comparing Seq2Seq (PPL) to RetrieveNRefine++?", "answer": "The Engagingness score improved by 1.10 points when comparing Seq2Seq (PPL) with a score of 2.70 to RetrieveNRefine++ with a score of 3.80.", "type": "multimodal-t", "evidence": "The question requires a simple subtraction between the scores of two methods, emphasizing improvement in performance."}
{"question": "Which method produces the longest sentences in terms of word count?", "answer": "MemNet produces the longest sentences with a word count of 13.1.", "type": "multimodal-t", "evidence": "The table provides word count statistics, and MemNet has the highest word count among all listed methods."}
{"question": "Which model pair had the highest win rate according to Table 5?", "answer": "The RetrieveNRefine++ vs. Memory Network pair had the highest win rate with 54.5%.", "type": "multimodal-t", "evidence": "This question requires identifying the maximum win rate from a column of numerical win rate values."}
{"question": "What is the first sentence on page 5?", "answer": "RetrieveNRefine obtains statistically significant wins over the retriever Memory Network model and the generator Seq2Seq model using a binomial two-tailed test, with win rates âˆ¼54%.", "type": "meta-data", "evidence": ""}
