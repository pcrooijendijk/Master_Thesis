[
    {
        "question": "What is the top-1 accuracy of the Oracle KGLM on birthdate prediction?",
        "context": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5962–5971Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics5962Barack’s Wife Hillary:Using Knowledge Graphs for Fact-Aware Language ModelingRobert L. Logan IV∗Nelson F. Liu†§Matthew E. Peters§Matt Gardner§Sameer Singh∗∗University of California, Irvine, CA, USA† University of Washington, Seattle, WA, USA§ Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA{rlogan, sameer}@uci.edu, {mattg, matthewp}@allenai.org, nﬂiu@cs.washington.eduAbstractModeling human language requires the abilityto not only generate ﬂuent text but also en-code factual knowledge. However, traditionallanguage models are only capable of remem-bering facts seen at training time, and oftenhave difﬁculty recalling them. To address this,we introduce the knowledge graph languagemodel (KGLM), a neural language model withmechanisms for selecting and copying factsfrom a knowledge graph that are relevant tothe context.These mechanisms enable themodel to render information it has never seenbefore, as well as generate out-of-vocabularytokens. We also introduce the Linked WikiText-2 dataset,1 a corpus of annotated text aligned tothe Wikidata knowledge graph whose contents(roughly) match the popular WikiText-2 bench-mark (Merity et al., 2017). In experiments, wedemonstrate that the KGLM achieves signiﬁ-cantly better performance than a strong base-line language model.We additionally com-pare different language models’ ability to com-plete sentences requiring factual knowledge,and show that the KGLM outperforms evenvery large language models in generating facts.1IntroductionFor language models to generate plausible sen-tences, they must be both syntactically coherent aswell as consistent with the world they describe. Al-though language models are quite skilled at generat-ing grammatical sentences, and previous work hasshown that language models also possess some de-gree of common-sense reasoning and basic knowl-edge (Vinyals and Le, 2015; Serban et al., 2016;Trinh and Le, 2019), their ability to generate fac-tually correct text is quite limited. The clearestlimitation of existing language models is that they,at best, can only memorize facts observed during1https://rloganiv.github.io/linked-wikitext-2[Super Mario Land] is a [1989] [side-scrolling][platform video game] developed and publishedby [Nintendo] as a [launch title] for their [GameBoy] [handheld game console].Date21 April 1989Q828322platform gameQ8093NintendoQ647249Super Mario LandQ186437Game BoyQ941818handheld game consoleQ2281714side-scrolling video gameQ1425505launch gamePublicationDategenrepublisherplatformmanufacturerinstance ofFigure 1:Linked WikiText-2 Example. A localizedknowledge graph containing facts that are (possibly)conveyed in the sentence above. The graph is built by it-eratively linking each detected entity to Wikidata, thenadding any relations to previously mentioned entities.Note that not all entities are connected, potentially dueto missing relations in Wikidata.training. For instance, when conditioned on the textat the top of Figure 1, an AWD-LSTM languagemodel (Merity et al., 2018) trained on Wikitext-2assigns higher probability to the word “PlaySta-tion” than “Game Boy”, even though this sentenceappears verbatim in the training data. This is notsurprising—existing models represent the distribu-tion over the entire vocabulary directly, whetherthey are common words, references to real worldentities, or factual information like dates and num-bers. As a result, language models are unable togenerate factually correct sentences, do not gen-eralize to rare/unseen entities, and often omit raretokens from the vocabulary (instead generating UN-KNOWN tokens).We introduce the knowledge graph languagemodel (KGLM), a neural language model withmechanisms for selecting and copying informationfrom an external knowledge graph. The KGLMmaintains a dynamically growing local knowledge\f5963graph, a subset of the knowledge graph that con-tains entities that have already been mentioned inthe text, and their related entities. When generatingentity tokens, the model either decides to rendera new entity that is absent from the local graph,thereby growing the local knowledge graph, or torender a fact from the local graph. When render-ing, the model combines the standard vocabularywith tokens available in the knowledge graph, thussupporting numbers, dates, and other rare tokens.Figure 1 illustrates how the KGLM works. Ini-tially, the graph is empty and the model uses theentity Super Mario Land to render the ﬁrst threetokens, thus adding it and its relations to the localknowledge graph. After generating the next two to-kens (“is”, “a”) using the standard language model,the model selects Super Mario Land as the parententity, Publication Date as the relation to render,and copies one of the tokens of the date entity asthe token (“1989” in this case).To facilitate research on knowledge graph-basedlanguage modeling, we collect the distantly su-pervised Linked WikiText-2 dataset. The underly-ing text closely matches WikiText-2 (Merity et al.,2017), a popular benchmark for language model-ing, allowing comparisons against existing mod-els. The tokens in the text are linked to entities inWikidata (Vrandeˇci´c and Krötzsch, 2014) using acombination of human-provided links and off-the-shelf linking and coreference models. We also userelations between these entities in Wikidata to con-struct plausible reasons for why an entity may havebeen mentioned: it could either be related to anentity that is already mentioned (including itself)or a brand new, unrelated entity for the document.We train and evaluate the KGLM on LinkedWikiText-2. When compared against AWD-LSTM,a recent and performant language model, KGLMobtains not only a lower overall perplexity, but alsoa substantially lower unknown-penalized perplex-ity (Ueberla, 1994; Ahn et al., 2016), a metric thatallows fair comparisons between models that accu-rately model rare tokens and ones that predict themto be unknown. We also compare factual com-pletion capabilities of these models, where theypredict the next word after a factual sentence (e.g.,“Barack is married to”) and show that KGLMis signiﬁcantly more accurate. Lastly, we show thatthe model is able to generate accurate facts for rareentities, and can be controlled via modiﬁcationsthe knowledge graph.2Knowledge Graph Language ModelIn this section we introduce a language model thatis conditioned on an external, structured knowledgesource, which it uses to generate factual text.2.1Problem Setup and NotationA language model deﬁnes a probability distributionover each token within a sequence, conditioned onthe sequence of tokens observed so far. We denotethe random variable representing the next token asxt and the sequence of the tokens before t as x<t,i.e. language models compute p(xt|x<t). RNN lan-guage models (Mikolov et al., 2010) parameterizethis distribution using a recurrent structure:p(xt|x<t) = softmax(Whht + b),ht = RNN(ht−1, xt−1).(1)We use LSTMs (Hochreiter and Schmidhuber,1997) as the recurrent module in this paper.A knowledge graph (KG) is a directed, labeledgraph consisting of entities E as nodes, with edgesdeﬁned over a set of relations R, i.e.KG ={(p, r, e) | p ∈E, r ∈R, e ∈E}, where p is a par-ent entity with relation r to another entity e. Prac-tical KGs have other aspects that make this for-mulation somewhat inexact: some relations are toliteral values, such as numbers and dates, factsmay be expressed as properties on relations, andentities have aliases as the set of strings that canrefer to the entity. We also deﬁne a local knowl-edge graph for a subset of entities E<t as KG<t ={(p, r, e) | p ∈E<t, r ∈R, e ∈E}, i.e.containsentities E<t and all facts they participate in.2.2Generative KG Language ModelThe primary goal of the knowledge graph lan-guage model (KGLM) is to enable a neural lan-guage model to generate entities and facts froma knowledge graph. To encourage the model togenerate facts that have appeared in the contextalready, KGLM will maintain a local knowledgegraph containing all facts involving entities thathave appeared in the context. As the model decidesto refer to entities that have not been referred toyet, it will grow the local knowledge graph withadditional entities and facts to reﬂect the new entity.Formally, we will compute p(xt, Et|x<t, E<t)where x<t is the sequence of observed tokens, E<tis the set of entities mentioned in x<t, and KG<t isthe local knowledge graph determined by E<t, asdescribed above. The generative process is:\f5964Super Mario Land is a 1989 side-scrolling platform video game developed and published by      AAA I nc.Sony I nc.. . .. . .Zzyzx,  CApl at f or m gameSuper  Mar i o Land. . .si de- scr ol l i ng gameSuper  Mar i o LandNi nt endoGame Boypl at f or m game1989PUBLI SHERGENREPLATFORMPUB. DATEathedog...companyKabushikiKoppaiNintendo...Relation toExisting EntityMention of a New EntityNot an Entity MentionDistribution over standard vocabulary and aliases of etDistribution over standard vocabularystandard vocabularyaliases of etSELFNintendopick from all entitiesparent from local entitiesFigure 2: KGLM Illustration. When trying to generate the token following “published by”, the model ﬁrst decidesthe type of the mention (tt) to be a related entity (darker indicates higher probability), followed by identifying theparent (pt), relation (rt), and entity to render (et) from the local knowledge graph as (Super Mario Land, Publisher,Nintendo). The ﬁnal distribution over the words includes the standard vocabulary along with aliases of Nintendo,and the model selects “Nintendo” as the token xt. Facts related to Nintendo will be added to the local graph.• Decide the type of xt, which we denote bytt: whether it is a reference to an entity inKG<t (related), a reference to an entity not inKG<t (new), or not an entity mention (∅).• If tt = new then choose the upcoming entity etfrom the set of all entities E.• If tt = related then:– Choose a parent entity pt from E<t.– Choose a factual relation rt to render,rt ∈{(p, r, e) ∈KG<t|p = pt}.– Choose et as one of the tail entities,et ∈{e|(pt, rt, e) ∈KG<t}.• If tt = ∅then et = ∅.• Generate xt conditioned on et, potentially copy-ing one of et’s aliases.• If et /∈E<t, then E<(t+1) ←E<t ∪{et},else E<(t+1) ←E<t.For the model to refer to an entity it has alreadymentioned, we introduce a Reﬂexive relation thatself-relates, i.e. p = e for (p, Reﬂexive, e).An illustration of this process and the variablesis provided in Figure 2, for generating a token inthe middle of the same sentence as in Figure 1.Amongst the three mention types (tt), the modelchooses a reference to existing entity, which re-quires picking a fact to render. As the parent entityof this fact (pt), the model picks Super Mario Land,and then follows the Publisher relation (rt) to se-lect Nintendo as the entity to render (et). Whenrendering Nintendo as a token xt, the model has anexpanded vocabulary available to it, containing thestandard vocabulary along with all word types inany of the aliases of et.Marginalizing out the KG There is a mismatchbetween our initial task requirement, p(xt|x<t),and the model we describe so far, which computesp(xt, Et|x<t, E<t). We will essentially marginal-ize out the local knowledge graph to compute theprobability of the tokens, i.e. p(x) = PE p(x, E).We will clarify this, along with describing the train-ing and the inference/decoding algorithms for thismodel and other details of the setup, in Section 4.2.3Parameterizing the DistributionsThe parametric distributions used in the generativeprocess above are deﬁned as follows. We beginby computing the hidden state ht using the for-mula in Eqn (1). We then split the vector intothree components: ht = [ht,x; ht,p; ht,r], whichare respectively used to predict words, parents, andrelations. The type of the token, tt, is computedusing a single-layer softmax over ht,x to predictone of {new, related, ∅}.Picking an Entity We also introduce pretrainedembeddings for all entities and relations in the\f5965knowledge graph, denoted by ve for entity e andvr for relation r. To select et from all entities incase tt = new, we use:p(et) = softmax(ve · (ht,p + ht,r))over all e ∈E. The reason we add ht,p and ht,r isto mimic the structure of TransE, which we use toobtain entity and relation embeddings. Details onTransE will be provided in Section 4. For mentionof a related entity, tt = related, we pick a parententity pt usingp(pt) = softmax(vp · ht,p)over all p ∈Et, then pick the relation rt usingp(rt) = softmax(vr · ht,r)over all r ∈{r|(pt, r, e) ∈KGt}. The combina-tion of pt and rt determine the entity et (whichmust satisfy (pt, rt, et) ∈KGt; if there are multi-ple options one is chosen at random).Rendering the Entity If et = ∅, i.e. there isno entity to render, we use the same distributionover the vocabulary as in Eqn (1) - a softmax usinght,x. If there is an entity to render, we constructthe distribution over the original vocabulary anda vocabulary containing all the tokens that appearin aliases of et. This distribution is conditionedon et in addition to xt. To compute the scoresover the original vocabulary, ht,x is replaced byh′t,x = Wproj[ht,x; vet] where Wproj is a learnedweight matrix that projects the concatenated vectorinto the same vector space as ht,x.To obtain probabilities for words in the aliasvocabulary, we use a copy mechanism Gu et al.(2016). The token sequences comprising each alias{aj} are embedded then encoded using an LSTMto form vectors aj. Copy scores are computed as:p(xt = aj) ∝exphσ\u0010\u0000h′t,x\u0001T Wcopy\u0011aji3Linked WikiText-2Modeling aside, one of the primary barriers to in-corporating factual knowledge into language mod-els is that training data is hard to obtain. Standardlanguage modeling corpora consist only of text,and thus are unable to describe which entities orfacts each token is referring to. In contrast, whilerelation extraction datasets link text to a knowledgegraph, the text is made up of disjoint sentences thatdo not provide sufﬁcient context to train a pow-erful language model. Our goals are much morealigned to the data-to-text task (Ahn et al., 2016;Lebret et al., 2016; Wiseman et al., 2017; Yanget al., 2017; Gardent et al., 2017; Ferreira et al.,2018), where a small table-sized KB is provided togenerate a short piece of text; we are interested inlanguage models that dynamically decide the factsto incorporate from the knowledge graph, guidedby the discourse.For these reasons we introduce the LinkedWikiText-2 dataset, consisting of (approximately)the same articles appearing in the WikiText-2 lan-guage modeling corpus, but linked to the Wiki-data (Vrandeˇci´c and Krötzsch, 2014) knowledgegraph. Because the text closely matches, mod-els trained on Linked WikiText-2 can be comparedto models trained on WikiText-2.Furthermore,because many of the facts in Wikidata are de-rived from Wikipedia articles, the knowledge graphhas a good coverage of facts expressed in thetext.The dataset is available for download at:https://rloganiv.github.io/linked-wikitext-2. Oursystem annotates one document at a time, and con-sists of entity linking, relation annotations, andpost-processing.The following paragraphs de-scribe each step in detail.Initial entity annotations We begin by identify-ing an initial set of entity mentions within the text.The primary source of these mentions is the human-provided links between Wikipedia articles. When-ever a span of text is linked to another Wikipediaarticle, we associate its corresponding Wikidataentity with the span. While article links provide alarge number of gold entity annotations, they are in-sufﬁcient for capturing all of the mentions in the ar-ticle since entities are only linked the ﬁrst time theyoccur. Accordingly, we use the neural-el (Guptaet al., 2017) entity linker to identify additional linksto Wikidata, and identify coreferences using Stan-ford CoreNLP2 to cover pronouns, nominals, andother tokens missed by the linker.Local knowledge graph The next step iterativelycreates a generative story for the entities using rela-tions in the knowledge graph as well as identiﬁesnew entities. To do this, we process the text tokenby token. Each time an entity is encountered, weadd all of the related entities in Wikidata as candi-2https://stanfordnlp.github.io/CoreNLP/\f5966Tokens xt Super Mario Land is a1989side - scrolling platform video game developedMention typettnew∅∅relatednewrelated∅Entity Mentioned etSML∅∅04-21-1989 SIDE_SCROLLPVG∅Relation rt∅∅∅pub date∅genre∅Parent Entity pt∅∅∅SML∅SML∅xt and published by Nintendo as a launch title for their GameBoyhandheld game console.tt∅∅∅related∅∅new∅∅relatedrelated∅et∅∅∅NIN∅∅LT∅∅GAME_BOYHGC∅rt∅∅∅pub∅∅∅∅∅R:manu / platforminstance of∅pt∅∅∅SML∅∅∅∅∅NIN / SMLGAME_BOY∅Table 1: Example Annotation of the sentence from Figure 1, including corresponding variables from Figure 2.Note that Game Boy has multiple parent and relation annotations, as the platform for Super Mario Land and asmanufactured by Nintendo. Wikidata identiﬁers are made human-readable (e.g., SML is Q647249) for clarity.dates for matching. If one of these related entitiesis seen later in the document, we identify the entityas a parent for the later entity. Since multiple re-lations may appear as explanations for each token,we allow a token to have multiple facts.Expanding the annotations Since there may beentities that were missed in the initial set, as wellas non-entity tokens of interest such as dates andquantities we further expand the entity annotationsusing string matching. For entities, we match theset of aliases provided in Wikidata. For dates, wecreate an exhaustive list of all of the possible waysof expressing the date (e.g. \"December 7, 1941\",\"7-12-1941\", \"1941\", ...). We perform a similarapproach for quantities, using the pint library inPython to handle the different ways of expressingunits (e.g. \"g\", \"gram\", ...). Since there are manyways to express a numerical quantity, we only ren-der the quantity at the level of precision suppliedby Wikidata, and do not perform unit conversions.Example Annotation An example annotation isprovided in Table 1 corresponding to the instance inFigure 1, along with the variables that correspondto the generative process of the knowledge graphlanguage model (KGLM). The entity mentioned formost tokens here are human-provided links, apartfrom “1989” that is linked to 04-21-1989 by thestring matching process. The annotations indicatewhich of the entities are new and related based onwhether they are reachable by entities linked so far,clearly making a mistake for side-scrolling gameand platform video game due to missing links inWikidata. Finally, multiple plausible reasons forGame Boy are included: it’s the platform for SuperMario Land and it is manufactured by Nintendo,even though only the former is more relevant here.TrainDevTestDocuments6006060Tokens2,019,195207,982236,062Vocab. Size33,558--Mention Tokens207,80321,22624,441Mention Spans122,98312,21415,007Unique Entities41,0585,4155,625Unique Relations1,291484504Table 2: Linked WikiText-2 Corpus Statistics.Even with these omissions and mistakes, it is clearthat the annotations are rich and detailed, with ahigh coverage, and thus should prove beneﬁcial fortraining knowledge graph language models.Dataset Statistics Statistics for Linked WikiText-2are provided in Table 2. In this corpus, more than10% of the tokens are considered entity tokens, i.e.they are generated as factual references to informa-tion in the knowledge graph. Each entity is onlymentioned a few times (less than 5 on average, witha long tail), and with more than thousand differentrelations. Thus it is clear that regular languagemodels would not be able to generate factual text,and there is a need for language models to be ableto refer to external sources of information.Differences from WikiText-2Although ourdataset is designed to closely replicate WikiText-2,there are some differences that prevent direct com-parison. Firstly, there are minor variations in textacross articles due to edits between download dates.Secondly, according to correspondence with Merityet al. (2017), WikiText-2 was collected by queryingthe Wikipedia Text API. Because this API discardsuseful annotation information (e.g. article links),Linked WikiText-2 instead was created by directlyfrom the article HTML.\f59674Training and Inference for KGLMIn this section, we describe the training and infer-ence algorithm for KGLM.Pretrained KG Embeddings During evaluation,we may need to make predictions on entities andrelations that have not been seen during training.Accordingly, we use ﬁxed entity and relations em-beddings pre-trained using TransE (Bordes et al.,2013) on Wikidata. Given (p, r, e), we learn em-beddings vp, vr and ve to minimize the distance:δ(vp, vr, ve) = ∥vp + vr −ve∥2 .We use a max-margin loss to learn the embeddings:L = max\u00000, γ + δ (vp, vr, ve) −δ\u0000v′p, vr, v′e\u0001\u0001where γ is the margin, and either p′ or e′ is a ran-domly chosen entity embedding.Training with Linked WikiText-2 Although thegenerative process in KGLM involves many steps,training the model on Linked WikiText-2 is straight-forward. Our loss objective is the negative log-likelihood of the training data:ℓ(Θ) =Xtlog p(xt, Et|x<t, E<t; Θ),where Θ is the set of model parameters. Note thatif an annotation has multiple viable parents such asGame Boy in 1, then we marginalize over all of theparents. Since all random variables are observed,training can performed using off-the-shelf gradient-based optimizers.Inference While observing annotations makes themodel easy to train, we do not assume that themodel has access to annotations during evaluation.Furthermore, as discussed in Section 2.2, the goalin language modelling is to measure the marginalprobability p(x) = PE p(x, E) not the joint proba-bility. However, this sum is intractable to computedue to the large combinatorial space of possibleannotations. We address this problem by approxi-mating the marginal distribution using importancesampling. Given samples from a proposal distribu-tion q(E|x) the marginal distribution is:p(x) =XEp (x, E) =XEp (x, E)q (E|x) q (E|x)≈1NXE∼qp (x, E)q (E|x)This approach is used to evaluate models in Ji et al.(2017) and Dyer et al. (2016). Following Ji et al.(2017), we compute q (E|x) using a discriminativeversion of our model that predicts annotations forthe current token instead of for the next token.5ExperimentsTo evaluate the proposed language model, weﬁrst introduce the baselines, followed by an evalua-tion using perplexity of held-out corpus, accuracyon fact completion, and an illustration of how themodel uses the knowledge graph.5.1Evaluation SetupBaseline Models We compare KGLM to the fol-lowing baseline models:• AWD-LSTM (Merity et al., 2018):strongLSTM-based model used as the foundation ofmost state-of-the-art models on WikiText-2.• ENTITYNLM (Ji et al., 2017): an LSTM-basedlanguage model with the ability to track entitymentions. Embeddings for entities are created dy-namically, and are not informed by any externalsources of information.• EntityCopyNet: a variant of the KGLM wherett = new for all mentions, i.e.entities areselected from E and entity aliases are copied, butrelations in the knowledge graph are unused.Hyperparameters We pre-train 256 dimensionalentity and relation embeddings for all entitieswithin two hops of the set of entities that occur inLinked WikiText-2 using TransE with margin γ = 1.Weights are tied between all date embeddings andbetween all quantity embeddings to save memory.Following Merity et al. (2018) we use 400 dimen-sional word embeddings and a 3 layer LSTM withhidden dimension 1150 to encode tokens. We alsoemploy the same regularization strategy (DropCon-nect (Wan et al., 2013) + Dropout(Srivastava et al.,2014)) and weight tying approach. However, weperform optimization using Adam (Kingma and Ba,2015) with learning rate 1e-3 instead of NT-ASGD,having found that it is more stable.5.2ResultsPerplexity We evaluate our model using the stan-dard perplexity metric: exp\u00101TPTt=1 log p(xt)\u0011.However, perplexity suffers from the issue that it\f5968PPLUPPENTITYNLM* (Ji et al., 2017)85.4189.2EntityCopyNet*76.1144.0AWD-LSTM (Merity et al., 2018)74.8165.8KGLM*44.188.5Table 3: Perplexity Results on Linked WikiText-2. Re-sults for models marked with * are obtained using im-portance sampling.overestimates the probability of out-of-vocabularytokens when they are mapped to a single UNKtoken. This is problematic for comparing the per-formance of the KGLM to traditional languagemodels on Linked WikiText-2 since there are a largenumber of rare entities whose alias tokens are out-of-vocabulary. That is, even if the KGLM identiﬁesthe correct entity and copies the correct alias tokenwith high probability, other models can attain bet-ter perplexity by assigning a higher probability toUNK. Accordingly, we also measure unknown pe-nalized perplexity (UPP) (a.k.a adjusted perplexity)introduced by Ueberla (1994), and used recentlyby Ahn et al. (2016) and Spithourakis and Riedel(2018). This metric penalizes the probability ofUNK tokens by evenly dividing their probabilitymass over U, the set of tokens that get mappedto UNK . We can be compute UPP by replacingp(UNK) in the perplexity above by1|U|p(UNK),where |U| is estimated from the data.We present the model perplexities in Table 3. Tomarginalize over annotations, perplexities for theENTITYNLM, EntityCopyNet, and KGLM are es-timated using the importance sampling approachdescribed in Section 4. We observe that the KGLMattains substantially lower perplexity than the otherentity-based language models (44.1 vs. 76.1/85.4),providing strong evidence that leveraging knowl-edge graphs is crucial for accurate language mod-eling. Furthermore, KGLM signiﬁcantly outper-forms all models in unknown penalized perplexity,demonstrating its ability to generate rare tokens.Fact Completion Since factual text generationis our primary objective, we evaluate the abilityof language models to complete sentences withfactual information. We additionally compare withthe small GPT-2 (Radford et al., 2019), a languagemodel trained on a much larger corpus of text. Weselect 6 popular relations from Freebase, and writea simple completion template for each, such as “Xwas born in” for the birthplace relation. WeAWD-LSTMGPT-2KGLMOracleNELnation-capital0 / 06 / 70 / 00 / 4birthloc0 / 914 / 1494 / 9585 / 92birthdate0 / 258 / 965 / 6861 / 67spouse0 / 02 / 32 / 21 / 19city-state0 / 1362 / 629 / 594 / 59book-author0 / 20 / 061 / 6225 / 28Average0.0/8.215.3/15.838.5/47.729.3/44.8Table4:FactCompletion.Top-kaccuracy(@1/@5,%) for predicting the next token for an incom-plete factual sentence. See examples in Table 5.generate sentences for these templates for a numberof (X, Y ) pairs for which the relation holds, andmanually examine the ﬁrst token generated by eachlanguage model to determine whether it is correct.Table 4 presents performance of each languagemodel on the relations. The oracle KGLM is giventhe correct entity annotation for X, while the NELKGLM uses the discriminative model used for im-portance sampling combined with the NEL entitylinker to produce an entity annotation for X.Amongst models trained on the same data, bothKGLM variants signiﬁcantly outperform AWD-LSTM; they produce accurate facts, while AWD-LSTM produced generic, common words. KGLMsare also competitive with models trained on ordersof magnitude more data, producing factual com-pletions that require speciﬁc knowledge, such asbirthplaces, dates, and authors. However, they donot capture facts or relations that frequently appearin large corpora, like the cities within states.3 It isencouraging to see that the KGLM with automaticlinking performs comparably to oracle linking.We provide examples in Table 5 to highlightqualitative differences between KGLM, trained on600 documents, and the recent state-of-the-art lan-guage model, GPT-2, trained on the WebText cor-pus with over 8 million documents (Radford et al.,2019). For examples that both models get factu-ally correct or incorrect, the generated tokens byKGLM are often much more speciﬁc, as opposedto selection of more popular/generic tokens (GPT-2often predicts “New York” as the birthplace, evenfor popular entities). KGLM, in particular, getsfactual statements correct when the head or tail en-tities are rare, while GPT-2 can only complete factsfor more-popular entities while using more-generictokens (such as “January” instead of “20”).3This is not a failure of the KG, but of the model’s abilityto pick the correct relation from the KG given the prompt.\f5969Input SentenceGoldGPT-2KGLMBoth correctParis Hilton was born inNew York CityNew1981Arnold Schwarzenegger was born on1947-07-30July30KGLM correctBob Dylan was born inDuluthNewDuluthBarack Obama was born on1961-08-04JanuaryAugustUlysses is a book that was written byJames JoyceaJamesGPTv2 correctSt. Louis is a city in the state ofMissouriMissouriOldhamRichard Nixon was born on1913-01-09January20Kanye West is married toKim KardashianKimtheBoth incorrectThe capital of India isNew DelhitheaMadonna is married toCarlos LeonaAlexTable 5: Completion Examples. Examples of fact completion by KGLM and GPT-2, which has been trained ona much larger corpus. GPT-2 tends to produce very common and general tokens, such as one of a few popularcities to follow “born in”. KGLM sometimes makes mistakes in linking to the appropriate fact in the KG, however,the generated facts are more speciﬁc and contain rare tokens. We omit AWD-LSTM from this ﬁgure as it rarelyproduced tokens apart from the generic “the” or “a”, or “⟨UNK⟩”.Effect of changing the KG For most languagemodels, it is difﬁcult to control their generationsince factual knowledge is entangled with gener-ation capabilities of the model. For KGLM, anadditional beneﬁt of its use of an external sourceof knowledge is that KGLM is directly control-lable via modiﬁcations to the KG. To illustrate thiscapability with a simple example, we create com-pletion of “Barack Obama was born on” withthe original fact (Barack Obama, birthDate, 1961-08-04), resulting in the top three decoded tokensas “August”, “4”, “1961”. After changing the birthdate to 2013-03-21, the top three decoded tokensbecome “March”, “21”, “2013”. Thus, changingthe fact in the knowledge graph directly leads to acorresponding change in the model’s prediction.6Related WorkKnowledge-based language models Our workdraws inspiration from two existing knowledge-based language models:(i) ENTITYNLM (Ji et al., 2017) which im-proves a language model’s ability to track entitiesby jointly modeling named entity recognition andcoreference. Our model similarly tracks entitiesthrough a document, improving its ability to gener-ate factual information by modeling entity linkingand relation extraction.(ii) The neural knowledge language model(NKLM) (Ahn et al., 2016) which established theidea of leveraging knowledge graphs in neural lan-guage models. The main differentiating factor be-tween the KGLM and NKLM is that the KGLMoperates on an entire knowledge graph and can beevaluated on text without additional conditioninginformation, whereas the NKLM operates on a rel-atively smaller set of predeﬁned edges emanatingfrom a single entity, and requires that entity be pro-vided as conditioning information ahead of time.This requirement precludes direct comparison be-tween NKLM and the baselines in Section 5.Data-to-text generation Our work is also relatedto the task of neural data-to-text generation. Fora survey of early non-neural text generation meth-ods we refer the reader to Reiter and Dale (1997).Recent neural methods have been applied to gener-ating text from tables of sports statistics (Wisemanet al., 2017), lists and tables (Yang et al., 2017), andWikipedia info-boxes (Lebret et al., 2016). The pri-mary difference between these works and ours isour motivation. These works focus on generatingcoherent text within a narrow domain (e.g. sports,recipes, introductory sentences), and optimize met-rics such as BLEU and METEOR score. Our focusinstead is to use a large source of structured knowl-edge to improve language model’s ability to handlerare tokens and facts on a broad domain of topics,and our emphasis is on improving perplexity.General language modeling Also related are therecent papers proposing modiﬁcations to the AWD-LSTM that improve performance on Wikitext-2 (Gong et al., 2018; Yang et al., 2018; Krauseet al., 2018). We chose to benchmark against AWD-LSTM since these contributions are orthogonal,and many of the techniques are compatible withthe KGLM. KGLM improves upon AWD-LSTM,and we expect using KGLM in conjunction withthese methods will yield further improvement.\f59707Conclusions and Future WorkBy relying on memorization, existing languagemodels are unable to generate factually correct textabout real-world entities. In particular, they areunable to capture the long tail of rare entities andword types like numbers and dates. In this work,we proposed the knowledge graph language model(KGLM), a neural language model that can accessan external source of facts, encoded as a knowledgegraph, in order to generate text. Our implementa-tion is available at: https://github.com/rloganiv/kglm-model. We also introduced Linked WikiText-2 containing text that has been aligned to facts inthe knowledge graph, allowing efﬁcient trainingof the model. Linked WikiText-2 is freely avail-able for download at: https://rloganiv.github.io/linked-wikitext-2. In our evaluation, we showedthat by utilizing this graph, the proposed KGLMis able to generate higher-quality, factually correcttext that includes mentions of rare entities and spe-ciﬁc tokens like numbers and dates.This work lays the groundwork for future re-search into knowledge-aware language modeling.The limitations of the KGLM model, such as theneed for marginalization during inference and re-liance on annotated tokens, raise new research prob-lems for advancing neural NLP models. Our dis-tantly supervised approach to dataset creation canbe used with other knowledge graphs and otherkinds of text as well, providing opportunities foraccurate language modeling in new domains.AcknowledgementsFirst and foremost, we would like to thank StephenMerity for sharing the materials used to collect theWikiText-2 dataset, and Nitish Gupta for modify-ing his entity linker to assist our work. We wouldalso like to thank Dheeru Dua and Anthony Chenfor their thoughtful feedback. This work was sup-ported in part by Allen Institute of Artiﬁcial In-telligence (AI2), and in part by NSF award #IIS-1817183. The views expressed are those of theauthors and do not reﬂect the ofﬁcial policy or po-sition of the funding agencies.ReferencesSungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, andYoshua Bengio. 2016. A neural knowledge languagemodel. ArXiv:1608.00318.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.2013. Translating embeddings for modeling multi-relational data. In Proc. of NeurIPS.Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,and Noah A. Smith. 2016. Recurrent neural networkgrammars. In Proc. of NAACL.Thiago Castro Ferreira, Diego Moussallem, EmielKrahmer, and Sander Wubben. 2018. Enriching theWebNLG corpus. In Proc. of INLG.Claire Gardent, Anastasia Shimorina, Shashi Narayan,and Laura Perez-Beltrachini. 2017. The WebNLGchallenge: Generating text from RDF data. In Proc.of INLG.Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,and Tie-Yan Liu. 2018. Frage: frequency-agnosticword representation. In Proc. of NeurIPS.Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.Li. 2016.Incorporating copying mechanism insequence-to-sequence learning. In Proc. of ACL.Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En-tity linking via joint encoding of types, descriptions,and context. In Proc. of EMNLP.Sepp Hochreiter and Jürgen Schmidhuber. 1997.Long short-term memory.Neural computation,9(8):1735–1780.Yangfeng Ji, Chenhao Tan, Sebastian Martschat, YejinChoi, and Noah A. Smith. 2017.Dynamic entityrepresentations in neural language models. In Proc.of EMNLP.Diederik P. Kingma and Jimmy Ba. 2015.Adam:A method for stochastic optimization. In Proc. ofICLR.Ben Krause, Emmanuel Kahembwe, Iain Murray, andSteve Renals. 2018. Dynamic evaluation of neuralsequence models. In Proc. of ICML.Rémi Lebret, David Grangier, and Michael Auli. 2016.Neural text generation from structured data withapplication to the biography domain.In Proc. ofEMNLP.Stephen Merity, Nitish Shirish Keskar, and RichardSocher. 2018. Regularizing and optimizing LSTMlanguage models. In Proc. of ICLR.Stephen Merity, Caiming Xiong, James Bradbury, andRichard Socher. 2017. Pointer sentinel mixture mod-els. In Proc. of ICLR.\f5971Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, JanˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrentneural network based language model. In Proc. ofINTERSPEECH.Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners. Techni-cal report, OpenAI.Ehud Reiter and Robert Dale. 1997. Building appliednatural language generation systems. Natural Lan-guage Engineering, 3(1):57–87.Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio,Aaron Courville, and Joelle Pineau. 2016. Buildingend-to-end dialogue systems using generative hierar-chical neural network models. In Proc. of AAAI.Georgios P. Spithourakis and Sebastian Riedel. 2018.Numeracy for language models: Evaluating and im-proving their ability to predict numbers. In Proc. ofACL.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov. 2014.Dropout: a simple way to prevent neural networksfrom overﬁtting. The Journal of Machine LearningResearch, 15(1):1929–1958.Trieu H. Trinh and Quoc V. Le. 2019. Do languagemodels have common sense? In Proc. of ICLR.Joerg Ueberla. 1994.Analysing a simple languagemodelÂ·some general conclusions for languagemodels for speech recognition. Computer Speech &Language, 8(2):153 – 176.Oriol Vinyals and Quoc V. Le. 2015.A neural con-versational model. Proc. of ICML Deep LearningWorkshop.Denny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-data: A free collaborative knowledgebase. Commu-nications of the ACM, 57(10):78–85.Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun,and Rob Fergus. 2013. Regularization of neural net-works using dropconnect. In Proc. of ICML.Sam Wiseman, Stuart M. Shieber, and Alexander M.Rush. 2017. Challenges in data-to-document gener-ation. In Proc. of EMNLP.Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, andWilliam W Cohen. 2018. Breaking the softmax bot-tleneck: A high-rank RNN language model. In Proc.of ICLR.Zichao Yang, Phil Blunsom, Chris Dyer, and WangLing. 2017. Reference-aware language models. InProc. of EMNLP.",
        "answer": "The top-1 accuracy of the Oracle KGLM on birthdate prediction is 65%.",
        "space_key_index": 1,
        "metadata": {
            "format": "PDF 1.3",
            "title": "Barack's Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling",
            "author": "Robert Logan ; Nelson F. Liu ; Matthew E. Peters ; Matt Gardner ; Sameer Singh",
            "subject": "P19-1 2019",
            "keywords": "",
            "creator": "LaTeX with hyperref package",
            "producer": "GPL Ghostscript 9.18",
            "creationDate": "D:20190604063539Z00'00'",
            "modDate": "D:20190604063539Z00'00'",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "How many categories of aggression were participants asked to classify texts into?",
        "context": "Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying, pages 1–11Santa Fe, USA, August 25, 2018.1Benchmarking Aggression Identiﬁcation in Social MediaRitesh Kumar1, Atul Kr. Ojha2, Shervin Malmasi3, Marcos Zampieri41Bhim Rao Ambedkar University, 2Jawaharlal Nehru University,3Harvard Medical School, 4University of Wolverhampton,AbstractIn this paper, we present the report and ﬁndings of the Shared Task on Aggression Identiﬁcationorganised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1)at COLING 2018. The task was to develop a classiﬁer that could discriminate between OvertlyAggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants wereprovided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each inHindi (in both Roman and Devanagari script) and English for training and validation. For testing,two different sets - one from Facebook and another from a different social media - were provided.A total of 130 teams registered to participate in the task, 30 teams submitted their test runs,and ﬁnally 20 teams also sent their system description paper which are included in the TRACworkshop proceedings. The best system obtained a weighted F-score of 0.64 for both Hindi andEnglish on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50for English and Hindi respectively. The results presented in this report depict how challengingthe task is. The positive response from the community and the great levels of participation in theﬁrst edition of this shared task also highlights the interest in this topic.1IntroductionIn the last decade, with the emergence of an interactive web and especially popular social networkingand social media platforms like Facebook and Twitter, there has been an exponential increase in theuser-generated content being made available over the web. Now any information online has the powerto reach billions of people within a matter of seconds. This has resulted in not only positive exchange ofideas but has also lead to a widespread dissemination of aggressive and potentially harmful content overthe web. While most of the potentially harmful incidents like bullying or hate speech have predated theInternet, the reach and extent of Internet has given these incidents an unprecedented power and inﬂuenceto affect the lives of billions of people. It has been reported that these incidents have not only createdmental and psychological agony to the users of the web but has in fact forced people to deactivate theiraccounts and in extreme cases also commit suicides (Hinduja and Patchin, 2010). Thus the incidents ofaggression and unratiﬁed verbal behaviour have not remained a minor nuisance, but have acquired theform of a major criminal activity that affects a large number of people. It is therefore important thatpreventive measures can be taken to cope with abusive behaviour aggression online.One of the strategies to cope with aggressive behaviour online is to manually monitor and moderateuser-generated content, however, the amount and pace at which new data is being created on the web hasrendered manual methods of moderation and intervention almost completely impractical. As such theuse (semi-) automatic methods to identify such behaviour has become important and has attracted moreattention from the research community in recent years (Davidson et al., 2017; Malmasi and Zampieri,2017).This work is licensed under a Creative Commons Attribution 4.0 International License.License details:http://creativecommons.org/licenses/by/4.0/\f2This paper reports the results of the ﬁrst Shared Task on Aggression Identiﬁcation which was organisedjointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018.2Related WorkVerbal aggression per se has been rarely explored within the ﬁeld of Natural Language Processing.However, previous research in the ﬁeld has been carried out to automatically recognise several relatedbehaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al.,2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014;Hee et al., 2015), ﬂaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen etal., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017;Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana,2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wie-gand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy andSmeaton, 2004; Greevy, 2004) and others. In addition to these, there have been some pragmatic studieson behaviour like trolling (Hardaker, 2010; Hardaker, 2013).This huge interest in the ﬁeld from different perspectives has created a conglomeration of terminolo-gies as well as understandings of the phenomenon. On the one hand, this provides us with a very richand extensive insight into the phenomena yet, on the other hand, it has also created a theoretical gapin the understanding of interrelationship among these. Moreover, it has also resulted in duplication ofresearch, to certain extent, and a certain kind of lack of focus and reusability of datasets across differentstrands of research. In order to make improvements towards solving a complex phenomenon like this,it is of utmost importance that some kind of uniform understanding of problem be achieved so that, atleast, standardised datasets and an understanding of different approaches to solving the problem may bedeveloped.While a large part of the research has focused on any one of these phenomena and their computationalprocessing, it seems there is a signiﬁcant overlap among these phenomenon in the way they are under-stood in these studies - and because of this underlying overlap, insights from different studies mightprove useful for solving these seemingly different phenomena. All of these behaviours are consideredundesirable, aggressive and detrimental for those on the receiving end. So, trolling is intended “to causedisruption and/or to trigger or exacerbate conﬂict for the purposes of their own amusement” (Hardaker,2010). Cyberbullying is “humiliating and slandering behavior towards other people” (Nitta et al., 2013).Flaming intends “to offend someone through e-mail, posting, commenting or any statement using insults,swearing and hostile, intense language, trolling, etc.” (Krol, 1992).Waseem et al. (2017) makes an attempt to unify these different trends of research in what may be con-sidered a signiﬁcantly overlapping ﬁeld and proposes a 2-way typology for understanding what they call’abusive language’ over the web. They propose 2 scales on which abusive language could be categorised- the target of the abuse (an individual or a group) and the nature of the language (explicit or implicit).Our classiﬁcation of aggression into overt and covert aggression is largely similar to the explicit-implicitdistinction. However, we make a more detailed distinction in relation to the target of the abuse (Kumaret al., 2018b) and it is not made along the axis of individual vs. group. This is so because we noticedin a large number of instances both individuals and groups are simultaneously targeted - in such casesindividuals are targeted as members of certain groups or the individuals’ actions were considered thoseof the group and became the locus of attack. As such it was not feasible to distinguish between theindividual and group attack in lot of instances while annotating the dataset. The distinction that we madewas related to the “locus” of attack and included such targets as gender, religion, caste, country of origin,race, etc. This classiﬁcation, on the one hand, gave scope for focusing on different kinds of attack (forexample, racial attacks or communal attacks) and, on the other hand, each of these targets may actuallybe attacked using a different set of vocabulary, thereby, making these more natural classes that could beclassiﬁed using the surface-level linguistic features. Of course, it cannot be denied that these targets arenot mutually exclusive and, as such, it makes the problem not just a multi-class classiﬁcation problembut also multi-label classiﬁcation problem. In addition to this, we also make use of a different terminol-\f3ogy taking into account its use within socio-pragmatics. This was done with an understanding that hugeamount of literature within the ﬁeld of aggression and impoliteness studies might be able to contributeand provide insights to understanding the phenomenon in a better way.The aim of this shared task was much simpler than the one discussed in the previous para. It onlyinvolved classiﬁcation of the texts into 3 categories - overt aggression, covert aggression and non-aggression. We wanted to use the dataset for experimenting with different approaches to make the mosttop-level classiﬁcation of aggression on social media.3Task Setup and ScheduleThe participants interested in competing in the shared task were required to register using a Google Form.The form gave them an option to participate for either English or Hindi or both the languages. All theregistered participants were sent the links to the annotated dataset in the language(s) of their choice, alongwith a description of the format of the dataset. The participants were allowed to use additional data fortraining the system, with the condition that the additional dataset should be either publicly available ormake available immediately after submission (and well before the submission of the system papers) andthis must be mentioned in the submission. Use of non-public additional data for training was not allowed.The participants were given around 6 weeks to experiment and develop the system. However, since morethan half of the participants registered after the ﬁrst release of the data, most of them got less time thanthis. Initially, the dataset was not released publicly but was emailed only to the registered participants.After the 6 weeks of release of train and dev sets, the test set was released and the participants had 5 daysto test and upload their system. The complete timeline of the shared task is given in Table 1. We madeuse of CodaLab 1 for the evaluation. Each team was allowed to submit up to 3 systems for evaluation.We used the best of the 3 runs for the ﬁnal ranking and evaluation of the systems.DateEvent1 February, 2018Shared Task Announcement and Start of Registration13 March, 2018Release of train and dev sets25 April, 2018Release of test set30 April, 2018Deadline for Submission of System2 May, 2018Declaration of Results28 May, 2018Deadline for Submission of System Description PaperTable 1: Timeline of the Aggression Identiﬁcation Shared Task at TRAC - 1.4DatasetThe participants of the shared task were provided with a dataset of 12,000 randomly sampled Face-book comments for training and 3,000 comments for development and in English and Hindi each, an-notated with 3 levels of aggression - Overtly Aggressive (OAG), Covertly Aggressive (CAG) and Non-Aggressive (NAG). For test, 916 English comments and 970 Hindi comments were provided. Addi-tionally, 1,257 English tweets and 1,194 Hindi tweets were given as the surprise test set 2. The datasetreleased for the task is a subset of a larger dataset discussed in Kumar et al. (2018b).4.1Issues with the DatasetWhile most of the participants considered the dataset to be of high quality, two major problems came upduring the task -• The language issue: Some of the comments in English dataset contained code-mixed Hindi-Englishdata as well as data from other languages like German. These formed a minuscule proportion of thedata but nevertheless these need to be ﬁltered out.1https://competitions.codalab.org/2The complete dataset used for the shared task can be downloaded here - http://trac1-dataset.kmiagra.org/\f4• The annotation issue: The second and more serious issue that was raised by some participantsis related to the the annotation itself. Several instances of supposedly inaccurate annotation werepointed out. Despite the fact that aggression is a highly subjective phenomenon and different anno-tators may have different judgments about the same comment, some of the annotation indeed lookedhighly implausible and consequently it needs further scrutiny and validation.5Participants and ApproachesThe shared task gave the participants an option to register for either one of the two languages - Englishor Hindi - or both. A total of 131 participants registered for the shared task, with 73 teams registeringto participate only in English track, 2 teams only in Hindi track and 56 teams registered to participatein both the tracks. Out of these, ﬁnally a total of 30 teams submitted their systems - 15 teams for bothEnglish and Hindi and 30 teams for only English track. All the systems who submitted their systemwere invited to submit the system description paper, describing the experiments conducted by them. 18participants submitted the ﬁnal description paper which are included in the workshop proceedings - itincluded papers by majority of the top 10 teams. Table 2, lists the participating teams and the languagethey took part in.TeamHindiEnglishSystem Description Papersaroyehun✓(Aroyehun and Gelbukh, 2018)EBSI-LIA-UNAM✓(Arroyo-Fern´andez et al., 2018)DA-LD-Hildesheim✓✓(Modha et al., 2018)TakeLab✓(Golem et al., 2018)sreeIN✓(Madisetty and Desarkar, 2018)Julian✓✓(Risch and Krestel, 2018)taraka rama✓✓uOttawa✓(Orabi et al., 2018)Isistanitos✓(Tommasel et al., 2018)hakuchumu✓DataGeeks✓✓na14✓✓(Samghabadi et al., 2018)dinel✓(Orasan, 2018)vista.ue✓✓(Raiyani et al., 2018)MANITBHOPALINDIA✓✓IRIT✓(Ramiandrisoa and Mothe, 2018)quine✓✓(Nikhil et al., 2018)IIIT-Delhi✓PMRS✓✓(Maitra and Sarkhel, 2018)resham✓✓IreneR✓Nestor✓✓UAEMex-UAPT1✓✓forest and trees✓(Galery et al., 2018)groutar✓(Fortuna et al., 2018)Shusrut✓✓(Roy et al., 2018)malaypramanick✓UAEMex-UAPT-TAC2✓✓Unito✓✓bhanodaig✓(Kumar et al., 2018a)Total153018Table 2: The teams that participated in the Aggression Identiﬁcation Shared Task at TRAC - 1.Next we give a short description of the approach taken by each team for building their system. Moredetails about the approaches could be found in the paper submitted by the respective teams.\f5• saroyehun system gives the best performance with LSTM and they resorted to translation as dataaugmentation strategy. With the surprise twitter set, a combination of the representations of theRNN and CNN as features, along with additional preprocessing like spelling correction, translationof emoji, and computation of sentiment score gave the best performance. In this case, the datasetwas also augmented using translation and pseudolabelled using an external dataset on hate speech.3This is the only approach in the competition that performs better on the Twitter dataset, despitebeing trained the Facebook dataset, thereby, depicting the ability of the approach to generaliseacross domain.• EBSI-LIA-UNAM system uses a combination of the Passive-Aggressive (PA) and SVM classiﬁerswith character based n-gram (1 - 5 grams) TF-IDF for feature representation.• DA-LD-Hildesheim uses LSTM with pretrained Fasttext vector for embeddings for classifyingEnglish Facebook texts. For all other datasets including Twitter data in English and both Facebookand Twitter dataset in Hindi, CNN performs better.• TakeLab uses a Bidirectional LSTM on Glove embeddings to give the best performance.• sreeIN system uses a voting-based ensemble method with 3 classiﬁers - CNN with 4 layers, LSTMand Bidirectional LSTM.• Julian team uses translation as data augmentation strategy and use an ensemble of TF-IDF basedapproaches, using character n-grams (2 - 6) and word n-grams (1 - 2) with a bi-directional RNN,using fasttext embeddings, to get the best performance in the task..• taraka rama uses different systems for different datasets. For English Facebook dataset and HindiTwitter dataset, the team uses a stacked ensemble classiﬁer that uses a SVM on top of the ensembleof SVM classiﬁers. The SVMs were trained on 1 - 6 character n-grams and word unigrams. ForHindi Facebook and English Twitter dataset, however, a plain SVM trained using character and wordbag-of-n-grams gave the best performance. In this case, the overlapping character and word n-gramfeatures are weigthed with sublinear tf-idf before being used for training and testing. The systemis tuned using 5-fold CV on the combined training and develpment sets for maximum numberof character and word n-grams included, case normalization, and SVM margin (regularization)parameter C.• uOttawa system is trained using a novel deep-learning architecture for text classiﬁcation based onMulti-task learning (MTL). The approach, MTL, is evaluated using three neural network models.MultiCNN, multiple convolution structure with a trainable embedding layer, gives the best perfor-mance.• Isistanitos system uses a soft voting (average the class probabilities of other models) of two models- a recurrent neural network, and an SVM. The recurrent neural network uses 3 preprocesed setof features. The ﬁrst set uses an ad-hoc glove model for representing the words, the second isa sentiwornet based model, and the third is a traditional TfIdf plus Vader Sentiment analysis andsentiments associated with the emojis. The SVM model is trained on a TF-IDF of the post stemmedterms, excluding stopwords, and 3 - 5 character n-grams.• hakuchumu system makes use of a Random Forest classiﬁer with some preprocessing includingremoval of urls and non letter characters and stop words. Along with the bag-of-word, the approachuses multiple occurrences of letters, exclamation marks and question marks in a row and emoticonsas binary features.• DataGeeks system uses Logistic Regression classiﬁer with some preprocessing on the data suchas removing non-ascii characters, replacing new line with ’.’, replacing n’t with not, removingstopwords and 1 - 3 word n-grams and 2 - 6 character n-grams for training the classiﬁer.3https://github.com/ZeerakW/hatespeech\f6• na14 also uses Logistic Regression classiﬁer with preprocessing involving replacing URLs, num-bers, email addresses and spelling correction. The classiﬁer is trained using word unigrams, tf-idfvectors of word unigram, character 4-gram, character 5-gram and Google news pre-trained wordembedding model. For the Hindi dataset, Devanagari texts were transliterated into Roman at thepreprocessing stage.• dinel achieves the best accuracy on the Facebook test set using a Random Forest classiﬁer whileSVMs performed better for the surprise Twiiter test set. Both the classiﬁers were trained using 300semantic features which represent the vector representation of the text, average scores of the topemojis for each of the classes and positive and negative sentiment scores.• vista.ue system is developed using dense neural networks.• MANITBHOPALINDIA system for English is developed using SVM while for English it is trainedusing deep neural networks.• IRIT system gets the best performance for the English Facebook test set by using a combination oftwo models - a doc2vec model and a logistic regression classiﬁer. For the Twitter test set, it uses acombination of CNN and LSTM to get the best performance.• quine system is trained using an LSTM with attention and simple embeddings (word to index)instead of pre-trained embeddings.• IIIT-Delhi system uses a Single channel CNN for this task. Bayesian Optimization is used fortuning the parameters.• PMRS system employs a winner-takes- all autoencoder, called Emoti-KATE for Twitter senti-ment classiﬁcation. Each input dimension of Emoti-KATE is a log-normalized, sentiwordnet-scoreweighted word-count vector. A binary cross-entropy loss function is used to train the network.• resham system for English has been made using an open vocabulary approach and ensemble modelof two predictors with soft voting. The ﬁrst predictor is a Naive Bayes model with CountVectorizerfor preprocessing. The second predictor is a recurrent neural network with one embedding layerand two LSTM layers. Pre-trained word vectors have been used for the embedding layer. For Hindidataset, a Naive Bayes classiﬁer is trained using the dataset augmented with English translations.• IreneR system is based on a Multinomial Naive Bayes classiﬁer that uses unigrams, bigrams,hedging bigrams and trigrams such as ’do you’, someone who is’,’to see that’, that potentiallysignal covert aggressivity, identiﬁed with chi-squared test as features. It also includes features fromLIWC2015 (list of anger and swear words).• Nestor uses an approach that combines Neural Networks and a new word representation model.The patterns obtained from the word model representation are used for training the back propa-gation neural network with ﬁx parameters. The length of the post was ﬁxed and the word modelrepresentation is language independent, so it was used for both the English and the Hindi tasks.• UAEMex-UAPT1 uses the same approach as used by the team Nestor.• forest and trees system uses a Pooled Recurrent Unit architecture combined with pre-trained En-glish and Hindi fasttext word embeddings as a representation of the sequence input. In this approach,Hindi and English vectors were aligned using pre-computed SVD matrices that pulls representationsfrom different languages into a single space. This enabled the same model to be used for both thelanguages, thereby, making data re-utilization and model deployability easier.• groutar system is trained using random forests. The dataset is augmented with an external toxicitydataset 4. The approach involved understanding the effects of new data on aggression identiﬁcation.4https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\f7• Shusrut system uses an ensemble of CNN 2D with MAXPOOL classiﬁer and a SVM classiﬁer. Theensemble model is passed through 3 dense layers to ﬁnally predict the output. Softmax activation isused in the outer layer for classiﬁcation.• malaypramanick system uses a random forest classiﬁer trained using a set of surface-level fea-tures including number of line,s uppercase and lowercase letters, digits, named entities, unicodecharacters, etc.• UAEMex-UAPT-TAC2 system is generated by combination of twelve distance measures, througha K Nearest Neighbors classiﬁcation algorithm and a canonical genetic algorithm.• Unito is the only unsupervised system submitted in the task. It is based only on a multilinguallexicon of aggressive words. The lexicon is obtained by automatic translation from an handmadelexicon of offensive words in Italian, with minimal human supervision. The original words areexpanded into a list of their senses. The senses are manually annotated to ﬁlter out senses that arenever used in an offensive context. Finally, all the lemmas of the remaining senses are generatedwith BabelNet in 50+ languages. The words in the lexicon are divided in those translating sense thatcan be used in an offensive context (but not necessarily are) and words translating senses that aredirectly offensive. This distinction is mapped to the Overtly Aggressive and Covertly Aggressiveclasses respectively. The classiﬁcation of sentences is straightforward: a sentence that does notcontain any word from the lexicon is tagged as NAG, a sentence containing more directly offensivewords than potentially offensive words is tagged as OAG, and the other cases are tagged as CAG.• bhanodaig system uses a bidirectional LSTM.6ResultsIn this section, we present the results of the experiments carried out by different teams during the sharedtask. The results of the top 15 teams on English dataset is given in Figure 1 and that on Hindi dataset isin Figure 2.Figure 1: Performance of top 15 teams on English Dataset\f8The participants were allowed to use other datasets, in addition to the one provided by the organizers ofthe task. However, because of the lack of similar alternative datasets, all the groups, except ’groutar’ and’saroyehun’ team, used only the dataset provided for the task. As we mentioned earlier, the participantswere given two kinds of test sets for the ﬁnal testing of the system - one from Facebook and a surprisetest set from Twitter.Figure 2: Performance of teams on Hindi Dataset7ConclusionIn this paper, we have presented the report of the First Shared task on Aggression Identiﬁcation organizedwith the TRAC workshop at COLING 2018. The shared task received a very encouraging response fromthe community which underlines the relevance and need of the task. More than 100 teams registered and30 teams ﬁnally submitted their system.The performance of the best systems in the task show that aggression identiﬁcation is a hard problem tosolve. Moreover, the performance of the neural networks-based systems as well as the other approachesdo not seem to differ much. If the features are carefully selected then classiﬁers like SVM and evenrandom forest and logistic regression perform at par with deep neural networks. On the other had, weﬁnd quite a few neural networks-based systems not performing quite well in the task. Nonetheless, 14systems were trained using one or the other architectures of deep neural networks - either solely or aspart of an ensemble. Moreover, 8 systems out of the top 15 are trained on neural networks, which showsthe efﬁcacy of the approach but at the same time does not rule out the usefulness and relevance of linearmodels for the task. There was only one system, Unito, that made use of a lexicon-based approach tosolve the task. A few participants of the task pointed out the apparent “inconsistencies” in the annotation.It points towards the need to get the annotations validated by multiple human annotators.AcknowledgementsWe would like to thank Microsoft Research India for providing grants to prepare the dataset and to ourannotators who worked very hard to ﬁnish the annotations within a strict deadline.\f9We would also like to thank the participants of the Shared Task for their participation and feedbackand the TRAC workshop PC members for thoroughly reviewing the shared task papers within a veryshort span of time.ReferencesSwati Agarwal and Ashish Sureka. 2015. Using knn and svm based one-class classiﬁer for detecting onlineradicalization on twitter. In International Conference on Distributed Computing and Internet Technology, pages431 – 442. Springer.Swati Agarwal and Ashish Sureka. 2017. Characterizing linguistic attributes for automatic classiﬁcation of intentbased racist/radicalized posts on tumblr micro-blogging website.Segun Taofeek Aroyehun and Alexander Gelbukh. 2018. Aggression detection in social media: Using deepneural networks, data augmentation, and pseudo labeling. In Proceedings of the First Workshop on Trolling,Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Ignacio Arroyo-Fern´andez, Dominic Forest, Juan-Manuel Torres-Moreno, Mauricio Carrasco-Ruiz, Thomas Leg-eleux, and Karen Joannette. 2018. Cyber-bullying detection task: the ebsi-lia-unam system (elu) at coling’18trac-1. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe,USA.Peter Burnap and Matthew L. Williams. 2014. Hate speech, machine classiﬁcation and statistical modelling ofinformation ﬂows on twitter: Interpretation and communication for policy decision making. In Proceedings ofInternet, Policy & Politics, pages 1 – 18.Erik Cambria, Praphul Chandra, Avinash Sharma, and Amir Hussain. 2010. Do not feel the trolls. In ISWC,Shanghai.Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu. 2012. Detecting offensive language in social media to pro-tect adolescent online safety. privacy, security, risk and trust (passat). In International Conference on SocialComputing (SocialCom), pages 71–80.Maral Dadvar, Dolf Trieschnigg, Roeland Ordelman, and Franciska de Jong. 2013. Improving cyberbullyingdetection with user context. In Advances in Information Retrieval, pages 693–696. Springer.Maral Dadvar, Dolf Trieschnigg, and Franciska de Jong. 2014. Experts and machines against bullies: a hybridapproach to detect cyberbullies. In Advances in Artiﬁcial Intelligence, pages 275–281. Springer, Berlin.Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detectionand the problem of offensive language. In Proceedings of ICWSM.Karthik Dinakar, Birago Jones, Catherine Havasi Henry Lieberman, and Rosalind Picard. 2012. Common sensereasoning for detection, prevention, and mitigation of cyberbullying. ACM Transactions on Interactive Intelli-gent Systems (TiiS), 2(3):18:1–18:30.Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan Radosavljevic, and Narayan Bhamidipati.2015. Hate speech detection with comment embeddings. In Proceedings of the 24th International Conferenceon World Wide Web, pages 29 – 30.Paula Fortana. 2017. Automatic detection of hate speech in text: an overview of the topic and dataset annotationwith hierarchical classes. Master’s thesis, Faculdade de Engenharia da Universidade do Porto.Paula Fortuna, Jos´e Ferreira, Luiz Pires, Guilherme Routar, and S´ergio Nunes. 2018. Merging datasets for aggres-sive text identiﬁcation. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC– 1), Santa Fe, USA.Thiago Galery, Efstathios Charitos, and Ye Tian. 2018. Aggression identiﬁcation and multi lingual word embed-dings. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe,USA.Njagi Dennis Gitari, Zhang Zuping, Hanyurwimfura Damien, and Jun Long. 2015. A lexicon- based approach forhate speech detection. International Journal of Multimedia and Ubiquitous Engineering, 10(4):215 – 230.\f10Viktor Golem, Mladen Karan, and Jan najder. 2018. Combining traditional machine learning models with deeplearning for aggressive text detection. In Proceedings of the First Workshop on Trolling, Aggression and Cyber-bullying (TRAC – 1), Santa Fe, USA.Edel Greevy and Alan F. Smeaton. 2004. Classifying racist texts using a support vector machine. In Proceedingsof the 27th annual international ACM SIGIR conference on Research and development in information retrieval,pages 468 – 469. ACM.Edel Greevy. 2004. Automatic text categorisation of racist webpages. Ph.D. thesis, Dublin City University.Claire Hardaker. 2010. Trolling in asynchronous computer-mediated communication: From user discussions toacademic deﬁnitions. Journal of Politeness Research. Language, Behaviour, Culture, 6(2):215–242.Claire Hardaker. 2013. uh. . . . not to be nitpicky,,,,,but...the past tense of drag is dragged, not drug. an overviewof trolling strategies. Journal of Language Aggression and Conﬂict, 1(1):58–86.Cynthia Van Hee, Els Lefever, Ben Verhoeven, Julie Mennes, Bart Desmet, Guy De Pauw, Walter Daelemans, andVronique Hoste. 2015. Detection and ﬁne-grained classiﬁcation of cyberbullying events. In Proceedings ofInternational Conference Recent Advances in Natural Language Processing (RANLP), pages 672–680.Sameer Hinduja and Justin W Patchin. 2010. Bullying, Cyberbullying, and Suicide. Archives of suicide research,14(3):206–221.E. Krol. 1992. The whole internet: User’s guide & catalog. O’Reilly & Associates, Inc., Sebastopol, CA.Sudhakar Kumar, Francesca Spezzano, and VS Subrahmanian. 2014. Accurately detecting trolls in slashdotzoo via decluttering. In Proceedings of IEEE/ACM International Conference on Advances in Social NetworksAnalysis and Mining (ASONAM), pages 188–195.Ritesh Kumar, Guggilla Bhanodai, Rajendra Pamula, and Maheshwar Reddy Chennuru. 2018a. Trac-1 shared taskon aggression identiﬁcation: Iit(ism)@coling18. In Proceedings of the First Workshop on Trolling, Aggressionand Cyberbullying (TRAC – 1), Santa Fe, USA.Ritesh Kumar, Aishwarya N. Reganti, Akshit Bhatia, and Tushar Maheshwari. 2018b. Aggression-annotated cor-pus of hindi-english code-mixed data. In Nicoletta Calzolari (Conference chair), Khalid Choukri, ChristopherCieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, HlneMazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of theEleventh International Conference on Language Resources and Evaluation (LREC 2018), Paris, France, may.European Language Resources Association (ELRA).Sreekanth Madisetty and Maunendra Sankar Desarkar. 2018. Aggression detection in social media using deepneural networks. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1),Santa Fe, USA.Promita Maitra and Ritesh Sarkhel. 2018. Emoti-kate: a k-competitive autoencoder for aggression detection insocial media text. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1),Santa Fe, USA.Shervin Malmasi and Marcos Zampieri. 2017. Detecting Hate Speech in Social Media. In Proceedings of theInternational Conference Recent Advances in Natural Language Processing (RANLP), pages 467–472.Shervin Malmasi and Marcos Zampieri. 2018. Challenges in discriminating profanity from hate speech. Journalof Experimental & Theoretical Artiﬁcial Intelligence, 30:1 – 16.Todor Mihaylov, Georgi D Georgiev, AD Ontotext, and Preslav Nakov. 2015. Finding opinion manipulation trollsin news community forums. In Proceedings of the Nineteenth Conference on Computational Natural LanguageLearning, CoNLL, pages 310–314.Sandip Modha, Prasenjit Majumder, and Thomas Mandl. 2018. Filtering aggression from multilingual socialmedia feed. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), SantaFe, USA.Luis G Mojica. 2016. Modeling trolling in social media conversations.Nishant Nikhil, Ramit Pahwa, Mehul Kumar Nirala, and Rohan Khilnani. 2018. Lstms with attention for aggres-sion detection. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1),Santa Fe, USA.\f11Nitin, Ankush Bansal, Siddhartha Mahadev Sharma, Kapil Kumar, Anuj Aggarwal, Sheenu Goyal, Kanika Choud-hary, Kunal Chawla, Kunal Jain, and Manav Bhasinar. 2012. Classiﬁcation of ﬂames in computer mediatedcommunications.Taisei Nitta, Fumito Masui, Michal Ptaszynski, Yasutomo Kimura, Rafal Rzepka, and Kenji Araki. 2013. Detect-ing cyberbullying entries on informal school websites based on category relevance maximization. In Proceed-ings of IJCNLP, pages 579–586.Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. 2016. Abusive Language Detec-tion in Online User Content. In Proceedings of the 25th International Conference on World Wide Web, pages145–153. International World Wide Web Conferences Steering Committee.Ahmed Husseini Orabi, Mahmoud Husseini Orabi, Qianjia Huang, Diana Inkpen, and David Van Bruwaene. 2018.Cyber-aggression detection using cross segment-and-concatenate multi-task learning from text. In Proceedingsof the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Constantin Orasan. 2018. Aggressive Language Identiﬁcation Using Word Embeddings and Sentiment Features.In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Manish Gupta Pinkesh Badjatiya, Shashank Gupta and Vasudeva Varma. 2017. Deep learning for hate speechdetection in tweets. In Proceedings of the 26th International Conference on World Wide Web Companion, pages759 – 760. International World Wide Web Conferences Steering Committee.Kashyap Raiyani, Teresa Gonc¸alves, Paulo Quaresma, and Vitor Beires Nogueira. 2018. Fully connected neuralnetwork with advance preprocessor to identify aggression over facebook and twitter. In Proceedings of the FirstWorkshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Faneva Ramiandrisoa and Josiane Mothe. 2018. Irit at trac 2018. In Proceedings of the First Workshop on Trolling,Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Julian Risch and Ralf Krestel. 2018. Aggression identiﬁcation using deep learning and data augmentation. InProceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1), Santa Fe, USA.Arjun Roy, Prashant Kapil, Kingshuk Basak, and Asif Ekbal. 2018. An ensemble approach for aggression identiﬁ-cation in english and hindi text. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying(TRAC – 1), Santa Fe, USA.Niloofar SaﬁSamghabadi, Deepthi Mave, Sudipta Kar, and Thamar Solorio. 2018. Ritual-uh at trac 2018 sharedtask: Aggression identiﬁcation. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbully-ing (TRAC – 1), Santa Fe, USA.Sasha Sax. 2016. Flame Wars: Automatic Insult Detection. Technical report, Stanford University.Anna Schmidt and Michael Wiegand. 2017. A Survey on Hate Speech Detection Using Natural Language Pro-cessing. In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media.Association for Computational Linguistics, pages 1–10, Valencia, Spain.Antonela Tommasel, Juan Manuel Rodriguez, and Daniela Godoy. 2018. Textual aggression detection throughdeep learning. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC – 1),Santa Fe, USA.Fabio Del Vigna, Andrea Cimino, Felice DellOrletta, Marinella Petrocchi, and Maurizio Tesconi. 2017. Hate me,hate me not: Hate speech detection on facebook. In Proceedings of the First Italian Conference on Cybersecu-rity, pages 86 – 95.Zeerak Waseem and Dirk Hovy. 2016. Hateful symbols or hateful people? predictive features for hate speechdetection on twitter. In Proceedings of NAACL-HLT, pages 88 – 93.Zeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017. Understanding abuse: A typologyof abusive language detection subtasks. In Proceedings of the First Workshop on Abusive Language Online,pages 78–84. Association for Computational Linguistics.",
        "answer": "3 categories: overt aggression, covert aggression, and non-aggression.",
        "space_key_index": 1,
        "metadata": {
            "format": "PDF 1.3",
            "title": "Benchmarking Aggression Identification in Social Media",
            "author": "Ritesh Kumar ; Atul Kr. Ojha ; Shervin Malmasi ; Marcos Zampieri",
            "subject": "W18-44 2018",
            "keywords": "",
            "creator": "TeX",
            "producer": "pdfTeX-1.40.18",
            "creationDate": "D:20180717122342Z",
            "modDate": "D:20180717122342Z",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "Based on Figure 2, which language exhibited the greatest bias in accuracy between stereotypical and non-stereotypical gender roles when using Google Translate?",
        "context": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics1679Evaluating Gender Bias in Machine TranslationGabriel Stanovsky1,2, Noah A. Smith1,2, and Luke Zettlemoyer11Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, USA2Allen Institute for Artiﬁcial Intelligence, Seattle, USA{gabis,nasmith,lsz}@cs.washington.eduAbstractWe present the ﬁrst challenge set and eval-uation protocol for the analysis of genderbias in machine translation (MT). Our ap-proach uses two recent coreference resolutiondatasets composed of English sentences whichcast participants into non-stereotypical genderroles (e.g., “The doctor asked the nurse to helpher in the operation”). We devise an automaticgender bias evaluation method for eight tar-get languages with grammatical gender, basedon morphological analysis (e.g., the use of fe-male inﬂection for the word “doctor”). Ouranalyses show that four popular industrial MTsystems and two recent state-of-the-art aca-demic MT models are signiﬁcantly prone togender-biased translation errors for all testedtarget languages. Our data and code are pub-licly available at https://github.com/gabrielStanovsky/mt_gender.1IntroductionLearned models exhibit social bias when theirtraining data encode stereotypes not relevant forthe task, but the correlations are picked up any-way. Notable examples include gender biases invisual SRL (cooking is stereotypically done bywomen, construction workers are stereotypicallymen; Zhao et al., 2017), lexical semantics (“manis to computer programmer as woman is to home-maker”; Bolukbasi et al., 2016), and natural lan-guage inference (associating women with gossip-ing and men with guitars; Rudinger et al., 2017).In this work, we conduct the ﬁrst large-scalemultilingual evaluation of gender-bias in machinetranslation (MT), following recent small-scalequalitative studies which observed that online MTservices, such as Google Translate or MicrosoftTranslator, also exhibit biases, e.g., translatingnurses as females and programmers as males, re-gardless of context (Alvarez-Melis and Jaakkola,The doctor asked the nurse to help her in the procedureEl doctor le pidio a la enfermera que le ayudara con el procedimientoFigure 1: An example of gender bias in machine trans-lation from English (top) to Spanish (bottom).Inthe English source sentence, the nurse’s gender is un-known, while the coreference link with “her” identi-ﬁes the “doctor” as a female. On the other hand, theSpanish target sentence uses morphological featuresfor gender: “el doctor” (male), versus “la enfermer-a” (female). Aligning between source and target sen-tences reveals that a stereotypical assignment of genderroles changed the meaning of the translated sentence bychanging the doctor’s gender.2017; Font and Costa-Juss`a, 2019). Google Trans-late recently tried to mitigate these biases by al-lowing users to sometimes choose between gen-dered translations (Kuczmarski, 2018).As shown in Figure 1, we use data introducedby two recent coreference gender-bias studies: theWinogender (Rudinger et al., 2018), and the Wino-Bias (Zhao et al., 2018) datasets. Following theWinograd schema (Levesque, 2011), each instancein these datasets is an English sentence which de-scribes a scenario with human entities, who areidentiﬁed by their role (e.g., “the doctor” and “thenurse” in Figure 1), and a pronoun (“her” in theexample), which needs to be correctly resolvedto one of the entities (“the doctor” in this case).Rudinger et al. (2018) and Zhao et al. (2018) foundthat while human agreement on the task was high(roughly 95%), coreference resolution models of-ten ignore context and make socially biased pre-dictions, e.g., associating the feminine pronoun“her” with the stereotypically female “nurse.”We observe that for many target languages, afaithful translation requires a similar form of (at\f1680least implicit) gender identiﬁcation. In addition,in the many languages which associate betweenbiological and grammatical gender (e.g., most Ro-mance, Germanic, Slavic, and Semitic languages;Craig, 1986; Mucchi-Faina, 2005; Corbett, 2007),the gender of an animate object can be identiﬁedvia morphological markers. For instance, whentranslating our running example in Figure 1 toSpanish, a valid translation may be: “La doc-tora le pidio a la enfermera que le ayudara conel procedimiento,” which indicates that the doctoris a woman, by using a feminine sufﬁx inﬂection(“doctora”) and the feminine deﬁnite gendered ar-ticle (“la”). However, a biased translation systemmay ignore the given context and stereotypicallytranslate the doctor as male, as shown at the bot-tom of the ﬁgure.Following these observations, we design a chal-lenge set approach for evaluating gender bias inMT using a concatenation of Winogender andWinoBias.We devise an automatic translationevaluation method for eight diverse target lan-guages, without requiring additional gold trans-lations, relying instead on automatic measuresfor alignment and morphological analysis (Sec-tion 2). We ﬁnd that four widely used commercialMT systems and two recent state-of-the-art aca-demic models are signiﬁcantly gender-biased onall tested languages (Section 3). Our method andbenchmarks are publicly available, and are easilyextensible with more languages and MT models.2Challenge Set for Gender Bias in MTWe compose a challenge set for gender bias in MT(which we dub “WinoMT”) by concatenating theWinogender and WinoBias coreference test sets.Overall, WinoMT contains 3,888 instances, and isequally balanced between male and female gen-ders, as well as between stereotypical and non-stereotypical gender-role assignments (e.g., a fe-male doctor versus a female nurse). Additionaldataset statistics are presented in Table 1.We use WinoMT to estimate the gender-bias ofan MT model, M, in target-language L by per-forming following steps (exempliﬁed in Figure 1):(1) Translate all of the sentences in WinoMT intoL using M, thus forming a bilingual corpus of En-glish and the target language L.(2) Align between the source and target transla-tions, using fast align (Dyer et al., 2013), trainedon the automatic translations from from step (1).WinogenderWinoBiasWinoMTMale24015821826Female24015861822Neutral2400240Total72031683888Table 1:The coreference test sets and resultingWinoMT corpus statistics (in number of instances).We then map the English entity annotated in thecoreference datasets to its translation (e.g., alignbetween “the doctor” and “el doctor” in Figure 1).(3) Finally, we extract the target-side entity’sgender using simple heuristics over language-speciﬁc morphological analysis, which we per-form using off-the-shelf tools for each target lan-guage, as discussed in the following section.This process extracts the translated genders, ac-cording to M, for all of the entities in WinoMT,which we can then evaluate against the gold anno-tations provided by the original English dataset.This process can introduce noise into our eval-uation in steps (2) and (3), via wrong alignmentsor erroneous morphological analysis. In Section 3,we will present a human evaluation showing theseerrors are infrequent.3EvaluationIn this section, we brieﬂy describe the MT systemsand the target languages we use, our main results,and their human validation.3.1Experimental SetupMT systemsWe test six widely used MT mod-els, representing the state of the art in bothcommercial and academic research: (1) GoogleTranslate,1 (2) Microsoft Translator,2 (3) AmazonTranslate,3 (4) SYSTRAN,4 (5) the model of Ottet al. (2018), which recently achieved the best per-formance on English-to-French translation on theWMT’14 test set, and (6) the model of Edunovet al. (2018), the WMT’18 winner on English-to-German translation. We query the online API forthe ﬁrst four commercial MT systems, while forthe latter two academic models we use the pre-trained models provided by the Fairseq toolkit.51https://translate.google.com2https://www.bing.com/translator3https://aws.amazon.com/translate4http://www.systransoft.com5https://github.com/pytorch/fairseq\f1681Google TranslateMicrosoft TranslatorAmazon Translate∗SYSTRANAcc∆G∆SAcc∆G∆SAcc∆G∆SAcc∆G∆SES53.123.421.347.336.823.259.415.422.345.646.315.0FR63.66.426.744.736.429.755.217.724.945.044.09.4IT39.632.921.539.839.817.042.427.818.538.947.59.4RU37.736.811.436.842.18.539.734.79.237.344.19.3UK38.443.610.841.346.911.8–––28.922.412.9HE53.77.937.848.114.932.950.510.347.346.620.524.5AR48.543.716.147.348.313.449.838.519.047.049.45.3DE59.412.512.574.10.030.262.412.016.748.634.510.3Table 2: Performance of commercial MT systems on the WinoMT corpus on all tested languages, categorized bytheir family: Spanish, French, Italian, Russian, Ukrainian, Hebrew, Arabic, and German. Acc indicates overallgender accuracy (% of instances the translation had the correct gender), ∆G denotes the difference in performance(F1 score) between masculine and feminine scores, and ∆S is the difference in performance (F1 score) betweenpro-stereotypical and anti-stereotypical gender role assignments (higher numbers in the two latter metrics indicatestronger biases). Numbers in bold indicate best accuracy for the language across MT systems (row), and underlinednumbers indicate best accuracy for the MT system across languages (column). ∗Amazon Translate does not havea trained model for English to Ukrainian.Acc∆G∆SFR (Ott et al., 2018)49.42.616.1DE (Edunov et al., 2018)52.57.38.4Table 3: Performance of recent state-of-the-art aca-demic translation models from English to French andGerman. Metrics are the same as those in Table 2.Target languages and morphological analysisWe selected a set of eight languages with gram-matical gender which exhibit a wide range ofother linguistic properties (e.g., in terms of al-phabet, word order, or grammar), while still al-lowing for highly accurate automatic morpholog-ical analysis. These languages belong to four dif-ferent families: (1) Romance languages: Span-ish, French, and Italian, all of which have gen-dered noun-determiner agreement and spaCy mor-phological analysis support (Honnibal and Mon-tani, 2017). (2) Slavic languages (Cyrillic alpha-bet): Russian and Ukrainian, for which we usethe morphological analyzer developed by Korobov(2015). (3) Semitic languages: Hebrew and Ara-bic, each with a unique alphabet.For Hebrew,we use the analyzer developed by Adler and El-hadad (2006), while gender inﬂection in Arabiccan be easily identiﬁed via the ta marbuta charac-ter, which uniquely indicates feminine inﬂection.(4) Germanic languages: German, for which weuse the morphological analyzer developed by Al-tinok (2018).3.2ResultsOur main ﬁndings are presented in Tables 2 and 3.For each tested MT system and target language wecompute three metrics with respect to their abil-ity to convey the correct gender in the target lan-guage. Ultimately, our analyses indicate that alltested MT systems are indeed gender biased.First, the overall system Accuracy is calculatedby the percentage of instances in which the trans-lation preserved the gender of the entity fromthe original English sentence. We ﬁnd that mosttested systems across eight tested languages per-form quite poorly on this metric. The best per-forming model on each language often does notdo much better than a random guess for the correctinﬂection. An exception to this rule is the transla-tion accuracies on German, where three out of foursystems acheive their best performance. This maybe explained by German’s similarity to the Englishsource language (Hawkins, 2015).In Table 2, ∆G denotes the difference in per-formance (F1 score) between male and femaletranslations. Interestingly, all systems, except Mi-crosoft Translator on German, perform signiﬁ-cantly better on male roles, which may stem fromthese being more frequent in the training set.Perhaps most tellingly, ∆S measures the differ-\f1682ESFRITRUUKHEARDE2040608010067805244467660694654303335384457Accuracy (%)StereotypicalNon-StereotypicalFigure 2: Google Translate’s performance on gender translation on our tested languages. The performance on thestereotypical portion of WinoMT is consistently better than that on the non-stereotypical portion. The other MTsystems we tested display similar trends.Original+Adj∆ES53.163.5+10.4RU37.748.9+11.2UK38.442.9+4.5Table 4: Performance of Google Translate on Spanish,Russian, and Ukranian gender prediction accuracy (%correct) on the original WinoMT corpus, versus a mod-iﬁed version of the dataset where we add sterotypicalgender adjectives (see Section 3.3).ence in performance (F1 score) between stereo-typical and non-stereotypical gender role assign-ments, as deﬁned by Zhao et al. (2018) whouse statistics provided by the US Department ofLabor.6This metric shows that all tested sys-tems have a signiﬁcant and consistently better per-formance when presented with pro-stereotypicalassignments (e.g., a female nurse), while theirperformance deteriorates when translating anti-stereotypical roles (e.g., a male receptionist).For instance, Figure 2 depicts Google Trans-late absolute accuracies on stereotypical and non-stereotypical gender roles across all tested lan-guages. Other tested systems show similar trends.3.3Fighting Bias with BiasFinally, we tested whether we can affect thetranslations by automatically creating a versionof WinoMT with the adjectives “handsome” and“pretty” prepended to male and female entities, re-spectively. For example, the sentence in Figure 1will be converted to: “The pretty doctor asked thenurse to help her in the operation”. We are inter-ested in evaluating whether this “corrects” the pro-fession bias by mixing signals, e.g., while “doc-6https://www.bls.gov/cps/cpsaat11.htmtor” biases towards a male translation, “pretty”tugs the translation towards a female inﬂection.Our results show that this improved performancein some languages, signiﬁcantly reducing bias inSpanish, Russian, and Ukrainian (see Table 4).Admittedly, this is impractical as a general debi-asing scheme, since it assumes oracle coreferenceresolution, yet it attests to the relation betweencoreference resolution and MT, and serves as a fur-ther indication of gender bias in MT.3.4Human ValidationWe estimate the accuracy of our gender bias evalu-ation method by randomly sampling 100 instancesof all translation systems and target languages, an-notating each sample by two target-language na-tive speakers (resulting in 9,600 human annota-tions). Each instance conformed to a format sim-ilar to that used by our automatic gender detec-tion algorithm: human annotators were asked tomark the gender of an entity within a given target-language sentence. (e.g., see “el doctor” as high-lighted in the Spanish sentence in Figure 1). Byannotating at the sentence-level, we can accountfor both types of possible errors, i.e., alignmentand gender extraction.We compare the sentence-level human anno-tations to the output of our automatic method,and ﬁnd that the levels of agreement for all lan-guages and systems were above 85%, with anaverage agreement on 87% of the annotations.In comparison, human inter-annotator agreementwas 90%, due to noise introduced by several inco-herent translations.Our errors occur when language-speciﬁc id-iosyncrasies introduce ambiguity to the morpho-logical analysis. For example, gender for certainwords in Hebrew cannot be distinguished without\f1683Source[Target lang.] Predicted translationPhenomenonThe janitor does not like the bakerbecause she always messes up thekitchen.[ES] Al conserje no le gusta el panaderoporque ella siempre desordena la cocina.Biased translation, giving “baker” amale inﬂection, with a mismatched pro-noun reference.The janitor does not like the prettybaker because she always messes upthe kitchen.[ES] Al conserje no le gusta la panaderabonita porque ella siempre desordena lacocina.Adding a stereotypically female adjec-tive “ﬁxes” the translation.The counselor asked the guard a fewquestions and praised her for the goodwork.[FR] Le conseiller a pos´e quelques ques-tions `a la garde et l’a lou´ee pour le bontravail.French uses “garde” for both male andfemale guards, allowing for a more di-rect translation from English.Table 5: Examples of Google Translate’s output for different sentences in the WinoMT corpus. Words in blue,red, and orange indicate male, female and neutral entities, respectively.diacritics (e.g., the male and female versions of theword “baker” are spelled identically), and the con-tracted determiner in French and Italian (l’) is usedfor both masculine and feminine nouns. In ad-dition, some languages have only male or femaleinﬂections for professions which were stereotypi-cally associated with one of the genders, for exam-ple “sastre” (tailor) in Spanish or “soldat” (soldier)in French, which do not have female inﬂections.See Table 5 for detailed examples.4DiscussionRelated workThis work is most related to sev-eral recent efforts which evaluate MT throughthe use of challenge sets.Similarly to our useWinoMT, these works evaluate MT systems (ei-ther manually or automatically) on test sets whichare specially created to exhibit certain linguis-tic phenomena, thus going beyond the traditionalBLEU metric (Papineni et al., 2002). These in-clude challenge sets for language-speciﬁc idiosyn-crasies (Isabelle et al., 2017), discourse phenom-ena (Bawden et al., 2018), pronoun translation(M¨uller et al., 2018; Webster et al., 2018), orcoreference and multiword expressions (Burchardtet al., 2017).Limitations and future workWhile our workpresents the ﬁrst large-scale evaluation of genderbias in MT, it still suffers from certain limitationswhich could be addressed in follow up work. First,like some of the challenge sets discussed above,WinoMT is composed of synthetic English source-side examples. On the one hand, this allows fora controlled experiment environment, while, onthe other hand, this might introduce some artiﬁ-cial biases in our data and evaluation.Ideally,WinoMT could be augmented with natural “in thewild” instances, with many source languages, allannotated with ground truth entity gender. Sec-ond, similar to any medium size test set, it is clearthat WinoMT serves only as a proxy estimation forthe phenomenon of gender bias, and would prob-ably be easy to overﬁt. A larger annotated cor-pus can perhaps provide a better signal for train-ing. Finally, even though in Section 3.3 we showa very rudimentary debiasing scheme which relieson oracle coreference system, it is clear that thisis not applicable in a real-world scenario. Whilerecent research has shown that getting rid of suchbiases may prove to be very challenging (Elazarand Goldberg, 2018; Gonen and Goldberg, 2019),we hope that this work will serve as a ﬁrst step fordeveloping more gender-balanced MT models.5ConclusionsWe presented the ﬁrst large-scale multilingualquantitative evidence for gender bias in MT,showing that on eight diverse target languages,all four tested popular commercial systems andtwo recent state-of-the-art academic MT mod-els are signiﬁcantly prone to translate basedon gender stereotypes rather than more mean-ingful context.Our data and code are pub-liclyavailableathttps://github.com/gabrielStanovsky/mt_gender.AcknowledgmentsWe would like to thank Mark Yatskar, Iz Beltagy,Tim Dettmers, Ronan Le Bras, Kyle Richardson,Ariel and Claudia Stanovsky, and Paola Virga formany insightful discussions about the role genderplays in the languages evaluated in this work, aswell as the reviewers for their helpful comments.\f1684ReferencesMeni Adler and Michael Elhadad. 2006. An unsuper-vised morpheme-based HMM for Hebrew morpho-logical disambiguation. In ACL.Duygu Altinok. 2018. DEMorphy, German languagemorphological analyzer. CoRR, abs/1803.00902.David Alvarez-Melis and Tommi S. Jaakkola. 2017.A causal framework for explaining the predictionsof black-box sequence-to-sequence models.InEMNLP.Rachel Bawden, Rico Sennrich, Alexandra Birch, andBarry Haddow. 2018.Evaluating discourse phe-nomena in neural machine translation. In NAACL-HLT.Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou,Venkatesh Saligrama, and Adam Tauman Kalai.2016. Man is to computer programmer as womanis to homemaker? debiasing word embeddings. InNIPS.Aljoscha Burchardt,Vivien Macketanz,Jon De-hdari, Georg Heigold, Jan-Thorsten Peter, andPhilip Williams. 2017.A linguistic evaluation ofrule-based, phrase-based, and neural mt engines.The Prague Bulletin of Mathematical Linguistics,108(1):159–170.Greville G Corbett. 2007. Gender and noun classes.Colette G Craig. 1986. Noun Classes and Categoriza-tion: Proceedings of a Symposium on Categoriza-tion and Noun Classiﬁcation, volume 7. John Ben-jamins Publishing Company.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013. A simple, fast, and effective reparameteriza-tion of ibm model 2. In HLT-NAACL.Sergey Edunov, Myle Ott, Michael Auli, and DavidGrangier. 2018. Understanding back-translation atscale. arXiv preprint arXiv:1808.09381.Yanai Elazar and Yoav Goldberg. 2018. Adversarialremoval of demographic attributes from text data. InEMNLP.Joel Escud´e Font and Marta R. Costa-Juss`a. 2019.Equalizing gender biases in neural machine trans-lation with word embeddings techniques.CoRR,abs/1901.03116.Hila Gonen and Yoav Goldberg. 2019. Lipstick on apig: Debiasing methods cover up systematic genderbiases in word embeddings but do not remove them.HLT-NAACL.John A Hawkins. 2015. A Comparative Typology ofEnglish and German: Unifying the Contrasts. Rout-ledge.Matthew Honnibal and Ines Montani. 2017. spaCy 2:Natural language understanding with Bloom embed-dings, convolutional neural networks and incremen-tal parsing. To appear.Pierre Isabelle, Colin Cherry, and George F. Foster.2017. A challenge set approach to evaluating ma-chine translation. In EMNLP.Mikhail Korobov. 2015. Morphological analyzer andgenerator for Russian and Ukrainian languages.In Mikhail Yu. Khachay, Natalia Konstantinova,Alexander Panchenko, Dmitry I. Ignatov, and Va-leri G. Labunets, editors, Analysis of Images, SocialNetworks and Texts, volume 542 of Communicationsin Computer and Information Science, pages 320–332. Springer International Publishing.James Kuczmarski. 2018.Reducing gender bias ingoogle translate.Hector J. Levesque. 2011. The Winograd schema chal-lenge. In AAAI Spring Symposium: Logical Formal-izations of Commonsense Reasoning.Angelica Mucchi-Faina. 2005. Visible or inﬂuential?language reforms and gender (in) equality. SocialScience Information, 44(1):189–215.Mathias M¨uller, Annette Rios, Elena Voita, and RicoSennrich. 2018. A large-scale test set for the evalu-ation of context-aware pronoun translation in neuralmachine translation. CoRR, abs/1810.02268.Myle Ott,Sergey Edunov,David Grangier,andMichael Auli. 2018. Scaling neural machine trans-lation. arXiv preprint arXiv:1806.00187.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic eval-uation of machine translation. In ACL.Rachel Rudinger, Chandler May, and Benjamin VanDurme. 2017.Social bias in elicited natural lan-guage inferences. In EthNLP@EACL.Rachel Rudinger, Jason Naradowsky, Brian Leonard,and Benjamin Van Durme. 2018.Gender bias incoreference resolution. In NAACL-HLT.Kellie Webster, Marta Recasens, Vera Axelrod, and Ja-son Baldridge. 2018.Mind the gap: A balancedcorpus of gendered ambiguous pronouns. Transac-tions of the Association for Computational Linguis-tics, 6:605–617.Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-donez, and Kai-Wei Chang. 2017.Men also likeshopping: Reducing gender bias ampliﬁcation usingcorpus-level constraints. In EMNLP.Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-donez, and Kai-Wei Chang. 2018. Gender bias incoreference resolution:Evaluation and debiasingmethods. In NAACL-HLT.",
        "answer": "HE (Hebrew) exhibited the greatest bias in accuracy between stereotypical and non-stereotypical gender roles when using Google Translate, with an accuracy of 76% for stereotypical roles and 38% for non-stereotypical roles, resulting in a difference of 38 percentage points.",
        "space_key_index": 2,
        "metadata": {
            "format": "PDF 1.3",
            "title": "Evaluating Gender Bias in Machine Translation",
            "author": "Gabriel Stanovsky ; Noah A. Smith ; Luke Zettlemoyer",
            "subject": "P19-1 2019",
            "keywords": "",
            "creator": "LaTeX with hyperref package",
            "producer": "pdfTeX-1.40.17",
            "creationDate": "D:20190602234849Z",
            "modDate": "D:20190602234849Z",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "Which training data size from the Common Crawl resulted in the highest average accuracy across all GLUE tasks?",
        "context": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processingand the 9th International Joint Conference on Natural Language Processing, pages 5360–5369,Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics5360Cloze-driven Pretraining of Self-attention NetworksAlexei Baevski, Sergey Edunov∗, Yinhan Liu∗, Luke Zettlemoyer, Michael AuliFacebook AI ResearchMenlo Park, CA and Seattle, WAAbstractWe present a new approach for pretraining abi-directional transformer model that providessigniﬁcant performance gains across a vari-ety of language understanding problems. Ourmodel solves a cloze-style word reconstructiontask, where each word is ablated and must bepredicted given the rest of the text. Experi-ments demonstrate large performance gains onGLUE and new state of the art results on NERas well as constituency parsing benchmarks,consistent with BERT. We also present a de-tailed analysis of a number of factors that con-tribute to effective pretraining, including datadomain and size, model capacity, and varia-tions on the cloze objective.1IntroductionLanguage model pretraining has recently beenshown to provide signiﬁcant performance gainsfor a range of challenging language understand-ing problems (Dai and Le, 2015; Peters et al.,2018; Radford et al., 2018).However, existingwork has either used unidirectional (left-to-right)language models (LMs) (Radford et al., 2018) orbi-directional (both left-to-right and right-to-left)LMs (BiLMs) where each direction is trained withan independent loss function (Peters et al., 2018).In this paper, we show that even larger perfor-mance gains are possible by jointly pretrainingboth directions of a large language-model-inspiredself-attention cloze model.Our bi-directional transformer architecture pre-dicts every token in the training data (Figure 1).We achieve this by introducing a cloze-style train-ing objective where the model must predict thecenter word given left-to-right and right-to-leftcontext representations.Our model separatelycomputes both forward and backward states with∗Equal contribution.<s>abc<s>Block1BlockNBlock1BlockNBlock1BlockNBlock1BlockNcombbFigure 1: Illustration of the model. Blocki is a standardtransformer decoder block. Green blocks operate left toright by masking future time-steps and blue blocks op-erate right to left. At the top, states are combined witha standard multi-head self-attention module whose out-put is fed to a classiﬁer that predicts the center token.a masked self-attention architecture, that closelyresembles a language model. At the top of the net-work, the forward and backward states are com-bined to jointly predict the center word. This ap-proach allows us to consider both contexts whenpredicting words and to incur loss for every wordin the training set, if the model does not assign ithigh likelihood.Experiments on the GLUE (Wang et al., 2018)benchmark show strong gains over the state of theart for each task, including a 9.1 point gain on RTEover Radford et al. (2018). These improvementsare consistent with, if slightly behind, BERT (De-vlin et al., 2018), which we will discuss in moredetail in the next section. We also show that itis possible to stack task-speciﬁc architectures forNER and constituency parsing on top of our pre-trained representations, and achieve new state-of-the-art performance levels for both tasks. We alsopresent extensive experimental analysis to better\f5361understand these results, showing that (1) hav-ing multiple sentences in each training exampleis crucial for many tasks; (2) pre-training contin-ues to improve performance with up to 18B tokensand would likely continue to improve with moredata; and ﬁnally (3) our novel cloze-driven train-ing regime is more effective than predicting leftand right tokens separately.2Related workThere has been much recent work on learningsentence-speciﬁc representations for language un-derstanding tasks. McCann et al. (2017) learn con-textualized word representations from a sequenceto sequence translation task and uses the represen-tations from the encoder network to improve a va-riety of language understanding tasks. Subsequentwork focused on language modeling pretrainingwhich has been shown to be more effective andwhich does not require bilingual data (Zhang andBowman, 2018).Our work was inspired by ELMo (Peters et al.,2018) and the generative pretraining (GPT) ap-proach of Radford et al. (2018). ELMo introduceslanguage models to pretrain word representationsfor downstream tasks including a novel mecha-nism to learn a combination of different layersin the language model that is most beneﬁcial tothe current task.GPT relies on a left to rightlanguage model and an added projection layerfor each downstream task without a task-speciﬁcmodel. Our approach mostly follows GPT, thoughwe show that our model also works well with anELMo module on NER and constituency parsing.The BERT model (Devlin et al., 2018) is atransformer encoder model that captures left andright context. There is signiﬁcant overlap betweentheir work and ours but there are also signiﬁcantdifferences: our model is a bi-directional trans-former language model that predicts every singletoken in a sequence.Our model has two uni-directional components encoding either the left orright context and both are combined to predict cen-ter words. BERT is also a transformer encoder thathas access to the entire input but this choice re-quires a special training regime. In particular, theymulti-task between predicting a subset of maskedinput tokens, similar to a denoising autoencoder,and a next sentence prediction task. In compar-ison, we optimize a single loss function that re-quires the model to predict each token of an in-put sentence given all surrounding tokens. We useall tokens as training targets and therefore extractlearning signal from every single token in the sen-tence and not just a subset. Melamud et al. (2016)follow a similar approach to ours by predictingthe center word but their architecture is based onLSTMs and we include the center word when weactually ﬁne-tune on downstream tasks.BERT tailors pretraining to capture dependen-cies between sentences via a next sentence predic-tion task as well as by constructing training exam-ples of sentence-pairs with input markers that dis-tinguish between tokens of the two sentences. Ourmodel is trained similarly to a classical languagemodel since we do not adapt the training exam-ples to resemble the end task data and we do notsolve a denoising task during training.Finally, BERT as well as Radford et al. (2018)consider only a single data source to pretraintheir models, either BooksCorpus (Radford et al.,2018), or BooksCorpus and additional Wikipediadata (Devlin et al., 2018), whereas our study ab-lates the effect of various amounts of training dataas well as different data sources.3Two tower modelOur cloze model represents a probability distribu-tion p(ti|t1, . . . , ti−1, ti+1, . . . , tn) for a sentencewith n tokens t1, . . . , tn.There are two self-attentional towers each consisting of N stackedblocks: the forward tower operates left-to-rightand the backward tower operates in the oppositedirection.To predict a token, we combine therepresentations of the two towers, as described inmore detail below, taking care that neither repre-sentation contains information about the currenttarget token.The forward tower computes the representationF li for token i at layer l based on the forward rep-resentations of the previous layer F l−1≤ivia self-attention; the backward tower computes represen-tation Bli based on information from the oppositedirection Bl−1≥i . When examples of uneven lengthare batched, one of the towers may not have anycontext at the beginning. We deal with this issueby adding an extra zero state over which the self-attention mechanism can attend.We pretrain on individual examples as they oc-cur in the training corpora (§5.1). For News Crawlthis is individual sentences while on Wikipedia,Bookcorpus, and Common Crawl examples are\f5362paragraph length. Sentences are prepended andappended with sample boundary markers < s >.3.1Block structureThe structure of the blocks follows most of thearchitectural choices described in Vaswani et al.(2017). Each block consists of two sub-blocks:the ﬁrst is a multi-head self-attention module withH = 16 heads for which we mask out any sub-sequent time-steps, depending on if we are deal-ing with the forward or backward tower. The sec-ond sub-block is a feed-forward module (FFN)of the form ReLU(W1X + b1)W2 + b2 whereW1 ∈Re×f, W1 ∈Rf×e. Different to Vaswaniet al. (2017) we apply layer normalization beforethe self-attention and FFN blocks instead of af-ter, as we ﬁnd it leads to more effective training.Sub-blocks are surrounded by a residual connec-tion (He et al., 2015).Position is encoded viaﬁxed sinusoidal position embeddings and we usea character CNN encoding of the input tokens forword-based models (Kim et al., 2016). Input em-beddings are shared between the two towers.3.2Combination of representationsThe forward and backward representations com-puted by the two towers are combined to pre-dict the ablated word. To combine them we usea self-attention module which is followed by anFFN block (§3.1). The output of the FFN blockf is projected by W into V classes represent-ing the types in the vocabulary: WT f to whicha softmax is applied. When the model predictstoken i, the input to the attention module areforward states F L1 . . . F Li−1 and backward statesBLi+1 . . . B:n where n is the length of the sequenceand L is the number of layers. We implement thisby masking BL≤i and F L≥i.The attention queryfor token i is a combination of F Li−1 and BLi+1.For the base model we sum the two representa-tions and for the larger models they are concate-nated. Keys and values are based on the forwardand backward states fed to the attention module.In summary, this module has access to informationabout the entire input surrounding the current tar-get token. During training, we predict every tokenin this way. The output of this module is fed to anoutput classiﬁer which predicts the center token.We use an adaptive softmax for the output classi-ﬁer (Grave et al., 2017) for the word based modelsand regular softmax for the BPE based models.abccombcombcombEmbedding of cEmbedding of bEmbedding of aFigure 2: Illustration of ﬁne-tuning for a downstreamtask. For classiﬁcation problems, output of the ﬁrstand last token is fed to a task-speciﬁc classiﬁer. Mask-ing for the ﬁnal combination layer (comb) is removedwhich results in representations based on all forwardand backward states (cf. Figure 1). The red dot-dashedarrows show connections that are masked during train-ing, but unmasked for ﬁne-tuning.While all states that contain information aboutthe current target word are masked in the ﬁnal self-attention block during training, we found it bene-ﬁcial to disable this masking when ﬁne tuning thepretrained model for downstream tasks. This is es-pecially true for tasks that label each token, suchas NER, as this allows the model to access the fullcontext including the token itself.4Fine-tuningWe use the following approach to ﬁne-tune thepretrained two tower model to speciﬁc down-stream tasks (Figure 2).Classiﬁcation and regression tasks.For sin-gle sentence classiﬁcation tasks, we consider thelanguage model outputs for the boundary tokens< s > which we add before the start and endof each sentence.The language model outputsare the representations f just before the ﬁnal soft-max layer (§3.2). The outputs are of dimensiond = 1024 and we concatenate them to project tothe number of classes C in the downstream taskwith W1 ∈RC×2d (Radford et al., 2018); we add abias term b ∈RC and initialize all weights as wellas the bias to zero. The output of the projectionis softmax-normalized and the model is optimizedwith cross-entropy for classiﬁcation tasks.Re-\f5363gression tasks such as the Semantic Textual Sim-ilarity benchmark (STS-B; Cer et al., 2017) useC = 1 and are trained with mean squared error.For tasks involving sentence-pairs, we concatenatethem and add a new separator token < sep > be-tween them. We add the output of this token to theﬁnal projection W2 ∈RC×3d.Structured prediction tasks.For named entityrecognition and parsing we use task-speciﬁc archi-tectures which we ﬁne-tune together with the lan-guage model but with different learning rate. Thearchitectures are detailed in the respective resultssections.The input to the architectures are theoutput representations of the pretrained languagemodel.No Masking.For ﬁne-tuning, we found it bene-ﬁcial to remove masking of the current token inthe ﬁnal layer that pools the output of the twotowers. This is different than in the actual pre-training. It is important to have access to informa-tion about the token to be classiﬁed for token levelclassiﬁcation tasks such as NER but we also foundthis to perform better for sentence classiﬁcationtasks. In practice, we completely disable maskingin the combination layer so that it operates overall forward and backward states. However, dis-abling masking below the combination layer doesnot perform well.Optimization.During ﬁne-tuning we use largerlearning rates for the new parameters, that is W1,W2, b or the task-speciﬁc architecture, comparedto the pretrained model. For GLUE tasks, we doso by simply scaling the output of the languagemodel before the W1 and W2 projections by afactor of 16. For structured prediction tasks, weexplicitly use different learning rates for the pre-trained model and the task-speciﬁc parameters.We ﬁne tune with the Adam optimizer (Kingmaand Ba, 2015).For GLUE tasks, we disabledropout in the language model and add 0.1 dropoutbetween language model output and the ﬁnal out-put projection; for structured prediction tasks, weuse 0.3 at all levels (within the pretrained model,within the task-speciﬁc architecture, and on theweights connecting them). In all settings, we use abatch size of 16 examples. We use a cosine sched-ule to linearly warm up the learning rate from 1e-07 to the target value over the ﬁrst 10% of train-ing steps, and then anneal the learning rate to 1e-06, following the cosine curve for the remainingsteps. For GLUE tasks, we tuned the learning ratefor each task and chose the best value over threesettings: 1e-04, 5e-05 and 3e-05. For structuredprediction tasks, we tuned on the pairs of learningrate, see the results section for details. For GLUEtasks, we train three seeds for each learning ratevalue for three epochs and choose the model af-ter each epoch that performs best on the validationset. For structured prediction tasks, we train for upto 25 epochs and stop if the validation loss doesnot improve over the previous epoch.5Experimental setup5.1Datasets for pretrainingWe train the two tower model on several datasets.Common Crawl.We consider various subsetsof Common Crawl which is web data. We fol-low the same pre-processing as Grave et al. (2018)which is based on the May 2017 Common Crawldump.This setup add 20 copies of EnglishWikipedia resulting in about 14% of the ﬁnaldataset to be Wikipedia. We subsample up to 18Btokens. All experiments use Common Crawl sub-sampled to 9B tokens, except §6.4.News Crawl.We use up to 4.5B words of En-glish news web data distributed as part of WMT2018 (Bojar et al., 2018).BooksCorpus + Wikipedia.This is similar tothe training data used by BERT which comprisesthe BooksCorpus (Zhu et al., 2015) of about 800Mwords plus English Wikipedia data of 2.5B words.5.2Pretraining hyper-parametersWe adapt the transformer implementation avail-able in the fairseq toolkit to our two tower archi-tecture (Ott et al., 2019). For hyper-parameter andoptimization choices we mostly follow Baevskiand Auli (2018). Our experiments consider threemodel sizes shown in Table 1: There are two CNNinput models in a base and large conﬁguration aswell as a Byte-Pair-Encoding based model (BPE;Sennrich et al., 2016). The CNN models have un-constrained input vocabulary, and an output vo-cabulary limited to 1M most common types forthe large model, and 700K most common typesfor the base model. CNN models use an adap-tive softmax in the output: the head band containsthe 60K most frequent types with dimensionality\f5364ModelParametersUpdatesBlocksFFNDimAttn Heads(ﬁnal layer)Query formation(ﬁnal layer)Train time(days)CNN Base177M600K6409612Sum6CNN Large330M1M12409632Concat10BPE Large370M1M12409632Concat4.5Table 1: Hyper-parameters for our models. Parameter count excludes the (adaptive) softmax layer. Train time asmeasured on 128 Volta GPUs for the CNN models and 64 Volta GPUs for the BPE model.1024, followed by a 160K band with dimension-ality 256. The remaining types have dimensional-ity 64; there are 480K types for the small modeland 780K for the large model. The BPE modeluses a vocabulary of 55K types and we share inputand output embeddings in a ﬂat softmax with di-mension 1024 (Inan et al., 2016; Press and Wolf,2017). The BPE vocabulary was constructed byapplying 30K merge operations over the trainingdata, then applying the BPE code to the trainingdata and retaining all types occurring at least threetimes.Every setup uses model dimensionaltiy d =1024 with H = 16 attention heads for all but theﬁnal attention layer. Model based on character in-puts use character embedding size 128 and we ap-ply six ﬁlters of size 1x128, 2x256, 3x384, 4x512,5x512, 6x512 followed by a single highway layer.The models are trained with model and attentiondropout rate of 0.1 and ReLU dropout rate of 0.05.Different to Vaswani et al. (2017) we use Nes-terov’s accelerated gradient method (Sutskeveret al., 2013) with a momentum of 0.99 and werenormalize gradients if their norm exceeds 0.1(Pascanu et al., 2013). The learning rate is lin-early warmed up from 10−7 to 1 for 16K steps andthen annealed using a cosine learning rate sched-ule with a single phase to 0.0001 (Loshchilov andHutter, 2016).We run experiments on DGX-1 machines with8 NVIDIA V100 GPUs and machines are inter-connected by Inﬁniband. We also use the NCCL2library and the torch.distributed package for inter-GPU communication. We train models with 16-bit ﬂoating point precision, following Ott et al.(2018). The BPE model trains much faster thanthe character CNN models (Table 1).6Results6.1GLUEFirst, we conduct experiments on the generallanguage understanding evaluation benchmark(GLUE; Wang et al., 2018) and present a shortoverview of the tasks. More information can befound in Wang et al. (2018). There are two single-sentence classiﬁcation tasks: First, the Corpus ofLinguistic Acceptability (CoLA; Warstadt et al.,2018) is a binary task to judge sentence grammat-icality; evaluation is in terms of the Matthews cor-relation coefﬁcient (mcc). Second, the StanfordSentiment Treebank (SST-2; Socher et al., 2013)requires to judge if movie reviews have positive ornegative sentiment; evaluation is in terms of accu-racy (acc).There are three tasks assessing sentence sim-ilarity: The Microsoft Research Paragraph Cor-pus (MRPC; Dolan and Brockett, 2015) and theQuora Question Pairs benchmark (QQP); we eval-uate in terms of F1. The Semantic Textual Similar-ity Benchmark (STS-B; Cer et al., 2017) requirespredicting a similarity score between 1 and 5 for asentence pair; we report the Spearman correlationcoefﬁcient (scc).Finally, there are four natural language infer-ence tasks: the Multi-Genre Natural Language In-ference (MNLI; Williams et al., 2018), the Stan-ford Question Answering Dataset (QNLI; Ra-jpurkar et al., 2016), the Recognizing Textual En-tailment (RTE; Dagan et al., 2006, Bar Haim et al.,2006, Ciampiccolo et al., 2007 Bentivogli et al.,2009). We exclude the Winograd NLI task fromour results similar to Radford et al. (2018); De-vlin et al. (2018) and report accuracy. For MNLIwe report both matched (m) and mismatched (mm)accuracy on test.We also report an average over the GLUE met-rics. This ﬁgure is not comparable to the aver-age on the ofﬁcial GLUE leaderboard since we ex-clude Winograd and do not report MRPC accuracy\f5365CoLA(mcc)SST-2(acc)MRPC(F1)STS-B(scc)QQP(F1)MNLI-(m/mm)(acc)QNLI(acc)RTE(acc)AvgOpenAI GPT45.491.382.380.070.382.1/81.488.156.075.2CNN Base53.193.681.382.270.582.5/82.289.564.677.7CNN Large52.894.683.783.471.784.3/83.889.863.778.6BPE Large51.894.083.084.270.682.9/82.289.365.178.1GPT on STILTs47.293.187.784.870.180.7/80.687.269.177.8BERTBASE52.193.588.985.871.284.6/83.490.166.479.6BERTLARGE60.594.989.386.572.186.7/85.991.170.181.9Table 2: Test results as per the GLUE evaluation server. The average column does not include the WNLI test set.mcc = Matthews correlation, acc = Accuracy, scc = Spearman correlation.STS-B Pearson correlation as well as QQP accu-racy.Table 2 shows results for three conﬁgurationsof our approach (cf. Table 1). The BPE modelhas more parameters than the CNN model butdoes not perform better in aggregate, however, it isfaster to train. All our models outperform the uni-directional transformer (OpenAI GPT) of Radfordet al. (2018), however, our model is about 50%larger than their model. We also show results forSTILTs (Phang et al., 2018) and BERT (Devlinet al., 2018). Our CNN base model performs aswell as STILTs in aggregate, however, on sometasks involving sentence-pairs, STILTs performsmuch better (MRPC, RTE); there is a similar trendfor BERT.STILTs adds another ﬁne-tuning step on an-other downstream task which is similar to the ﬁ-nal task. The technique is equally applicable toour approach. Training examples for our modelare Common Crawl paragraphs of arbitrary length.We expect that tailoring training examples for lan-guage model pretraining to the end tasks to signif-icantly improve performance. For example, BERTtrains on exactly two sentences while as we trainon entire paragraphs.6.2Structured PredictionWe also evaluated performance on two structuredpredictions tasks, NER and constituency parsing.For both problems, we stacked task-speciﬁc archi-tectures from recent work on top of our pretrainedtwo tower models. We evaluate two ways of stack-ing: (1) ELMo-style, where the pretrained mod-els are not ﬁne-tuned but are linearly combined atdifferent depths, and (2) with ﬁne-tuning, wherewe set different learning rates for the task-speciﬁcModeldev F1test F1ELMoBASE95.792.2CNN Large + ELMo96.493.2CNN Large + ﬁne-tune96.993.5BERTBASE96.492.4BERTLARGE96.692.8Table 3: CoNLL-2003 Named Entity Recognition re-sults. Test result was evaluated on parameter set withthe best dev F1.Modeldev F1test F1ELMoBASE95.295.1CNN Large + ELMo95.195.2CNN Large + ﬁne-tune95.595.6Table 4: Penn Treebank Constituency Parsing results.Test result was evaluated on parameter set with the bestdev F1.layers but otherwise update all of the parametersduring the task-speciﬁc training.6.2.1Named Entity RecognitionWe evaluated span-level F1 performance on theCoNLL 2003 Named Entity Recognition (NER)task, where spans of text must be segmented andlabeled as Person, Organization, Location, or Mis-cellaneous. We adopted the NER architecture inPeters et al. (2018), a biLSTM-CRF, with two mi-nor modiﬁcations: (1) instead of two layers of biL-STM, we only used one, and (2) a linear projectionlayer was added between the token embedding andbiLSTM layer. We did grid search on the pairs oflearning rate, and found that projection-biLSTM-\f5366CoLA(mcc)SST-2(acc)MRPC(F1)STS-B(scc)QQP(F1)MNLI-m(acc)QNLI(acc)RTE(acc)Avgcloze55.192.988.388.387.282.386.566.480.9bilm50.092.486.687.186.181.784.066.479.3cloze + bilm52.693.288.987.987.282.186.165.580.4Table 5: Different loss functions on the development sets of GLUE (cf. Table 2). Results are based on the CNNbase model (Table 1)CRF with 1E-03 and pretrained language modelwith 1E-05 gave us the best result.Table 3 shows the results, with comparisonto previous published ELMoBASE results (Peterset al., 2018) and the BERT models. Both of ourstacking methods outperform the previous state ofthe art, but ﬁne tuning gives the biggest gain.6.2.2Constituency ParsingWe also report parseval F1 for Penn Treebank con-stituency parsing. We adopted the current state-of-the-art architecture (Kitaev and Klein, 2018). Weagain used grid search for learning rates and num-ber of layers in parsing encoder, and used 8E-04for language model ﬁnetuning, 8E-03 for the pars-ing model parameters, and two layers for encoder.Table 4 shows the results. Here, ﬁne tuning isrequired to achieve gains over the previous stateof the art, which used ELMo embeddings.6.3Objective functions for pretrainingThe two-tower model is trained to predict the cur-rent token given representations of the entire leftand right context (cloze). Next we compare thischoice to two alternatives:First, Peters et al.(2018) train two language models operating left-to-right and right-to-left to predict the next wordfor each respective direction. We change the two-tower model to predict the next word using the in-dividual towers only and remove the combinationmodule on top of the two towers (bilm); however,we continue to jointly train the two towers.Second, we combine the cloze loss with thebilm loss to obtain a triplet loss which trains themodel to predict the current word given both leftand right context, as well as just right or left con-text. The latter is much harder than the cloze losssince less context is available and therefore gradi-ents for the bilm loss are much larger: the clozemodel achieves perplexity of about 4 while as forthe bilm it is 27-30, depending on the direction.This results in the bilm loss dominating the triplet562M1.1B2.25B4.5B9B18B8080.58181.5Train data tokensAvg. GLUE scoreAverage GLUE scoreFigure 3: Average GLUE score with different amountsof Common Crawl data for pretraining.loss and we found that scaling the bilm term by afactor of 0.15 results in better performance.Table 5 shows that the cloze loss performs sig-niﬁcantly better than the bilm loss and that com-bining the two loss types does not improve overthe cloze loss by itself.We conjecture that in-dividual left and right context prediction tasksare too different from center word prediction andthat their learning signals are not complementaryenough.6.4Domain and amount of training dataNext we investigate how much pretraining beneﬁtsfrom larger training corpora and how the domainof the data inﬂuences end-task performance.Figure 3 shows that more training data can sig-niﬁcantly increase accuracy. We train all modelswith the exact same hyper-parameter settings onCommon Crawl data using the CNN base archi-tecture for 600K updates. We train on up to 18BCommon Crawl tokens and the results suggest thatmore training data is likely to further increase per-formance.Table 6 shows a breakdown into individual\f5367train data(M tok)CoLA(mcc)SST-2(acc)MRPC(F1)STS-B(scc)QQP(F1)MNLI-m(acc)QNLI(acc)RTE(acc)Avgccrawl56252.592.988.288.387.181.785.763.379.9112555.593.186.188.487.181.985.765.280.4225055.492.487.788.487.282.286.266.980.8450056.693.087.388.687.082.086.265.780.8900055.192.988.388.387.282.386.566.480.91800056.393.188.088.887.282.386.368.481.3newscrawl56250.992.881.478.284.979.182.055.775.6112551.493.083.082.385.279.782.853.976.4225054.892.983.582.885.480.482.454.877.1450053.993.683.883.185.580.483.654.277.3BWiki - sent330053.591.686.486.286.982.386.963.879.7BWiki - blck330050.691.986.487.186.881.986.260.478.9Table 6: Effect of different domains and amount of data for pretraining on the on the development sets of GLUE(cf. Table 2). Results are based on the CNN base model (Table 1).GLUE tasks. For pretraining on Common Crawl,CoLA and RTE beneﬁt most from additional train-ing data. The same table also shows results forNews Crawl which contains newswire data. Thisdata generally performs less well than CommonCrawl, even on MRPC which is newswire.Alikely reason is that News Crawl examples are in-dividual sentences of 23 words on average whichcompares to several sentences or 50 words on av-erage for Common Crawl. Mutli-sentence trainingexamples are more effective for end-tasks basedon sentence pairs, e.g., there is a 14 point accu-racy gap on RTE between News Crawl and Com-mon Crawl with 4.5B tokens. More News Crawldata is most beneﬁcial for CoLA and STS-B.We also experiment with BooksCorpus (Zhuet al., 2015) as well as English Wikipedia, similarto Devlin et al. (2018). Examples in BooksCorpusare a mix of individual sentences and paragraphs;examples are on average 36 tokens. Wikipedia ex-amples are longer paragraphs of 66 words on av-erage. To reduce the effect of training on exam-ples of different lengths, we adopted the followingstrategy: we concatenate all training examples intoa single string and then crop blocks of 512 consec-utive tokens from this string. We train on a batchof these blocks (BWiki - blck). It turns out that thisstrategy did not work better compared to our exist-ing strategy of simply using the data as is (BWiki -sent). BooksCorpus and Wikipedia performs verywell on QNLI and MNLI but less well on othertasks.In summary, more data for pretraining improvesperformance, keeping everything else equal. Alsopretraining on corpora that retains paragraphstructure performs better than individual sen-tences.7ConclusionWe presented a pretraining architecture based on abi-directional transformer model that predicts ev-ery token in the training data. The model is trainedwith a cloze-style objective and predicts the centerword given all left and right context.Results on the GLUE benchmark show largegains over Radford et al. (2018) for each task,while experiments with model stacking set newstate of the art performance levels for parsing andnamed entity recognition. We also did extensiveexperimental analysis to better understand theseresults, showing that (1) having multiple sentencesin each training example is crucial for many tasks;(2) pre-training continues to improve performanceup to 18B tokens and would likely continue to im-prove with more data; and ﬁnally (3) our novelcloze-driven training regime is more effective thanpredicting left and right tokens separately.In future work, we will investigate variationsof our architecture. In particular, we had initialsuccess sharing the parameters of the two towerswhich allows training much deeper models with-out increasing the parameter count.\f5368ReferencesAlexei Baevski and Michael Auli. 2018. Adaptive in-put representations for neural language modeling.arXiv, abs/1809.10853.Luisa Bentivogli, Peter Clark, Ido Dagan, and DaniloGiampiccolo. 2009.The ﬁfth pascal recognizingtextual entailment challenge. In Proc. of TAC.Ondˇrej Bojar, Christian Federmann, Mark Fishel,Yvette Graham, Barry Haddow, Matthias Huck,Philipp Koehn, and Christof Monz. 2018.Find-ings of the 2018 conference on machine translation(WMT18). In Proc. of WMT.Daniel M. Cer, Mona T. Diab, Eneko Agirre, I˜nigoLopez-Gazpio, and Lucia Specia. 2018. Semeval-2017 task 1: Semantic textual similarity - multilin-gual and cross-lingual focused evaluation. In Proc.of SemEval.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.The pascal recognizing textual entailmentchallenge.Machine learning challenges, evaluat-ing predictive uncertainty, visual object classiﬁca-tion, and recognizing textual entailment, pages 177–190.AndrewM.DaiandQuocV.Le.2015.Semi-supervisedsequencelearning.arXiv,abs/1511.01432.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018. Bert: Pre-training of deepbidirectional transformers for language understand-ing. CoRR, abs/1810.04805.William B. Dolan and Chris Brockett. 2005. Automati-cally constructing a corpus of sentential paraphrases.In Proc. of IWP.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan. 2007.The pascal recognizingtextual entailment challenge.Proc. of the ACL-PASCAL workshop on textual entailment and para-phrasing.Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-mand Joulin, and Tomas Mikolov. 2018. Learningword vectors for 157 languages. In Proc. of LREC.Edouard Grave, Armand Joulin, Moustapha Ciss´e,David Grangier, and Herv´e J´egou. 2017. Efﬁcientsoftmax approximation for gpus. In Proc. of ICML.Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, and IdanSzpektor. 2006. The pascal recognising textual en-tailment challenge.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. 2015.Deep Residual Learning for ImageRecognition. In Proc. of CVPR.Hakan Inan, Khashayar Khosravi, and Richard Socher.2016.Tying word vectors and word classiﬁers:A loss framework for language modeling.arXiv,abs/1611.01462.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M Rush. 2016. Character-aware neural languagemodels. In AAAI, pages 2741–2749.Diederik P. Kingma and Jimmy Ba. 2015. Adam: AMethod for Stochastic Optimization.In Proc. ofICLR.Nikita Kitaev and Dan Klein. 2018. Constituency pars-ing with a self-attentive encoder. In Proc. of ACL.Ilya Loshchilov and Frank Hutter. 2016.SGDR:stochastic gradient descent with restarts.arXiv,abs/1608.03983.Bryan McCann, James Bradbury, Caiming Xiong, andRichard Socher. 2017. Learned in translation: Con-textualized word vectors. In Proc. of NIPS.Oren Melamud, Jacob Goldberger, and Ido Dagan.2016. context2vec: Learning generic context em-bedding with bidirectional lstm. In Proc. of CoNLL.Myle Ott, Sergey Edunov, Alexei Baevski, AngelaFan, Sam Gross, Nathan Ng, David Grangier, andMichael Auli. 2019.fairseq: A fast, extensibletoolkit for sequence modeling. In Proc. of NAACLSystem Demonstrations.Myle Ott,Sergey Edunov,David Grangier,andMichael Auli. 2018. Scaling neural machine trans-lation. In Proc. of WMT.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013. On the difﬁculty of training recurrent neuralnetworks. In Proc. of ICML.Matthew E Peters, Mark Neumann, Mohit Iyyer, MattGardner, Christopher Clark, Kenton Lee, and LukeZettlemoyer. 2018. Deep contextualized word rep-resentations. In Proc. of ACL.Jason Phang, Thibault Fevry, and Samuel R. Bowman.2018. Sentence encoders on stilts: Supplementarytraining on intermediate labeled-data tasks. arXiv,abs/1811.01088.Oﬁr Press and Lior Wolf. 2017. Using the output em-bedding to improve language models. In Proc. ofEACL.AlecRadford,KarthikNarasimhan,TimSali-mans,and Ilya Sutskever. 2018.Improvinglanguage understanding by generative pre-training.https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,and Percy Liang. 2016.Squad: 100, 000+ ques-tions for machine comprehension of text.arXiv,abs/1606.05250.\f5369Rico Sennrich, Barry Haddow, and Alexandra Birch.2016. Neural machine translation of rare words withsubword units. In Proc. of ACL.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. 2013.Recursive deep modelsfor semantic compositionality over a sentiment tree-bank. In Proc. of EMNLP.Ilya Sutskever, James Martens, George E. Dahl, andGeoffrey E. Hinton. 2013. On the importance of ini-tialization and momentum in deep learning. In Proc.of ICML.Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention Is AllYou Need. In Proc. of NIPS.Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R. Bowman. 2018.GLUE: A multi-task benchmark and analysis plat-form for natural language understanding.arXiv,abs/1804.07461.Alex Warstadt, Amanpreet Singh, and Sam Bow-man. 2018.Corpus of linguistic acceptability.https://nyu-mll.github.io/CoLA.Adina Williams, Nikita Nangia, and Samuel R Bow-man. 2018. A broad-coverage challenge corpus forsentence understanding through inference. In Proc.of NAACL.Kelly W. Zhang and Samuel R. Bowman. 2018. Lan-guage modeling teaches you more syntax than trans-lation does: Lessons learned through auxiliary taskanalysis. arXiv, abs/1809.10040.Yukun Zhu, Ryan Kiros, Richard S. Zemel, RuslanSalakhutdinov, Raquel Urtasun, Antonio Torralba,and Sanja Fidler. 2015. Aligning books and movies:Towards story-like visual explanations by watchingmovies and reading books. arXiv, abs/1506.06724.",
        "answer": "The 18,000 million tokens (M tok) training data size from the Common Crawl resulted in the highest average accuracy of 81.3 across all GLUE tasks.",
        "space_key_index": 2,
        "metadata": {
            "format": "PDF 1.3",
            "title": "Cloze-driven Pretraining of Self-attention Networks",
            "author": "Alexei Baevski ; Sergey Edunov ; Yinhan Liu ; Luke Zettlemoyer ; Michael Auli",
            "subject": "D19-1 2019",
            "keywords": "",
            "creator": "LaTeX with hyperref package",
            "producer": "pdfTeX-1.40.18",
            "creationDate": "D:20190829113438Z",
            "modDate": "D:20190829113438Z",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "Is the Seq2Seq model shown to produce longer or shorter sentences when compared to human responses?",
        "context": "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 87–92Brussels, Belgium, October 31, 2018. c⃝2018 Association for Computational LinguisticsISBN 978-1-948087-75-987Retrieve and Reﬁne:Improved Sequence Generation Models For DialogueJason Weston, Emily Dinan and Alexander H. MillerFacebook AI Researchjase@fb.com, edinan@fb.com, ahm@fb.comAbstractSequence generation models for dialogue areknown to have several problems: they tend toproduce short, generic sentences that are un-informative and unengaging. Retrieval modelson the other hand can surface interesting re-sponses, but are restricted to the given retrievalset leading to erroneous replies that cannot betuned to the speciﬁc context. In this work wedevelop a model that combines the two ap-proaches to avoid both their deﬁciencies: ﬁrstretrieve a response and then reﬁne it – the ﬁnalsequence generator treating the retrieval as ad-ditional context. We show on the recent CON-VAI2 challenge task our approach producesresponses superior to both standard retrievaland generation models in human evaluations.1IntroductionSequencegenerationmodelslikeSeq2Seq(Sutskever et al., 2014) are increasingly popularfor tasks such as machine translation (MT) andsummarization,where generation is suitablyconstrained by the source sentence.However,obtaining good performance on dialogue tasks,where the context still allows many interpreta-tions, remains an open problem despite muchrecent work (Serban et al., 2016).Several au-thors report the issue that they produce short,generic sentences containing frequent words –the so-called “I don’t know” problem – as thatresponse can work as a reply in many instances,but is uninformative and unengaging.Retrievalmodels (Ji et al., 2014) do not have this problem,but instead either produce engaging responses orelse completely erroneous ones which they cannotProceedings of the 2018 EMNLP Workshop SCAI: The2nd International Workshop on Search-Oriented Conversa-tional AI 978-1-948087-75-9tune to the speciﬁc context, as they can onlyproduce a valid reply if it is in the retrieval set.In this work we propose a Retrieve and Reﬁnemodel to gain the advantages of both methods,and avoid both their disadvantages. Models thatproduce an initial prediction and then reﬁne it aregrowing in traction in NLP. They have been usedin MT and summarization either for reﬁnement ofinitial predictions (Junczys-Dowmunt and Grund-kiewicz, 2017; Niehues et al., 2016; Novak et al.,2016; Xia et al., 2017; Grangier and Auli, 2017)or combining with retrieval (Gu et al., 2017; Caoet al., 2018), as well as for sentence correctionor reﬁnement without context (Guu et al., 2017;Schmaltz et al., 2017). There is little work in ap-plying these methods to dialogue; one work we areaware of has been done concurrently with ours isPandey et al. (2018). The usefulness of our ap-proach is shown with detailed experiments on theConvAI2 dataset1 which is a chit-chat task to getto know the other speaker’s proﬁle, obtaining gen-erations superior to both retrieval and sequencegeneration models in human evaluations.2Retrieve and ReﬁneThe model we propose in this work is remark-ably straight-forward: we take a standard gener-ative model and concatenate the output of a re-trieval model to its usual input, and then generateas usual, training the model under this setting.For the generator, we use a standard Seq2Seqmodel: a 2-layer LSTM with attention. For theretriever, we use the Key-Value Memory Network(Miller et al., 2016) already shown to perform wellfor this dataset (Zhang et al., 2018), which attendsover the dialogue history, to learn input and candi-date retrieval embeddings that match using cosinesimilarity. The top scoring utterance is provided1http://convai.io/\f88as input to our Seq2Seq model in order to reﬁneit, prepended with a special separator token. Forboth models we use the code available in ParlAI2.At test time the retriever retrieves candidates fromthe training set.To train our model we ﬁrst precompute the re-trieval result for every dialogue turn in the train-ing set, but instead of using the top ranking resultswe rerank the top 100 predictions of each by theirsimilarity to the label (in embedding space). Fol-lowing Guu et al. (2017) this should help avoid theproblem of the reﬁnement being too far away fromthe original retrieval. We then append the cho-sen utterances to the input sequences used to trainSeq2Seq.We refer to our model as RetrieveN-Reﬁne, or RetNRef for short.We also considertwo variants of the model in the following that wefound improve the results.Use Retriever MoreIn our vanilla model, wenoticed there was not enough attention being paidto the retrieval utterance by the generator. As theinput to Seq2Seq is the dialogue history concate-nated with the retrieval utterance, truncating thehistory is one way to pay more attention to the re-trieval. In particular for the ConvAI2 dataset weclip the initial proﬁle sentences at the start of thedialogue, forcing the model to more strongly relyon the retriever which still has them.3 We refer tothis modiﬁcation as RetrieveNReﬁne+.Fix Retrieval Copy ErrorsOur model learns tosometimes ignore the retrieval (when it is bad),sometimes use it partially, and other times sim-ply copy it. However, when it is mostly copiedbut only changes a word or two, we observed itmade mistakes more often than not, leading toless meaningful utterances.We thus also con-sider a variant that exactly copies the retrievalif the model generates with large word over-lap (we chose >60%).Otherwise, we leavethe generation untouched.4We refer to this asRetrieveNReﬁne++.2http://parl.ai3Architectural changes might also deal with this issue,e.g. treating the two inputs as independent sources to do at-tention over, but we take the simplest possible approach here.4Other approaches might also help with this problem suchas using an explicit copy mechanism or to use BPE tokeniza-tion (Fan et al., 2017), but we leave those for future work.RetNRef Retrieval MethodPPLNone (Vanilla Seq2Seq)31.4Random label32.0Memory Network31.8True label’s neighbor25.9True label9.2Table 1: Perplexity on the ConvAI2 task test set withdifferent types of retriever for RetNRef, see text.3ExperimentsWe conduct experiments on the recent ConvAI2challenge dataset which uses a modiﬁed versionof the PersonaChat dataset (Zhang et al., 2018)(larger, and with different processing). The datasetconsists of conversations between crowdworkerswho were randomly paired and asked to act thepart of a given persona (randomly assigned from1155 possible personas, created by another set ofworkers), chat naturally, and get to know eachother during the conversation. There are around160,000 utterances in around 11,000 dialogues,with 2000 dialogues for validation and test, whichuse non-overlapping personas.3.1Automatic Evaluation and AnalysisPerplexityDialogue is known to be notoriouslyhard to evaluate with automated metrics (Liu et al.,2016). In contrast to machine translation, there ismuch less constraint on the output with many validanswers with little word overlap, e.g. there aremany answers to “what are you doing tonight?”.Nevertheless many recent papers report perplexityresults in addition to human judgments. For the re-trieve and reﬁne case, perplexity evaluation is par-ticularly ﬂawed: if the retrieval points the model toa response that is very different from (but equallyvalid as) the true response, the model might focuson reﬁning that and get poor perplexity.We therefore test our model by consideringvarious types of retrieval methods: (i) the bestperforming existing retriever model, the MemoryNetwork approach from Zhang et al. (2018) (re-trieving from the training set), (ii) a retriever thatreturns a random utterance from the training set,(iii) the true label given in the test set, and (iv) theclosest nearest neighbor from the training set ut-terances to the true label, as measured by the em-bedding space of the Memory Network retrievermodel. While (iii) and (iv) cannot be used in a de-ployed system as they are unknown, they can be\f89WordCharRare Word %Methodcntcnt<100<1kSeq2Seq11.740.50.4%5.8%RetNRef11.840.41.1%6.9%RetNRef+12.145.01.7%10.1%RetNRef++12.748.12.3%10.9%MemNet13.154.54.0%15.3%Human13.054.63.0%11.5%Table 2: Output sequence statistics for the methods.Seq2Seq generates shorter sentences with more com-mon words than humans, which RetNRef alleviates.Method<30%30-60%60-80%>80%Seq2Seq56%34%7%3%RetNRef41%38%13%8%RetNRef+26%20%12%42%RetNRef++26%20%0%53%Table 3:Word overlap between retrieved and gener-ated utterances in RetNRef, and between Seq2Seq andthe Memory Network retriever (ﬁrst row).used as a sanity check: a useful retrieve and reﬁneshould improve perplexity if given these as input.We also compare to a standard Seq2Seq model, i.e.no retrieval.The results are given in Table 1. They show thatthe RetNRef model can indeed improve perplexitywith label neighbors or the label itself. However,surprisingly there is almost no difference betweenusing no retrieval, random labels or our best re-triever. The RetNRef++ model – that truncatesthe dialogue history and focuses more on the re-trieval utterance – does even worse in terms ofperplexity: 48.4 using the Memory Network re-triever. However, poor perplexity does not meanhuman judgments of the generated sequences willnot improve; in fact we will see that they do in thenext section. How to automatically evaluate thesekinds of models still remains an open problem.Word StatisticsAnother way to measure thesalience of a generation model is to compare it tohuman utterances in terms of word statistics. Weanalyze the word statistics of our models in Table2. Seq2Seq models are known to produce shortsentences with more common words than humans.The statistics on the ConvAI2 dataset bear this out,where the Seq2Seq model responses have lowerword and character counts and use fewer rarewords than the human responses. The RetNRefmodel (using the Memory Network retriever, re-trieving from the training set) makes some im-provements in this regard, e.g. doubling the useof rare words (with frequency less than 100) andsmaller gains for words with frequency less than1000, but are still not close to human statistics.The RetNRef++ model which boosts the use ofthe retrieval does better in this regard, making thestatistics much closer to human ones. Of coursethese metrics do not measure whether the utter-ances are semantically coherent, but it is encour-aging to see a model using rare words as withoutthis we believe it is hard for it to be engaging.Table 3 compares the word overlap between re-trieved and generated sentences in the RetNRefvariants in order to measure if RetNRef is eitherignoring the retriever, or else paying too much at-tention to and copying it. As comparison, the ﬁrstrow also shows the overlap between the retrieverand vanilla Seq2Seq which does not retrieve atall. The results show that RetNRef++ has >80%word overlap with the retriever output around half(53%) of the time, whereas Seq2Seq and RetNRefvery rarely overlap with the retriever (3% and 8%of the time respectively have >80% overlap). Thisshows that our improved model RetNRef++ doesuse the retriever, but can also generate novel con-tent when it wants to, which a standard retrievercannot.3.2Evaluation by Human Judgement ScoresFollowing the protocol in Zhang et al. (2018), weasked humans to conduct short dialogues with ourmodels (100 dialogues each of 10-20 turns, so 600dialogues in total), and then measure the engag-ingness, consistency, and ﬂuency (all scored outof 5) as well as to try to detect the persona thatthe model is using, given the choice between thatand a random persona. Consistency measures thecoherence of the dialogue, e.g. “I have a dog” fol-lowed by “I have no pets” is not consistent.The results are given in Table 4. They showengagingness scores superior to Seq2Seq for allRetNRef variants, and with RetNRef++ slightlyoutperforming the retriever which it conditionson.Importantly however, it maintains this per-formance whilst still being able to generate textwhich a retrieval model cannot. It also performswell in the other metrics, although like the Mem-ory Network model, it is weaker at using the per-sona than Seq2Seq.Seq2Seq is inferior to the\f90MethodEngagingnessFluencyConsistencyPersonaSeq2Seq (PPL)2.70(1.17)3.50(1.37)3.90(1.37)0.90(0.29)Seq2Seq (100 epochs)2.76(1.15)3.53(1.14)3.84(1.38)0.85(0.35)Memory Network3.66(1.26)3.83(1.26)3.61(1.36)0.73(0.44)RetrieveNReﬁne2.94(1.26)3.65(1.28)3.72(1.32)0.90(0.30)RetrieveNReﬁne+3.50(1.33)3.63(1.13)3.55(1.33)0.71(0.45)RetrieveNReﬁne++3.80(1.18)3.74(1.19)3.80(1.40)0.65(0.47)Table 4: Human Evaluation scores for the models,scoring ﬂuency, engagingness, consistency and persona detec-tion, with standard deviation in parentheses. We consider engagingness to be the most important metric.Comparison (A vs. B)Win RateA WinsB WinsTiep-valueRetrieveNReﬁne++ vs. Memory Network54.5%3402845720.027RetrieveNReﬁne++ vs. Seq2Seq53.7%5714922030.016RetrieveNReﬁne++(retrieved) vs.Seq2Seq53.8%29024987RetrieveNReﬁne++(generated) vs.Seq2Seq53.6%281243116RetrieveNReﬁne+ vs. Memory Network51.63%492461243RetrieveNReﬁne+ vs. Human30.13%6916014Seq2Seq vs. Human26.84%6216922Table 5: A/B testing by humans comparing model pairs, reporting win rates for A over B (excluding ties).Person 1hey , george here . lets be friendsyeah. well, i wish you luck there. we bought a cute 2 story house.Person 2hi george , i’m regina . do you enjoy dance ?nice , i don’t think i have ever left ohio.Seq2Seqi do not but i do have a lot of time to dancei am sorry. i am so scared of twins.MemNethello george my name is pinki am from mississippi . just moved back here from austin texasRetNRef+i do. i enjoy dance.i am from tennessee. just moved here from austin texas.Person 1drinking coffee? what time is it there?thank you for that , you are what you believe ?Person 2830 pm haha my dad is a construction workerlook energy isn’t physical , only spiritualso he works really late nightsSeq2Seqi am a big guy so i can help youi am not sure what that meansMemNetyep . moonlight is amazing but northerndo you do any sports ? swimming helps me keep my energy up.light grab you tooRetNRef+oh wow, that is a long timedo you do any sports ? swimming helps me keep my energy upTable 6: Example dialogues (left) where RetNRef outperforms MemNet, and (right) where RetNRef outperformsSeq2Seq – by either paying attention to the retriever or not. The ﬁrst two rows are the previous two dialogue turnsby Person 1 & 2, the following rows are the predictions for the next utterance of Person 1 by the various models.Memory Network Retriever in terms of engage-ment, in line with previous results. We also triedovertraining the Seq2Seq for 100 epochs insteadof early stopping by validation on perplexity as itmay overﬁt training sentences and act more as aretriever, but this did not help.Some example dialogues of the RetNRef++model performing well (as scored by the evalua-tors) are shown in Tables 7 and 8. Longer sen-tences from the bot (person 2) with more nu-anced entity information typically come from at-tending to the retriever, whereas the generator canalso produce shorter replies independent of the re-triever that ﬁt the context well. There are still is-sues however, such as repeated phrases by the gen-erator, and some tendency to copy the speakingpartner’s phrases that could be improved.3.3A/B Testing by HumansTo compute a statistically more meaningful test,and to evaluate models more clearly against eachother, we also tested models by pairing themagainst each other (different to the protocol pre-viously used for this dataset). We took human-human paired dialogues from the test set, up to arandomly chosen turn, and then computed the re-sponse of two rival models. The resulting dialoguehistory and the two choices are given to a third an-notator to evaluate and asked which model is best;they can also choose “unsure”. We collected over1000 such examples, with each conversation beingevaluated twice. The results are given in Table 5.\f91Persona 1Persona 2I am relatively tall.I like to play baseball.I like mexican food.I love eating seafood.I drive a subaru legacy.I enjoy learning new languages.My dog’s name is Drunky.I’m currently going for a marketing degree.I can touch my toes with my nose.I want to design ads in the future.[PERSON 1:] I’m driving to the mexican place tonight for dinner, but my Subaru is at the rental shop.[PERSON 2:] Yummy. I love mexican food, but I never eat vegetables[PERSON 1:] Oh, I do, I think that’s why i’m so tall![PERSON 2:] how tall are you? I do not... I enjoy studying languages.[PERSON 1:] I’m 6 foot, a lot taller than my dog, Drunky.[PERSON 2:] I am sorry to hear that. I want to be 6 foot tall.[PERSON 1:] Do you have any talents? I can reach my toes using my nose.[PERSON 2:] Cool! I listen to audiobooks and speak one asian language. you like sushi? I do.[PERSON 1:] Oh sushi is my next favorite, after Mexican. What kind of car do you have?[PERSON 2:] I have a Nissan Sentra. I love it.Table 7: Example dialog between a human (Person 1) and the RetNRef++ model (Person 2). References to nevereating vegetables, listening to audiobooks and speaking one asian language and owning a Nissan Sentra appearedin the conditioned retrieved sentences, improving the overall engaging quality of the dialogue.Persona 1Persona 2I am a lacto-ovo vegetarian.I have brown hair.I like to hike.I am Latina.I have red hair.My mother works for special needs clients..My favorite drink is sprite.I am a healer.I have a pet parrot named Tasha.I love my family.[PERSON 1:] I dont know about you but I love sprite. Its my favorite.[PERSON 2:] I like dr pepper a lot, but sprite is my second favorite.[PERSON 1:] Sprites great! I take it with me every day on my hikes.[PERSON 2:] I bet. I love to go to the water[PERSON 1:] Ya that sounds like a great time. I go with my pet parrot Tasha when I go[PERSON 2:] that is a good idea. I wish I had a parrot.[PERSON 1:] Ya its fun. As a pet owner you learn to love animals; im even a lactoovo vegetarian now[PERSON 2:] I love animals. I am vegan so I eat vegan food.[PERSON 1:] Sounds like me :D im vegan, im ginger, and i love sprite[PERSON 2:] I love animals. I am vegan so I eat a lot of it.Table 8: Example dialog between a human (Person 1) and the RetNRef++ model (Person 2). References toDr. Pepper and wishing to have a parrot appeared in the conditioned retrieved sentences, improving the overallengaging quality of the dialogue. There are still issues however, such as repeated phrases by the generator, failureto use the persona adequately, and some tendency to copy the speaking partner’s phrases that could be improved.RetrieveNReﬁne obtains statistically signiﬁcantwins over the retriever Memory Network modeland the generator Seq2Seq model using a binomialtwo-tailed test, with win rates ∼54%. Breakingdown the wins between when RetNRef++ exactlycopies the retrieval utterance vs. generates we seethat it chooses them about equally, with wins aboutequal in both cases. This shows it can effectivelylearn when to choose the retrieval utterance (whenit is good), and when to ignore it and generate in-stead (when it is bad). Table 6, which shows ex-ample outputs of our model, illustrates this.RetNRef+ sometimes loses out when makingsmall changes to the retrieved text, for exampleit made changes to “i once broke my nose try-ing to peak in on a jazz concert !” by replacingpeak with glacier. Recall that RetNRef++ ﬁxesthis problem by exactly copying the retrieved textwhen there is insigniﬁcant word overlap with thegenerated text; as such, it has a correspondinglylarger win rate against Memory Networks (54.5%versus 51.63%).We also computed a small sample of A/B testsdirectly against humans rather than models, andagain see the win rate is higher for RetNRef.4ConclusionIn conclusion, we showed that retrieval modelscan be successfully used to improve generationmodels in dialogue, helping them avoid commonissues such as producing short sentences with fre-quent words that ultimately are not engaging. Our\f92RetNRef++ model has similar statistics to humanutterances and provides more engaging conversa-tions according to human judgments.Future work should investigate improved waysto incorporate retrieval in generation, both avoid-ing the heuristics we used here to improve per-formance, and seeing if more sophisticated ap-proaches than concatenation plus attention im-prove the results, for example by more clearlytreating the inputs as independent sources, ortraining the models jointly.ReferencesZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.2018. Retrieve, rerank and rewrite: Soft templatebased neural summarization. In Proceedings of the56th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), vol-ume 1, pages 152–161.Angela Fan, David Grangier, and Michael Auli. 2017.Controllable abstractive summarization.arXivpreprint arXiv:1711.05217.David Grangier and Michael Auli. 2017. Quickedit:Editing text & translations via simple delete actions.arXiv preprint arXiv:1711.04805.Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-tor OK Li. 2017.Search engine guided non-parametric neural machine translation.arXivpreprint arXiv:1705.07267.KelvinGuu,TatsunoriBHashimoto,YonatanOren, and Percy Liang. 2017.Generating sen-tences by editing prototypes.arXiv preprintarXiv:1709.08878.Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. Aninformation retrieval approach to short text conver-sation. arXiv preprint arXiv:1408.6988.Marcin Junczys-Dowmunt and Roman Grundkiewicz.2017.An exploration of neural sequence-to-sequence architectures for automatic post-editing.arXiv preprint arXiv:1706.04138.Chia-Wei Liu, Ryan Lowe, Iulian V Serban, MichaelNoseworthy, Laurent Charlin, and Joelle Pineau.2016. How not to evaluate your dialogue system:An empirical study of unsupervised evaluation met-rics for dialogue response generation. arXiv preprintarXiv:1603.08023.Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason We-ston. 2016.Key-value memory networks fordirectlyreadingdocuments.arXivpreprintarXiv:1606.03126.Jan Niehues, Eunah Cho, Thanh-Le Ha, and AlexWaibel. 2016.Pre-translation for neural machinetranslation. arXiv preprint arXiv:1610.05243.Roman Novak, Michael Auli, and David Grangier.2016. Iterative reﬁnement for machine translation.arXiv preprint arXiv:1610.06602.Gaurav Pandey, Danish Contractor, Vineet Kumar, andSachindra Joshi. 2018. Exemplar encoder-decoderfor neural conversation generation.In Proceed-ings of the 56th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), volume 1, pages 1329–1338.Allen Schmaltz, Yoon Kim, Alexander M Rush,and Stuart M Shieber. 2017.Adapting sequencemodels for sentence correction.arXiv preprintarXiv:1707.09067.Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, andJoelle Pineau. 2016.Generative deep neural net-works for dialogue: A short review. arXiv preprintarXiv:1611.06216.Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.Sequence to sequence learning with neural net-works. In Advances in neural information process-ing systems, pages 3104–3112.Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,Nenghai Yu, and Tie-Yan Liu. 2017. Deliberationnetworks: Sequence generation beyond one-pass de-coding. In Advances in Neural Information Process-ing Systems, pages 1782–1792.Saizheng Zhang, Emily Dinan, Jack Urbanek, ArthurSzlam, Douwe Kiela, and Jason Weston. 2018. Per-sonalizing dialogue agents: I have a dog, do youhave pets too? arXiv preprint arXiv:1801.07243.",
        "answer": "Shorter.",
        "space_key_index": 3,
        "metadata": {
            "format": "PDF 1.3",
            "title": "Retrieve and Refine: Improved Sequence Generation Models For Dialogue",
            "author": "Jason Weston ; Emily Dinan ; Alexander Miller",
            "subject": "SCAI 2018",
            "keywords": "",
            "creator": "LaTeX with hyperref package",
            "producer": "pdfTeX-1.40.18",
            "creationDate": "D:20180904035547Z",
            "modDate": "D:20180904035547Z",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "What is the main content of section 4.2?",
        "context": "Proceedings of the 27th International Conference on Computational Linguistics, pages 1384–1397Santa Fe, New Mexico, USA, August 20-26, 2018.1384Diachronic word embeddings and semantic shifts: a surveyAndrey KutuzovLilja ØvrelidTerrence Szymanski♦Erik VelldalUniversity of Oslo, Norway{andreku | liljao | erikve}@ifi.uio.no♦ANZ, Melbourne, Australiaterry.szymanski@gmail.comAbstractRecent years have witnessed a surge of publications aimed at tracing temporal changes in lexicalsemantics using distributional methods, particularly prediction-based word embedding models.However, this vein of research lacks the cohesion, common terminology and shared practices ofmore established areas of natural language processing. In this paper, we survey the current stateof academic research related to diachronic word embeddings and semantic shifts detection. Westart with discussing the notion of semantic shifts, and then continue with an overview of theexisting methods for tracing such time-related shifts with word embedding models. We proposeseveral axes along which these methods can be compared, and outline the main challenges beforethis emerging subﬁeld of NLP, as well as prospects and possible applications.1IntroductionThe meanings of words continuously change over time, reﬂecting complicated processes in language andsociety. Examples include both changes to the core meaning of words (like the word gay shifting frommeaning ‘carefree’ to ‘homosexual’ during the 20th century) and subtle shifts of cultural associations(like Iraq or Syria being associated with the concept of ‘war’ after armed conﬂicts had started in thesecountries). Studying these types of changes in meaning enables researchers to learn more about humanlanguage and to extract temporal-dependent data from texts.The availability of large corpora and the development of computational semantics have given rise toa number of research initiatives trying to capture diachronic semantic shifts in a data-driven way. Re-cently, word embeddings (Mikolov et al., 2013b) have become a widely used input representation forthis task. There are dozens of papers on the topic, mostly published after 2011 (we survey them inSection 3 and further below). However, this emerging ﬁeld is highly heterogenous. There are at leastthree different research communities interested in it: natural language processing (and computationallinguistics), information retrieval (and computer science in general), and political science. This is re-ﬂected in the terminology, which is far from being standardized. One can ﬁnd mentions of ‘temporalembeddings,’ ‘diachronic embeddings,’ ‘dynamic embeddings,’ etc., depending on the background of aparticular research group. The present survey paper attempts to describe this diversity, introduce someaxes of comparison and outline main challenges which the practitioners face. Figure 1 shows the timelineof events that inﬂuenced the research in this area: in the following sections we cover them in detail.This survey is restricted in scope to research which traces semantic shifts using distributional word em-bedding models (that is, representing lexical meaning with dense vectors produced from co-occurrencedata). We only brieﬂy mention other data-driven approaches also employed to analyze temporal-labeledcorpora (for example, topic modeling). Also, we do not cover syntactic shifts and other changes in thefunctions rather than meaning of words.The paper is structured as follows. In Section 2 we introduce the notion of ‘semantic shift’ and pro-vide some linguistic background for it. Section 3 aims to compare different approaches to the task ofThis work is licensed under a Creative Commons Attribution 4.0 International License.License details:http://creativecommons.org/licenses/by/4.0/\f138520102011201220132014201520162017Time tensor with Random IndexingGoogle Ngrams corpusWord epoch disambiguationPrediction-based models (word2vec)Word embeddings with incremental updatesModels alignmentNYT corpusCOHA corpusLaws of semantic changeLocal measures better for cultural shiftsGigaword corpusDiachronic relationsCriticism of semantic change lawsJoint learning across time spansFigure 1: Distributional models in the task of tracing diachronic semantic shifts: research timelineautomatic detection of semantic shifts: in the choice of diachronic data, evaluation strategies, methodol-ogy of extracting semantic shifts from data, and the methods to compare word vectors across time spans.Sections 4 and 5 describe two particularly interesting results of diachronic embeddings research: namely,the statistical laws of semantic change and temporal semantic relations. In Section 6 we outline possibleapplications of systems that trace semantic shifts. Section 7 presents open challenges which we believeto be most important for the ﬁeld, and in Section 8 we summarize and conclude.2The concept of semantic shiftsHuman languages change over time, due to a variety of linguistic and non-linguistic factors and at alllevels of linguistic analysis. In the ﬁeld of theoretical (diachronic) linguistics, much attention has beendevoted to expressing regularities of linguistic change. For instance, laws of phonological change havebeen formulated (e.g., Grimm’s law or the great vowel shift) to account for changes in the linguistic soundsystem. When it comes to lexical semantics, linguists have studied the evolution of word meaning overtime, describing so-called lexical semantic shifts or semantic change, which Bloomﬁeld (1933) deﬁnesas “innovations which change the lexical meaning rather than the grammatical function of a form.”Historically, much of the theoretical work on semantic shifts has been devoted to documenting andcategorizing various types of semantic shifts (Bréal, 1899; Stern, 1931; Bloomﬁeld, 1933). The cat-egorization found in Bloomﬁeld (1933) is arguably the most used and has inspired a number of morerecent studies (Blank and Koch, 1999; Geeraerts, 1997; Traugott and Dasher, 2001). Bloomﬁeld (1933)originally proposed nine classes of semantic shifts, six of which are complimentary pairs along a di-mension. For instance, the pair ‘narrowing’ – ‘broadening’ describes the observation that word meaningoften changes to become either more speciﬁc or more general, e.g. Old English mete ‘food’ becomesEnglish meat ‘edible ﬂesh,’ or that the more general English word dog is derived from Middle Englishdogge which described a dog of a particular breed. Bloomﬁeld (1933) also describes change along thespectrum from positive to negative, describing the speaker’s attitude as one of either degeneration orelevation, e.g. from Old English cniht ’boy, servant’ to the more elevated knight.The driving forces of semantic shifts are varied, but include linguistic, psychological, socioculturalor cultural/encyclopedic causes (Blank and Koch, 1999; Grzega and Schoener, 2007). Linguistic pro-cesses that cause semantic shifts generally involve the interaction between words of the vocabulary andtheir meanings. This may be illustrated by the process of ellipsis, whereby the meaning of one word istransferred to a word with which it frequently co-occurs, or by the need for discrimination of synonymscaused by lexical borrowings from other languages. Semantic shifts may be also be caused by changesin the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are natu-rally separated into two important classes: linguistic drifts (slow and regular changes in core meaning ofwords) and cultural shifts (culturally determined changes in associations of a given word). Researchersstudying semantic shifts from a computational point of view have shown the existence of this divisionempirically (Hamilton et al., 2016c). In the traditional classiﬁcation of Stern (1931), the semantic shiftcategory of substitution describes a change that has a non-linguistic cause, namely that of technologi-\f1386cal progress. This may be exempliﬁed by the word car which shifted its meaning from non-motorizedvehicles after the introduction of the automobile.The availability of large corpora have enabled the development of new methodologies for the studyof lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of thiswork is that changes in a word’s collocational patterns reﬂect changes in word meaning (Hilpert, 2008),thus providing a usage-based account of semantics (Gries, 1999). For instance, Kerremans et al. (2010)study the very recent neologism detweet, showing the development of two separate usages/meaningsfor this word (‘to delete from twitter,’ vs ‘to avoid tweeting’) based on large amounts of web-crawleddata. The usage-based view of lexical semantics aligns well with the assumptions underlying the distri-butional semantic approach (Firth, 1957) often employed in NLP . Here, the time spans studied are oftenconsiderably shorter (decades, rather than centuries) and we ﬁnd that these distributional methods seemwell suited for monitoring the gradual process of meaning change. Gulordava and Baroni (2011), forinstance, showed that distributional models capture cultural shifts, like the word sleep acquiring morenegative connotations related to sleep disorders, when comparing its 1960s contexts to its 1990s contexts.To sum up, semantic shifts are often reﬂected in large corpora through change in the context of theword which is undergoing a shift, as measured by co-occurring words. It is thus natural to try to detectsemantic shifts automatically, in a ‘data-driven’ way. This vein of research is what we cover in the presentsurvey. In the following sections, we overview the methods currently used for the automatic detection ofsemantic shifts and the recent academic achievements related to this problem.3Tracing semantic shifts distributionallyConceptually, the task of discovery of semantic shifts from data can be formulated as follows. Givencorpora [C1, C2, ...Cn] containing texts created in time periods [1, 2, ...n], the task is to locate wordswith different meaning in different time periods, or to locate the words which changed most. Otherrelated tasks are possible: discovering general trends in semantic shifts (see Section 4) or tracing thedynamics of the relationships between words (see Section 5). In the next subsections, we address severalaxes along which one can categorize the research on detecting semantic shifts with distributional models.3.1Sources of diachronic data for training and testingWhen automatically detecting semantic shifts, the types of generalizations we will be able to infer areinﬂuenced by properties of the textual data being used, such as the source of the datasets and the temporalgranularity of the data. In this subsection we discuss the data choices made by researchers (of course,not pretending to cover the whole range of the diachronic corpora used).3.1.1Training dataThe time unit (the granularity of the temporal dimension) can be chosen before slicing the text collectioninto subcorpora. Earlier works dealt mainly with long-term semantic shifts (spanning decades or evencenturies), as they are easier to trace. One of the early examples is Sagi et al. (2011) who studieddifferences between Early Middle, Late Middle and Early Modern English, using the Helsinki Corpus(Rissanen and others, 1993).The release of the Google Books Ngrams corpus1 played an important role in the development ofthe ﬁeld and spurred work on the new discipline of ‘culturomics,’ studying human culture through dig-ital media (Michel et al., 2011). Mihalcea and Nastase (2012) used this dataset to detect differencesin word usage and meaning across 50-years time spans, while Gulordava and Baroni (2011) comparedword meanings in the 1960s and in the 1990s, achieving good correlation with human judgments. Un-fortunately, Google Ngrams is inherently limited in that it does not contain full texts. However, formany cases, this corpus was enough, and its usage as the source of diachronic data continued in Mitraet al. (2014) (employing syntactic ngrams), who detected word sense changes over several different timeperiods spanning from 3 to 200 years.1https://books.google.com/ngrams\f1387In more recent work, time spans tend to decrease in size and become more granular. In general,corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corporawith longer spans are necessary for the study of linguistically motivated semantic shifts. As researchersare attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks),the granularity of time spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016)analyzed the yearly changes of words. Note that, instead of using granular ‘bins’, time can also berepresented as a continuous differentiable value (Rosenfeld and Erk, 2018).In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) usedAmazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month).Their results indicated that computational methods for the detection of semantic shifts can be robustlyapplied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, theNew-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts.The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearlysubcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shiftswas expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2), withtime slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along withthe Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the EnglishGigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conﬂicts.3.1.2Test setsDiachronic corpora are needed not only as a source of training data for developing semantic shift de-tection systems, but also as a source of test sets to evaluate such systems. In this case, however, thesituation is more complicated. Ideally, diachronic approaches should be evaluated on human-annotatedlists of semantically shifted words (ranked by the degree of the shift). However, such gold standard datais difﬁcult to obtain, even for English, let alone for other languages. General linguistics research onlanguage change like that of Traugott and Dasher (2001) and others usually contain only a small numberof hand-picked examples, which is not sufﬁcient to properly evaluate an automatic unsupervised system.Various ways of overcoming this problem have been proposed. For example, Mihalcea and Nastase(2012) evaluated the ability of a system to detect the time span that speciﬁc contexts of a word undergoinga shift belong to (word epoch disambiguation). A similar problem was offered as SemEval-2015 Task 7:‘Diachronic Text Evaluation’ (Popescu and Strapparava, 2015). Another possible evaluation method isso-called cross-time alignment, where a system has to ﬁnd equivalents for certain words in different timeperiods (for example, ‘Obama’ in 2015 corresponds to ‘Trump’ in 2017). There exist several datasetscontaining such temporal equivalents for English (Yao et al., 2018). Yet another evaluation strategy isto use the detected diachronic semantic shifts to trace or predict real-world events like armed conﬂicts(Kutuzov et al., 2017b). Unfortunately, all these evaluation methods still require the existence of largemanually annotated semantic shift datasets. The work to properly create and curate such datasets is in itsinfancy.One reported approach to avoid this requirement is borrowed from research on word sense disambigua-tion and consists of making a synthetic task by merging two real words together and then modifying thetraining and test data according to a predeﬁned sense-shifting function. Rosenfeld and Erk (2018) suc-cessfully employed this approach to evaluate their system; however, it still operates on synthetic words,limiting the ability of this evaluation scheme to measure the models’ performance with regards to realsemantic shift data. Thus, the problem of evaluating semantic shift detection approaches is far from be-ing solved, and practitioners often rely on self-created test sets, or even simply manually inspecting theresults.3.2Methodology of extracting semantic shifts from dataAfter settling on a diachronic data set to be used in the system, one has to choose the methods to analyzeit. Before the broad adoption of word embedding models, it was quite common to use change in raw2http://corpus.byu.edu/coha/\f1388word frequencies in order to trace semantic shifts or other kinds of linguistic change; see, among others,Juola (2003), Hilpert and Gries (2009), Michel et al. (2011), Lijfﬁjt et al. (2012), or Choi and Varian(2012) for frequency analysis of words in web search queries. Researchers also studied the increase ordecrease in the frequency of a word A collocating with another word B over time, and based on thisinferred changes in the meaning of A (Heyer et al., 2009).However, it is clear that semantic shifts are not always accompanied with changes in word frequency(or this connection may be very subtle and non-direct). Thus, if one were able to more directly modelword meaning, such an approach should be superior to frequency-proxied methods. A number of recentpublications have showed that distributional word representations (Turney et al., 2010; Baroni et al.,2014) provide an efﬁcient way to solve these tasks. They represent meaning with sparse or dense (em-bedding) vectors, produced from word co-occurrence counts. Although conceptually the source of thedata for these models is still word frequencies, they ‘compress’ this information into continuous lexicalrepresentations which are both efﬁcient and convenient to work with. Indeed, Kulkarni et al. (2015)explicitly demonstrated that distributional models outperform the frequency-based methods in detectingsemantic shifts. They managed to trace semantic shifts more precisely and with greater explanatorypower. One of the examples from their work is the semantic evolution of the word gay: through time, itsnearest semantic neighbors changed, manifesting the gradual move away from the sense of ‘cheerful’ tothe sense of ‘homosexual.’In fact, distributional models were being used in diachronic research long before the paper of Kulkarniet al. (2015), although there was no rigorous comparison to the frequentist methods. Already in 2009, itwas proposed that one can use distributional methods to detect semantic shifts in a quantitative way. Thepioneering work by Jurgens and Stevens (2009) described an insightful conceptualization of a sequenceof distributional model updates through time: it is effectively a Word:Semantic Vector:Time tensor, inthe sense that each word in a distributional model possesses a set of semantic vectors for each time spanwe are interested in. It paved the way for quantitatively comparing not only words with regard to theirmeaning, but also different stages in the development of word meaning over time.Jurgens and Stevens (2009) employed the Random Indexing (RI) algorithm (Kanerva et al., 2000) tocreate word vectors. Two years later, Gulordava and Baroni (2011) used explicit count-based models,consisting of sparse co-occurrence matrices weighted by Local Mutual Information, while Sagi et al.(2011) turned to Latent Semantic Analysis (Deerwester et al., 1990). In Basile et al. (2014), an extensionto RI dubbed Temporal Random Indexing (TRI) was proposed. However, no quantitative evaluationof this approach was offered (only a few hand-picked examples based on the Italian texts from theGutenberg Project), and thus it is unclear whether TRI is any better than other distributional modelsfor the task of semantic shift detection.Further on, the diversity of the employed methods started to increase. For example, Mitra et al.(2014) analyzed clusters of the word similarity graph in the subcorpora corresponding to different timeperiods. Their distributional model consisted of lexical nodes in the graphs connected with weightededges. The weights corresponded to the number of shared most salient syntactic dependency contexts,where saliency was determined by co-occurrence counts scaled by Mutual Information (MI). Importantly,they were able to detect not only the mere fact of a semantic shift, but also its type: the birth of a newsense, splitting of an old sense into several new ones, or merging of several senses into one. Thus, thiswork goes into a much less represented class of ‘ﬁne-grained’ approaches to semantic shift detection.It is also important that Mitra et al. (2014) handle natively the issue of polysemous words, putting themuch-neglected problem of word senses in the spotlight.The work of Kim et al. (2014) was seminal in the sense that it is arguably the ﬁrst one employingprediction-based word embedding models to trace diachronic semantic shifts. Particularly, they usedincremental updates (see below) and Continuous Skipgram with negative sampling (SGNS) (Mikolovet al., 2013a).3 Hamilton et al. (2016a) showed the superiority of SGNS over explicit PPMI-baseddistributional models in semantic shifts analysis, although they noted that low-rank SVD approximations(Bullinaria and Levy, 2007) can perform on par with SGNS, especially on smaller datasets. Since then,3Continuous Bag-of-Words (CBOW) from the same paper is another popular choice for learning semantic vectors.\f1389the majority of publications in the ﬁeld started using dense word representations: either in the form ofSVD-factorized PPMI matrices, or in the form of prediction-based shallow neural models like SGNS.4There are some works employing other distributional approaches to semantic shifts detection. Forinstance, there is a strong vein of research based on dynamic topic modeling (Blei and Lafferty, 2006;Wang and McCallum, 2006), which learns the evolution of topics over time. In Wijaya and Yeniterzi(2011), it helped solve a typical digital humanities task of ﬁnding traces of real-world events in thetexts. Heyer et al. (2016) employed topic analysis to trace the so-called ‘context volatility’ of words. Inthe political science, topic models are also sometimes used as proxies to social trends developing overtime: for example, Mueller and Rauh (2017) employed LDA to predict timing of civil wars and armedconﬂicts. Frermann and Lapata (2016) drew on these ideas to trace diachronic word senses development.But most scholars nowadays seem to prefer parametric distributional models, particularly prediction-based embedding algorithms like SGNS, CBOW or GloVe (Pennington et al., 2014). Following theirwidespread adoption in NLP in general, they have become the dominant representations for the analysisof diachronic semantic shifts as well.3.3Comparing vectors across timeIt is rather straightforward to train separate word embedding models using time-speciﬁc corpora con-taining texts from several different time periods. As a consequence, these models are also time-speciﬁc.However, it is not that straightforward to compare word vectors across different models.It usually does not make sense to, for example, directly calculate cosine similarities between embed-dings of one and the same word in two different models. The reason is that most modern word embeddingalgorithms are inherently stochastic and the resulting embedding sets are invariant under rotation. Thus,even when trained on the same data, separate learning runs will produce entirely different numericalvectors (though with roughly the same pairwise similarities between vectors for particular words). Thisis expressed even stronger for models trained on different corpora. It means that even if word meaning iscompletely stable, the direct cosine similarity between its vectors from different time periods can still bequite low, simply because the random initializations of the two models were different. To alleviate this,Kulkarni et al. (2015) suggested that before calculating similarities, one should ﬁrst align the models toﬁt them in one vector space, using linear transformations preserving general vector space structure. Af-ter that, cosine similarities across models become meaningful and can be used as indicators of semanticshifts. They also proposed constructing the time series of a word embedding over time, which allowsfor the detection of ‘bursts’ in its meaning with the Mean Shift model (Taylor, 2000). Notably, almostsimultaneously the idea of aligning diachronic word embedding models using a distance-preserving pro-jection technique was proposed by Zhang et al. (2015). Later, Zhang et al. (2016) expanded on this byadding the so called ‘local anchors’: that is, they used both linear projections for the whole models andsmall sets of nearest neighbors for mapping the query words to their correct temporal counterparts.Instead of aligning their diachronic models using linear transformations, Eger and Mehler (2016) com-pared word meaning using so-called ‘second-order embeddings,’ that is, the vectors of words’ similaritiesto all other words in the shared vocabulary of all models. This approach does not require any transfor-mations: basically, one simply analyzes the word’s position compared to other words. At the same time,Hamilton et al. (2016a) and Hamilton et al. (2016c) showed that these two approaches can be used simul-taneously: they employed both ‘second order embeddings’ and orthogonal Procrustes transformations toalign diachronic models.Recently, it was shown in Bamler and Mandt (2017) (‘dynamic skip-gram’ model) and Yao et al. (2018)(‘dynamic Word2Vec’ model) that it is possible to learn the word embeddings across several time periodsjointly, enforcing alignment across all of them simultaneously, and positioning all the models in the samevector space in one step. This develops the idea of model alignment even further and eliminates the needto ﬁrst learn separate embeddings for each time period, and then align subsequent model pairs. Bamlerand Mandt (2017) additionally describe two variations of their approach: a) for the cases when data slicesarrive sequentially, as in streaming applications, where one can not use future observations, and b) for4Levy and Goldberg (2014) showed that these two approaches are equivalent from the mathematical point of view.\f1390the cases when data slices are available all at once, allowing for training on the whole sequence fromthe very beginning. A similar approach is taken by Rosenfeld and Erk (2018) who train a deep neuralnetwork on word and time representations. Word vectors in this setup turn into linear transformationsapplied to a continuous time variable, and thus producing an embedding of word w at time t.Yet another way to make the models comparable is made possible by the fact that prediction-basedword embedding approaches (as well as RI) allow for incremental updates of the models with new datawithout any modiﬁcations. This is not the case for the traditional explicit count-based algorithms, whichusually require a computationally expensive dimensionality reduction step. Kim et al. (2014) proposedthe idea of incrementally updated diachronic embedding models: that is, they train a model on the year yi,and then the model for the year yi+1 is initialized with the word vectors from yi. This can be consideredas an alternative to model alignment: instead of aligning models trained from scratch on different timeperiods, one starts with training a model on the diachronically ﬁrst period, and then updates this samemodel with the data from the successive time periods, saving its state each time. Thus, all the models areinherently related to each other, which, again, makes it possible to directly calculate cosine similaritiesbetween the same word in different time period models, or at least makes the models more comparable.Several works have appeared recently which aim to address the technical issues accompanying thisapproach of incremental updating. Among others, Peng et al. (2017) described a novel method of incre-mentally learning the hierarchical softmax function for the CBOW and Continuous Skipgram algorithms.In this way, one can update word embedding models with new data and new vocabulary much more ef-ﬁciently, achieving faster training than when doing it from scratch, while at the same time preservingcomparable performance. Continuing this line of research, Kaji and Kobayashi (2017) proposed a con-ceptually similar incremental extension for negative sampling, which is a method of training examplesselection, widely used with prediction-based models as a faster replacement for hierarchical softmax.Even after the models for different time periods are made comparable in this or that way, one still hasto choose the exact method of comparing word vectors across these models. Hamilton et al. (2016a)and Hamilton et al. (2016c) made an important observation that the distinction between linguistic andcultural semantic shifts is correlated with the distinction between global and local embedding compari-son methods. The former take into account the whole model (for example, ‘second-order embeddings,’when we compare the word’s similarities to all other words in the lexicon), while the latter focus on theword’s immediate neighborhood (for example, when comparing the lists of k nearest neighbors). Theyconcluded that global measures are sensitive to regular processes of linguistic shifts, while local mea-sures are better suited to detect slight cultural shifts in word meaning. Thus, the choice of particularembedding comparison approach should depend on what type of semantic shifts one seeks to detect.4Laws of semantic changeThe use of diachronic word embeddings for studying the dynamics of word meaning has resulted inseveral hypothesized ‘laws’ of semantic change. We review some of these law-like generalizations below,before ﬁnally describing a study that questions their validity.Dubossarsky et al. (2015) experimented with K-means clustering applied to SGNS embeddings trainedfor evenly sized yearly samples for the period 1850–2009. They found that the degree of semantic changefor a given word – quantiﬁed as the change in self-similarity over time – negatively correlates with itsdistance to the centroid of its cluster. They proposed that the likelihood for semantic shift correlates withthe degree of prototypicality (the ‘law of prototypicality’ in Dubossarsky et al. (2017)).Another relevant study is reported by Eger and Mehler (2016), based on two different graph models;one being a time-series model relating embeddings across time periods to model semantic shifts and theother modeling the self-similarity of words across time. Experiments were performed with time-indexedhistorical corpora of English, German and Latin, using time-periods corresponding to decades, yearsand centuries, respectively. To enable comparison of embeddings across time, second-order embeddingsencoding similarities to other words were used, as described in 3.3, limited to the ‘core vocabulary’(words occurring at least 100 times in all time periods). Based on linear relationships observed in thegraphs, Eger and Mehler (2016) postulate two ‘laws’ of semantic change:\f13911. word vectors can be expressed as linear combinations of their neighbors in previous time periods;2. the meaning of words tend to decay linearly in time, in terms of the similarity of a word to itself;this is in line with the ‘law of differentiation’ proposed by Xu and Kemp (2015).In another study, Hamilton et al. (2016a) considered historical corpora for English, German, Frenchand Chinese, spanning 200 years and using time spans of decades. The goal was to investigate therole of frequency and polysemy with respect to semantic shifts. As in Eger and Mehler (2016), therate of semantic change was quantiﬁed by self-similarity across time-points (with words represented byProcrustes-aligned SVD embeddings). Through a regression analysis, Hamilton et al. (2016a) investi-gated how the change rates correlate with frequency and polysemy, and proposed another two ‘laws’:1. frequent words change more slowly (‘the law of conformity’);2. polysemous words (controlled for frequency) change more quickly (‘the law of innovation’).Azarbonyad et al. (2017) showed that these laws (at least the law of conformity) hold not only fordiachronic corpora, but also for other ‘viewpoints’: for example, semantic shifts across models trained ontexts produced by different political actors or written in different genres (Kutuzov et al., 2016). However,the temporal dimension allows for a view of the corpora under analysis as a sequence, making the notionof ‘semantic shift’ more meaningful.Later, Dubossarsky et al. (2017) questioned the validity of some of these proposed ‘laws’ of semanticchange. In a series of replication and control experiments, they demonstrated that some of the regularitiesobserved in previous studies are largely artifacts of the models used and frequency effects. In particular,they considered 10-year bins comprising equally sized yearly samples from Google Books 5-grams ofEnglish ﬁction for the period 1990–1999. For control experiments, they constructed two additionaldata sets; one with chronologically shufﬂed data where each bin contains data from all decades evenlydistributed, and one synchronous variant containing repeated random samples from the year 1999 alone.Any measured semantic shifts within these two alternative data sets would have to be due to randomsampling noise.Dubossarsky et al. (2017) performed experiments using raw co-occurrence counts, PPMI weightedcounts, and SVD transformations (Procrustes aligned), and conclude that the ‘laws’ proposed in previ-ous studies – that semantic change is correlated with frequency, polysemy (Hamilton et al., 2016a) andprototypicality (Dubossarsky et al., 2015) – are not valid as they are also observed in the control condi-tions. Dubossarsky et al. (2017) suggested that these spurious effects are instead due to the type of wordrepresentation used – count vectors – and that semantic shifts must be explained by a more diverse setof factors than distributional ones alone. Thus, the discussion on the existence of the ‘laws of semanticchange’ manifested by distributional trends is still open.5Diachronic semantic relationsWord embedding models are known to successfully capture complex relationships between concepts, asmanifested in the well-known word analogies task (Mikolov et al., 2013a), where a model must ‘solve’equations of the form ‘A is to B is as C is to what?’ A famous example is the distributional model captur-ing the fact that the relation between ‘man’ and ‘woman’ is the same as between ‘king’ and ‘queen’ (byadding and subtracting the corresponding word vectors). Thus, it is a natural development to investigatewhether changes in semantic relationships across time can also be traced by looking at the diachronicdevelopment of distributional models.Zhang et al. (2015) considered the temporal correspondences problem, wherein the objective is toidentify the word in a target time period which corresponds to a query term in the source time period(for example, given the query term iPod in the 2000s, the counterpart term in the 1980s time period isWalkman). This is proposed as a means to improve the results of information retrieval from documentcollections with signiﬁcant time spans. Szymanski (2017) frames this as the temporal word analogyproblem, extending the word analogies concept into the temporal dimension. This work shows that\f1392diachronic word embeddings can successfully model relations like ‘word w1 at time period tα is likeword w2 at time period tβ’. To this end, embedding models trained on different time periods are alignedusing linear transformations. Then, the temporal analogies are solved by simply ﬁnding out which wordvector in the time period tβ is the closest to the vector of w1 in the time period tα.A variation of this task was studied in Rosin et al. (2017), where the authors learn the relatednessof words over time, answering queries like ‘in which time period were the words Obama and presidentmaximally related’. This technique can be used for a more efﬁcient user query expansion in general-purpose search engines. Kutuzov et al. (2017a) modeled a different semantic relation: ‘words w1 andw2 at time period tα are in the same semantic relation as words w3 and w4 at time period tβ’. To tracethe temporal dynamics of these relations, they re-applied linear projections learned on sets of w1 and w2pairs from the model for the period tn to the model trained on the subsequent time period tn+1. This wasused to solve the task of detecting lasting or emerging armed conﬂicts and the violent groups involved inthese conﬂicts.6ApplicationsApplications of diachronic word embeddings approaches can generally be grouped into two broad cat-egories: linguistic studies which investigate the how and why of semantic shifts, and event detectionapproaches which mine text data for actionable purposes.The ﬁrst category generally involves corpora with longer time spans, since linguistic changes happenat a relatively slow pace. Some examples falling into this category include tracking semantic drift ofparticular words (Kulkarni et al., 2015) or of word sentiment (Hamilton et al., 2016b), identifying thebreakpoints between epochs (Sagi et al., 2011; Mihalcea and Nastase, 2012), studying the laws of se-mantic change at scale (Hamilton et al., 2016c) and ﬁnding different words with similar meanings atdifferent points in time (Szymanski, 2017). This has been held up as a good use case of deep learningfor research in computational linguistics (Manning, 2015), and there are opportunities for future workapplying diachronic word embeddings not only in the ﬁeld of historical linguistics, but also in relatedareas like sociolinguistics and digital humanities.The second category involves mining texts for cultural semantic shifts (usually on shorter time spans)indicating real-world events. Examples of this category are temporal information retrieval (Rosin etal., 2017), predicting civil turmoils (Kutuzov et al., 2017b; Mueller and Rauh, 2017), or tracing thepopularity of entities using norms of word vectors (Yao et al., 2018). They can potentially be employedto improve user experience in production systems or for policy-making in governmental structures.We believe that the near future will see a more diverse landscape of applications for diachronic wordembeddings, especially related to the real-time analysis of large-scale news streams. ‘Between the lines,’these data sources contain a tremendous amount of information about processes in our world, manifestedin semantic shifts of various sorts. The task of researchers is to reveal this information and make itreliable and practically useful.7Open challengesThe study of temporal aspects of semantic shifts using distributional models (including word embed-dings) is far from being a solved problem. The ﬁeld still has a considerable number of open challenges.Below we brieﬂy describe the most demanding ones.• The existing methods should be expanded to a wider scope of languages. Hamilton et al. (2016a),Kutuzov and Kuzmenko (2018) and others have started to analyze other languages, but the over-whelming majority of publications still apply only to English corpora. It might be the case that thebest methodologies are the same for different languages, but this should be shown empirically.• There is a clear need to devise algorithms that work on small datasets, as they are very common inhistorical linguistics, digital humanities, and similar disciplines.\f1393• Carefully designed and robust gold standard test sets of semantic shifts (of different kinds) shouldbe created. This is a difﬁcult task in itself, but the experience from synchronic word embeddingsevaluation (Hill et al., 2015) and other NLP areas proves that it is possible.• There is a need for rigorous formal mathematical models of diachronic embeddings. Arguably, thiswill follow the vein of research in joint learning across several time spans, started by Bamler andMandt (2017) and Yao et al. (2018), but other directions are also open.• Most current studies stop after stating the simple fact that a semantic shift has occurred. However,more detailed analysis of the nature of the shift is needed. This includes:1. Sub-classiﬁcation of types of semantic shifts (broadening, narrowing, etc). This problem wasto some degree addressed by Mitra et al. (2014), but much more work is certainly required toempirically test classiﬁcation schemes proposed in much of the theoretical work described inSection 2.2. Identifying the source of a shift (for example, linguistic or extra-linguistic causes). This cau-sation detection is closely linked to the division between linguistic drifts and cultural shifts, asproposed in Hamilton et al. (2016c).3. Quantifying the weight of senses acquired over time. Many words are polysemous, and therelative importance of senses is ﬂexible (Frermann and Lapata, 2016). The issue of handlingsenses is central for detecting semantic shifts, but most of the algorithms described in this sur-vey are not sense-aware. To address this, methods from sense embeddings research (Bartunovet al., 2016) might be employed.4. Identifying groups of words that shift together in correlated ways. Some work in this directionwas started in Dubossarsky et al. (2016), who showed that verbs change more than nouns, andnouns change more than adjectives. This is also naturally related to proving the (non-)existenceof the ‘laws of semantic change’ (see Section 4).• Last but not least, we believe that the community around diachronic word embeddings researchseverely lacks relevant forums, like topical workshops or shared tasks. Diachronic text evaluationtasks like the one at SemEval-2015 (Popescu and Strapparava, 2015) are important but not enough,since they focus on identifying the time period when a text was authored, not the process of shiftingmeanings of a word. Organizing such events can promote the ﬁeld and help address many of thechallenges described above.8SummaryWe have presented an outline of the current research related to computational detection of semantic shiftsusing diachronic (temporal) word embeddings. We covered the linguistic nature of semantic shifts, thetypical sources of diachronic data and the distributional approaches used to model it, from frequentistmethods to contemporary prediction-based models. To sum up, Figure 1 shows the timeline of eventsthat have been inﬂuential in the development of research in this area: introducing concepts, usage ofcorpora and important ﬁndings.This emerging ﬁeld is still relatively new, and although recent years has seen a string of signiﬁcantdiscoveries and academic interchange, much of the research still appears slightly fragmented, not leastdue to the lack of dedicated venues like workshops, special issues, or shared tasks. We hope that thissurvey will be useful to those who want to understand how this ﬁeld has developed, and gain an overviewof what deﬁnes the current state-of-the-art and what challenges lie ahead.AcknowledgementsWe thank William Hamilton, Haim Dubossarsky and Chris Biemann for their helpful feedback duringthe preparation of this survey. All possible mistakes remain the sole responsibility of the authors.\f1394ReferencesHosein Azarbonyad, Mostafa Dehghani, Kaspar Beelen, Alexandra Arkut, Maarten Marx, and Jaap Kamps. 2017.Words are malleable: Computing semantic shifts in political and media discourse. In Proceedings of the ACMon Conference on Information and Knowledge Management, pages 1509–1518, Singapore.Robert Bamler and Stephan Mandt. 2017. Dynamic word embeddings. In Proceedings of the InternationalConference on Machine Learning, pages 380–389, Sydney, Australia.Marco Baroni, Georgiana Dinu, and Germán Kruszewski. 2014. Don’t count, predict! A systematic comparisonof context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics, volume 1, pages 238–247, Baltimore, USA.Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, and Dmitry Vetrov. 2016. Breaking sticks and ambiguitieswith adaptive skip-gram. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence andStatistics, pages 130–138, Cadiz, Spain.Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. 2014. Analysing word meaning over time by exploit-ing temporal random indexing. In Proceedings of the First Italian Conference on Computational Linguistics,pages 38–42, Turin, Italy.Andreas Blank and Peter Koch. 1999. Historical semantics and cognition. Walter de Gruyter.David M Blei and John D Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd InternationalConference on Machine learning, pages 113–120, Pittsburgh, USA.Leonard Bloomﬁeld. 1933. Language. Allen & Unwin.Michel Bréal. 1899. Essai de sémantique. Hachette, Paris.John A Bullinaria and Joseph P Levy. 2007. Extracting semantic representations from word co-occurrence statis-tics: A computational study. Behavior research methods, 39(3):510–526.Hyunyoung Choi and Hal Varian. 2012. Predicting the present with Google trends. Economic Record, 88(s1):2–9.Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. 1990. Index-ing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407.Haim Dubossarsky, Yulia Tsvetkov, Chris Dyer, and Eitan Grossman. 2015. A bottom up approach to categorymapping and meaning change. In Proceedings of the NetWordS 2015 Word Knowledge and Word Usage, pages66–70, Pisa, Italy.Haim Dubossarsky, Daphna Weinshall, and Eitan Grossman. 2016. Verbs change more than nouns: a bottom-upcomputational approach to semantic change. Lingue e linguaggio, 15(1):7–28.Haim Dubossarsky, Daphna Weinshall, and Eitan Grossman. 2017. Outta control: Laws of semantic change andinherent biases in word representation models. In Proceedings of the 2017 Conference on Empirical Methodsin Natural Language Processing, pages 1147–1156, Copenhagen, Denmark.Steffen Eger and Alexander Mehler. 2016. On the linearity of semantic change: Investigating meaning variationvia dynamic graph models. In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics, pages 52–58, Berlin, Germany.John Firth. 1957. A synopsis of linguistic theory, 1930-1955. Blackwell.Lea Frermann and Mirella Lapata. 2016. A bayesian model of diachronic meaning change. Transactions of theAssociation of Computational Linguistics, 4:31–45.Dirk Geeraerts. 1997. Diachronic prototype semantics: A contribution to historical lexicology. Clarendon Press,Oxford.Stefan Th. Gries. 1999. Particle movement: a cognitive and functional approach. Cognitive Linguistics, 10:105–145.Joachim Grzega and Marion Schoener. 2007. English and general historical lexicology. Eichstätt-Ingolstadt:Katholische Universität.\f1395Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of semanticchange in the Google Books Ngram corpus. In Proceedings of the GEMS 2011 Workshop on GeometricalModels of Natural Language Semantics, pages 67–71, Edinburgh, UK.L. William Hamilton, Jure Leskovec, and Dan Jurafsky. 2016a. Diachronic word embeddings reveal statisticallaws of semantic change. In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics, pages 1489–1501, Berlin, Germany.William L. Hamilton, Kevin Clark, Jure Leskovec, and Dan Jurafsky. 2016b. Inducing domain-speciﬁc sentimentlexicons from unlabeled corpora. In Proceedings of the Conference on Empirical Methods in Natural LanguageProcessing, pages 595–605, Austin, Texas.William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016c. Cultural shift or linguistic drift? Comparingtwo computational measures of semantic change. In Proceedings of the Conference on Empirical Methods inNatural Language Processing, pages 2116–2121, Austin, Texas.Gerhard Heyer, Florian Holz, and Sven Teresniak. 2009. Change of topics over time – tracking topics by theirchange of meaning. In Proceeding of the International Conference on Knowledge Discovery and InformationRetrieval, pages 223–228, Madeira, Portugal.Gerhard Heyer, Cathleen Kantner, Andreas Niekler, Max Overbeck, and Gregor Wiedemann. 2016. Modelingthe dynamics of domain speciﬁc terminology in diachronic corpora. In Proceedings of the 12th Internationalconference on Terminology and Knowledge Engineering (TKE 2016).Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine)similarity estimation. Computational Linguistics, 41(4):665–695.Martin Hilpert and Stefan Th. Gries. 2009. Assessing frequency changes in multistage diachronic corpora: Appli-cations for historical corpus linguistics and the study of language acquisition. Literary and Linguistic Comput-ing, 24(4):385–401.M. Hilpert. 2008. Germanic future constructions: A usage-based approach to language change. Benjamins,Amsterdam, Netherlands.Patrick Juola. 2003. The time course of language change. Computers and the Humanities, 37(1):77–96.David Jurgens and Keith Stevens. 2009. Event detection in blogs using Temporal Random Indexing.Nobuhiro Kaji and Hayato Kobayashi. 2017. Incremental skip-gram model with negative sampling. In Proceed-ings of the Conference on Empirical Methods in Natural Language Processing, pages 363–371, Copenhagen,Denmark.Pentti Kanerva, Jan Kristofersson, and Anders Holst. 2000. Random indexing of text samples for latent semanticanalysis. In Proceedings of the 22nd annual conference of the cognitive science society, volume 1036, pages103–106, Mahwah, USA.D. Kerremans, S. Stegmayr, and H.-J. Schmid. 2010. The neocrawler: Identifying and retrieving neologismsfrom the internet and monitoring ongoing change. In K. Allan and J. A. Robinson, editors, Current methods inhistorical semantics, pages 130–160. De Gruyter Mouton.Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, and Slav Petrov. 2014. Temporal analysis of languagethrough neural language models. In Proceedings of the 52nd Annual Meeting of the Association for Computa-tional Linguistics, pages 61–65, Baltimore, USA.Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2015. Statistically signiﬁcant detection oflinguistic change. In Proceedings of the 24th International Conference on World Wide Web, pages 625–635,Florence, Italy.Andrey Kutuzov and Elizaveta Kuzmenko. 2018. Two centuries in two thousand words: Neural embedding modelsin detecting diachronic lexical changes. Quantitative Approaches to the Russian Language, pages 95–112.Andrey Kutuzov, Elizaveta Kuzmenko, and Anna Marakasova. 2016. Exploration of register-dependent lexicalsemantics using word embeddings. In Proceedings of the Workshop on Language Technology Resources andTools for Digital Humanities (LT4DH), pages 26–34, Osaka, Japan.Andrey Kutuzov, Erik Velldal, and Lilja Øvrelid.2017a.Temporal dynamics of semantic relations in wordembeddings: an application to predicting armed conﬂict participants. In Proceedings of the Conference onEmpirical Methods in Natural Language Processing, pages 1824–1829, Copenhagen, Denmark.\f1396Andrey Kutuzov, Erik Velldal, and Lilja Øvrelid. 2017b. Tracing armed conﬂicts with diachronic word embeddingmodels. In Proceedings of the Events and Stories in the News Workshop at ACL 2017, pages 31–36, Vancouver,Canada.Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In Proceedingsof the 27th International Conference on Neural Information Processing Systems, pages 2177–2185, Montreal,Canada.Xuanyi Liao and Guang Cheng. 2016. Analysing the semantic change based on word embedding. In NaturalLanguage Understanding and Intelligent Applications, pages 213–223. Springer International Publishing.Jefrey Lijfﬁjt, Tanja Säily, and Terttu Nevalainen. 2012. CEECing the baseline: Lexical stability and signiﬁcantchange in a historical corpus. In Studies in Variation, Contacts and Change in English, volume 10. ResearchUnit for Variation, Contacts and Change in English (VARIENG).Christopher D. Manning.2015.Computational linguistics and deep learning.Computational Linguistics,41(4):701–707.Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, Joseph P. Pickett,Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and Erez LiebermanAiden. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176–182.Rada Mihalcea and Vivi Nastase. 2012. Word epoch disambiguation: Finding how words change over time. InProceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 259–263, JejuIsland, Korea.Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efﬁcient estimation of word representationsin vector space. arXiv preprint arXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations ofwords and phrases and their compositionality. Advances in Neural Information Processing Systems, 26:3111–3119.Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann, Animesh Mukherjee, and Pawan Goyal. 2014. That’ssick dude!: Automatic identiﬁcation of word sense change across different timescales. In Proceedings of the52nd Annual Meeting of the Association for Computational Linguistics, pages 1020–1029, Baltimore, Maryland.Hannes Mueller and Christofer Rauh. 2017. Reading between the lines: Prediction of political violence usingnewspaper text. American Political Science Review, page 1–18.Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword Fifth EditionLDC2011T07. Technical report, Technical Report. Linguistic Data Consortium, Philadelphia.Hao Peng, Jianxin Li, Yangqiu Song, and Yaopeng Liu. 2017. Incrementally learning the hierarchical softmaxfunction for neural language models. In Proceedings of the 31st AAAI Conference on Artiﬁcial Intelligence,pages 3267–327, San Francisco, California USA.Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word repre-sentation. In Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing, pages1532–1543, Doha, Qatar.Octavian Popescu and Carlo Strapparava. 2015. SemEval 2015, task 7: Diachronic text evaluation. In Proceedingsof the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 870–878, Denver, Colorado.Matti Rissanen et al. 1993. The helsinki corpus of english texts. Kyttö et. al, pages 73–81.Alex Rosenfeld and Katrin Erk. 2018. Deep neural models of semantic shift. In Proceedings of the 2018 Con-ference of the North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies, pages 474–484, New Orleans, Louisiana, USA.Guy D. Rosin, Eytan Adar, and Kira Radinsky. 2017. Learning word relatedness over time. In Proceedings ofthe 2017 Conference on Empirical Methods in Natural Language Processing, pages 1179–1189, Copenhagen,Denmark.Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2011. Tracing semantic change with latent semantic analysis.Current methods in historical semantics, pages 161–183.\f1397Evan Sandhaus. 2008. The New York Times annotated corpus overview. Linguistic Data Consortium, Philadel-phia, 6(12):e26752.Gustaf Stern. 1931. Meaning and change of meaning; with special reference to the English language. Wettergren& Kerbers.Terrence Szymanski. 2017. Temporal word analogies: Identifying lexical replacement with diachronic wordembeddings. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,pages 448–453, Vancouver, Canada.Wayne A Taylor. 2000. Change-point analysis: a powerful new tool for detecting changes.Elizabeth Closs Traugott and Richard B Dasher. 2001. Regularity in semantic change. Cambridge UniversityPress.Elizabeth Traugott. 2017. Semantic change. Oxford Research Encyclopedias: Linguistics.Peter Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journalof artiﬁcial intelligence research, 37(1):141–188.Xuerui Wang and Andrew McCallum. 2006. Topics over time: a non-markov continuous-time model of topicaltrends. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and datamining, pages 424–433, Philadelphia, PA, USA.Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Understanding semantic change of words over centuries. InProceedings of the 2011 international workshop on Detecting and Exploiting Cultural diversity on the socialweb, pages 35–40.Yang Xu and Charles Kemp. 2015. A computational evaluation of two laws of semantic change. In Proceedingsof the 37th Annual Meeting of the Cognitive Science Society, Austin, TX, USA.Zijun Yao, Yifan Sun, Weicong Ding, Nikhil Rao, and Hui Xiong. 2018. Dynamic word embeddings for evolvingsemantic discovery. In Proceedings of the Eleventh ACM International Conference on Web Search and DataMining, pages 673–681, Marina Del Rey, CA, USA.Yating Zhang, Adam Jatowt, Sourav Bhowmick, and Katsumi Tanaka.2015.Omnia mutantur, nihil interit:Connecting past with present by ﬁnding corresponding terms across time. In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguistics and the 7th International Joint Conference on NaturalLanguage Processing, pages 645–655, Beijing, China.Yating Zhang, Adam Jatowt, Sourav S. Bhowmick, and Katsumi Tanaka. 2016. The past is not a foreign country:Detecting semantically similar terms across time. IEEE Transactions on Knowledge and Data Engineering,28(10):2793–2807, October.",
        "answer": "The document does not contain section 4.2.",
        "space_key_index": 3,
        "metadata": {
            "format": "PDF 1.3",
            "title": "Diachronic word embeddings and semantic shifts: a survey",
            "author": "Andrey Kutuzov ; Lilja Øvrelid ; Terrence Szymanski ; Erik Velldal",
            "subject": "C18-1 2018",
            "keywords": "",
            "creator": "TeX",
            "producer": "pdfTeX-1.40.16",
            "creationDate": "D:20180609221548+02'00'",
            "modDate": "D:20180609221548+02'00'",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "On which platform were the annotations for the BoolQ dataset collected?",
        "context": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443–4458July 5 - 10, 2020. c⃝2020 Association for Computational Linguistics4443ERASER: A Benchmark to Evaluate Rationalized NLP ModelsJay DeYoung⋆Ψ, Sarthak Jain⋆Ψ, Nazneen Fatema Rajani⋆Φ, Eric LehmanΨ,Caiming XiongΦ, Richard SocherΦ, and Byron C. WallaceΨ⋆Equal contribution.ΨKhoury College of Computer Sciences, Northeastern UniversityΦSalesforce Research, Palo Alto, CA, 94301AbstractState-of-the-art models in NLP are now pre-dominantly based on deep neural networksthat are opaque in terms of how they cometo make predictions.This limitation hasincreased interest in designing more inter-pretable deep models for NLP that reveal the‘reasoning’ behind model outputs. But workin this direction has been conducted on dif-ferent datasets and tasks with correspondinglyunique aims and metrics; this makes it difﬁcultto track progress. We propose the EvaluatingRationales And Simple English Reasoning(ERASER) benchmark to advance researchon interpretable models in NLP. This bench-mark comprises multiple datasets and tasks forwhich human annotations of “rationales” (sup-porting evidence) have been collected. We pro-pose several metrics that aim to capture howwell the rationales provided by models alignwith human rationales, and also how faithfulthese rationales are (i.e., the degree to whichprovided rationales inﬂuenced the correspond-ing predictions). Our hope is that releasing thisbenchmark facilitates progress on designingmore interpretable NLP systems. The bench-mark, code, and documentation are availableat https://www.eraserbenchmark.com/1IntroductionInterest has recently grown in designing NLP sys-tems that can reveal why models make speciﬁcpredictions. But work in this direction has beenconducted on different datasets and using differentmetrics to quantify performance; this has made itdifﬁcult to compare methods and track progress.We aim to address this issue by releasing a stan-dardized benchmark of datasets — repurposed andaugmented from pre-existing corpora, spanning arange of NLP tasks — and associated metrics formeasuring different properties of rationales. We re-fer to this as the Evaluating Rationales And SimpleEnglish Reasoning (ERASER) benchmark.Commonsense Explanations (CoS-E)Where do you ﬁnd the most amount of leafs?(a) Compost pile  (b) Flowers  (c) Forest  (d) Field  (e) GroundMovie ReviewsIn this movie, … Plots to take over the world. The acting is great! The soundtrack is run-of-the-mill, but the action more than makes up for it(a) Positive  (b) NegativeEvidence InferenceArticle Patients for this trial were recruited … Compared with 0.9% saline, 120 mg of inhaled nebulized furosemide had no eﬀect on breathlessness during exercise. (a) Sig. decreased  (b) No sig. diﬀerence (c) Sig. increasedPrompt With respect to breathlessness, what is the reported diﬀerence between patients receiving placebo and those receiving furosemide?e-SNLIH A man in an orange vest leans over a pickup truckP A man is touching a truck (a) Entailment  (b) Contradiction  (c) NeutralFigure 1: Examples of instances, labels, and rationalesillustrative of four (out of seven) datasets included inERASER. The ‘erased’ snippets are rationales.In curating and releasing ERASER we take in-spiration from the stickiness of the GLUE (Wanget al., 2019b) and SuperGLUE (Wang et al., 2019a)benchmarks for evaluating progress in natural lan-guage understanding tasks, which have driven rapidprogress on models for general language repre-sentation learning. We believe the still somewhatnascent subﬁeld of interpretable NLP stands to ben-eﬁt similarly from an analogous collection of stan-dardized datasets and tasks; we hope these willaid the design of standardized metrics to measuredifferent properties of ‘interpretability’, and wepropose a set of such metrics as a starting point.Interpretability is a broad topic with many possi-ble realizations (Doshi-Velez and Kim, 2017; Lip-ton, 2016). In ERASER we focus speciﬁcally onrationales, i.e., snippets that support outputs. Alldatasets in ERASER include such rationales, ex-plicitly marked by human annotators. By deﬁnition,rationales should be sufﬁcient to make predictions,\f4444but they may not be comprehensive. Therefore, forsome datasets, we have also collected comprehen-sive rationales (in which all evidence supportingan output has been marked) on test instances.The ‘quality’ of extracted rationales will dependon their intended use. Therefore, we propose aninitial set of metrics to evaluate rationales thatare meant to measure different varieties of ‘inter-pretability’. Broadly, this includes measures ofagreement with human-provided rationales, and as-sessments of faithfulness. The latter aim to capturethe extent to which rationales provided by a modelin fact informed its predictions. We believe theseprovide a reasonable start, but view the problem ofdesigning metrics for evaluating rationales — espe-cially for measuring faithfulness — as a topic forfurther research that ERASER can facilitate. Andwhile we will provide a ‘leaderboard’, this is betterviewed as a ‘results board’; we do not privilegeany one metric. Instead, ERASER permits compar-ison between models that provide rationales withrespect to different criteria of interest.We implement baseline models and report theirperformance across the corpora in ERASER. Weﬁnd that no single ‘off-the-shelf’ architecture isreadily adaptable to datasets with very differentinstance lengths and associated rationale snippets(Section 3). This highlights a need for new modelsthat can consume potentially lengthy inputs andadaptively provide rationales at a task-appropriatelevel of granularity. ERASER provides a resourceto develop such models.In sum, we introduce the ERASER benchmark(www.eraserbenchmark.com), a uniﬁed set of di-verse NLP datasets (these are repurposed and aug-mented from existing corpora,1 including senti-ment analysis, Natural Language Inference, andQA tasks, among others) in a standardized for-mat featuring human rationales for decisions, alongwith starter code and tools, baseline models, andstandardized (initial) metrics for rationales.2Related WorkInterpretability in NLP is a large, fast-growingarea; we do not attempt to provide a comprehensiveoverview here. Instead we focus on directions par-ticularly relevant to ERASER, i.e., prior work onmodels that provide rationales for their predictions.Learning to explain. In ERASER we assume that1We ask users of the benchmark to cite all original papers,and provide a BibTeX entry for doing so on the website.rationales (marked by humans) are provided duringtraining. However, such direct supervision will notalways be available, motivating work on methodsthat can explain (or “rationalize”) model predic-tions using only instance-level supervision.In the context of modern neural models for textclassiﬁcation, one might use variants of attention(Bahdanau et al., 2015) to extract rationales. At-tention mechanisms learn to assign soft weights to(usually contextualized) token representations, andso one can extract highly weighted tokens as ratio-nales. However, attention weights do not in gen-eral provide faithful explanations for predictions(Jain and Wallace, 2019; Serrano and Smith, 2019;Wiegreffe and Pinter, 2019; Zhong et al., 2019;Pruthi et al., 2020; Brunner et al., 2020; Moradiet al., 2019; Vashishth et al., 2019). This likelyowes to encoders entangling inputs, complicatingthe interpretation of attention weights on inputsover contextualized representations of the same.2By contrast, hard attention mechanisms dis-cretely extract snippets from the input to pass to theclassiﬁer, by construction providing faithful expla-nations. Recent work has proposed hard attentionmechanisms as a means of providing explanations.Lei et al. (2016) proposed instantiating two modelswith their own parameters; one to extract rationales,and one that consumes these to make a prediction.They trained these models jointly via REINFORCE(Williams, 1992) style optimization.Recently, Jain et al. (2020) proposed a variantof this two-model setup that uses heuristic featurescores to derive pseudo-labels on tokens compris-ing rationales; one model can then be used to per-form hard extraction in this way, while a second(independent) model can make predictions on thebasis of these. Elsewhere, Chang et al. (2019)introduced the notion of classwise rationales thatexplains support for different output classes usinga game theoretic framework. Finally, other recentwork has proposed using a differentiable binarymask over inputs, which also avoids recourse toREINFORCE (Bastings et al., 2019).Post-hoc explanation. Another strand of inter-pretability work considers post-hoc explanationmethods, which seek to explain why a model madea speciﬁc prediction for a given input. Commonly2Interestingly, Zhong et al. (2019) ﬁnd that attention some-times provides plausible but not faithful rationales. Elsewhere,Pruthi et al. (2020) show that one can easily learn to deceivevia attention weights. These ﬁndings highlight that one shouldbe mindful of the criteria one wants rationales to fulﬁll.\f4445these take the form of token-level importancescores. Gradient-based explanations are a standardexample (Sundararajan et al., 2017; Smilkov et al.,2017). These enjoy a clear semantics (describinghow perturbing inputs locally affects outputs), butmay nonetheless exhibit counterintuitive behaviors(Feng et al., 2018).Gradients of course assume model differentia-bility. Other methods do not require any modelproperties. Examples include LIME (Ribeiro et al.,2016) and Alvarez-Melis and Jaakkola (2017);these methods approximate model behavior lo-cally by having it repeatedly make predictions overperturbed inputs and ﬁtting a simple, explainablemodel over the outputs.Acquiring rationales. Aside from interpretabilityconsiderations, collecting rationales from annota-tors may afford greater efﬁciency in terms of modelperformance realized given a ﬁxed amount of anno-tator effort (Zaidan and Eisner, 2008). In particular,recent work by McDonnell et al. (2017, 2016) hasobserved that at least for some tasks, asking anno-tators to provide rationales justifying their catego-rizations does not impose much additional effort.Combining rationale annotation with active learn-ing (Settles, 2012) is another promising direction(Wallace et al., 2010; Sharma et al., 2015).Learning from rationales. Work on learning fromrationales marked by annotators for text classiﬁca-tion dates back over a decade (Zaidan et al., 2007).Earlier efforts proposed extending standard dis-criminative models like Support Vector Machines(SVMs) with regularization terms that penalizedparameter estimates which disagreed with providedrationales (Zaidan et al., 2007; Small et al., 2011).Other efforts have attempted to specify generativemodels of rationales (Zaidan and Eisner, 2008).More recent work has aimed to exploit ratio-nales in training neural text classiﬁers. Zhang et al.(2016) proposed a rationale-augmented Convolu-tional Neural Network (CNN) for text classiﬁca-tion, explicitly trained to identify sentences support-ing categorizations. Strout et al. (2019) showed thatproviding this model with rationales during train-ing yields predicted rationales that are preferredby humans (compared to rationales produced with-out explicit supervision). Other work has proposed‘pipeline’ approaches in which independent mod-els are trained to perform rationale extraction andclassiﬁcation on the basis of these, respectively(Lehman et al., 2019; Chen et al., 2019), assumingNameSize (train/dev/test)TokensComp?Evidence Inference7958 / 972 / 9594761◇BoolQ6363 / 1491 / 28173583◇Movie Reviews1600 / 200 / 200774◆FEVER97957 / 6122 / 6111327!MultiRC24029 / 3214 / 4848303!CoS-E8733 / 1092 / 109228!e-SNLI911938 / 16449 / 1642916!Table 1: Overview of datasets in the ERASER bench-mark. Tokens is the average number of tokens in eachdocument. Comprehensive rationales mean that all sup-porting evidence is marked; !denotes cases where thisis (more or less) true by default; ◇, ◆are datasets forwhich we have collected comprehensive rationales foreither a subset or all of the test datasets, respectively.Additional information can be found in Appendix A..explicit training data is available for the former.Rajani et al. (2019) ﬁne-tuned a Transformer-based language model (Radford et al., 2018) onfree-text rationales provided by humans, with anobjective of generating open-ended explanations toimprove performance on downstream tasks.Evaluating rationales. Work on evaluating ratio-nales has often compared these to human judg-ments (Strout et al., 2019; Doshi-Velez and Kim,2017), or elicited other human evaluations of ex-planations (Ribeiro et al., 2016; Lundberg and Lee,2017; Nguyen, 2018). There has also been work onvisual evaluations of saliency maps (Li et al., 2016;Ding et al., 2017; Sundararajan et al., 2017).Measuring agreement between extracted andhuman rationales (or collecting subjective assess-ments of them) assesses the plausibility of ratio-nales, but such approaches do not establish whetherthe model actually relied on these particular ratio-nales to make a prediction. We refer to rationalesthat correspond to the inputs most relied upon tocome to a disposition as faithful.Most automatic evaluations of faithfulness mea-sure the impact of perturbing or erasing words ortokens identiﬁed as important on model output (Ar-ras et al., 2017; Montavon et al., 2017; Serrano andSmith, 2019; Samek et al., 2016; Jain and Wallace,2019). We build upon these methods in Section4. Finally, we note that a recent article urges thecommunity to evaluate faithfulness on a continuousscale of acceptability, rather than viewing this as abinary proposition (Jacovi and Goldberg, 2020).3Datasets in ERASERFor all datasets in ERASER we distribute both ref-erence labels and rationales marked by humansas supporting these in a standardized format. We\f4446delineate train, validation, and test splits for allcorpora (see Appendix A for processing details).We ensure that these splits comprise disjoint setsof source documents to avoid contamination.3 Wehave made the decision to distribute the test setspublicly,4 in part because we do not view the ‘cor-rect’ metrics to use as settled. We plan to acquireadditional human annotations on held-out portionsof some of the included corpora so as to offer hid-den test set evaluation opportunities in the future.Evidence inference (Lehman et al., 2019).Adataset of full-text articles describing randomizedcontrolled trials (RCTs).The task is to inferwhether a given intervention is reported to eithersigniﬁcantly increase, signiﬁcantly decrease, orhave no signiﬁcant effect on a speciﬁed outcome, ascompared to a comparator of interest. Rationaleshave been marked as supporting these inferences.As the original annotations are not necessarily ex-haustive, we collected exhaustive rationale annota-tions on a subset of the validation and test data.5BoolQ (Clark et al., 2019). This corpus consistsof passages selected from Wikipedia, and yes/noquestions generated from these passages. As theoriginal Wikipedia article versions used were notmaintained, we have made a best-effort attempt torecover these, and then ﬁnd within them the pas-sages answering the corresponding questions. Forpublic release, we acquired comprehensive annota-tions on a subset of documents in our test set.5Movie Reviews (Zaidan and Eisner, 2008). In-cludes positive/negative sentiment labels on moviereviews. Original rationale annotations were notnecessarily comprehensive; we thus collected com-prehensive rationales on the ﬁnal two folds of theoriginal dataset (Pang and Lee, 2004).5 In contrastto most other datasets, the rationale annotationshere are span level as opposed to sentence level.FEVER (Thorne et al., 2018). Short for Fact Ex-traction and VERiﬁcation; entails verifying claimsfrom textual sources. Speciﬁcally, each claim is tobe classiﬁed as supported, refuted or not enoughinformation with reference to a collection of source3Except for BoolQ, wherein source documents in the orig-inal train and validation set were not disjoint and we preservethis structure in our dataset. Questions, of course, are disjoint.4Consequently, for datasets that have been part of previ-ous benchmarks with other aims (namely, GLUE/superGLUE)but which we have re-purposed for work on rationales inERASER, e.g., BoolQ (Clark et al., 2019), we have carved outfor release test sets from the original validation sets.5Annotation details are in Appendix B.texts. We take a subset of this dataset, includingonly supported and refuted claims.MultiRC (Khashabi et al., 2018). A reading com-prehension dataset composed of questions withmultiple correct answers that by construction de-pend on information from multiple sentences. Hereeach rationale is associated with a question, whileanswers are independent of one another. We con-vert each rationale/question/answer triplet into aninstance within our dataset. Each answer candidatethen has a label of True or False.Commonsense Explanations (CoS-E) (Rajaniet al., 2019).This corpus comprises multiple-choice questions and answers from (Talmor et al.,2019) along with supporting rationales. The ratio-nales in this case come in the form both of high-lighted (extracted) supporting snippets and free-text, open-ended descriptions of reasoning. Givenour focus on extractive rationales, ERASER in-cludes only the former for now. Following Talmoret al. (2019), we repartition the training and valida-tion sets to provide a canonical test split.e-SNLI (Camburu et al., 2018). This dataset aug-ments the SNLI corpus (Bowman et al., 2015) withrationales marked in the premise and/or hypothesis(and natural language explanations, which we donot use). For entailment pairs, annotators were re-quired to highlight at least one word in the premise.For contradiction pairs, annotators had to highlightat least one word in both the premise and the hy-pothesis; for neutral pairs, they were only allowedto highlight words in the hypothesis.Human Agreement We report human agreementover extracted rationales for multiple annotatorsand documents in Table 2. All datasets have a highCohen κ (Cohen, 1960); with substantial or betteragreement.4MetricsIn ERASER models are evaluated both for theirpredictive performance and with respect to the ra-tionales that they extract. For the former, we relyon the established metrics for the respective tasks.Here we describe the metrics we propose to eval-uate the quality of extracted rationales. We donot claim that these are necessarily the best met-rics for evaluating rationales, however. Indeed, wehope the release of ERASER will spur additionalresearch into how best to measure the quality ofmodel explanations in the context of NLP.\f4447DatasetCohen κF1PR#Annotators/doc#DocumentsEvidence Inference------BoolQ0.618 ± 0.1940.617 ± 0.2270.647 ± 0.2600.726 ± 0.2173199Movie Reviews0.712 ± 0.1350.799 ± 0.1380.693 ± 0.1530.989 ± 0.102296FEVER0.854 ± 0.1960.871 ± 0.1970.931 ± 0.2050.855 ± 0.198224MultiRC0.728 ± 0.2680.749 ± 0.2650.695 ± 0.2840.910 ± 0.259299CoS-E0.619 ± 0.3080.654 ± 0.3170.626 ± 0.3190.792 ± 0.3712100e-SNLI0.743 ± 0.1620.799 ± 0.1300.812 ± 0.1540.853 ± 0.12439807Table 2: Human agreement with respect to rationales. For Movie Reviews and BoolQ we calculate the meanagreement of individual annotators with the majority vote per token, over the two-three annotators we hired viaUpwork and Amazon Turk, respectively. The e-SNLI dataset already comprised three annotators; for this wecalculate mean agreement between individuals and the majority. For CoS-E, MultiRC, and FEVER, members ofour team annotated a subset to use a comparison to the (majority of, where appropriate) existing rationales. Wecollected comprehensive rationales for Evidence Inference from Medical Doctors; as they have a high amount ofexpertise, we would expect agreement to be high, but have not collected redundant comprehensive annotations.4.1Agreement with human rationalesThe simplest means of evaluating extracted ratio-nales is to measure how well they agree with thosemarked by humans. We consider two classes ofmetrics, appropriate for models that perform dis-crete and ‘soft’ selection, respectively.For the discrete case, measuring exact matchesbetween predicted and reference rationales is likelytoo harsh.6 We thus consider more relaxed mea-sures.These include Intersection-Over-Union(IOU), borrowed from computer vision (Evering-ham et al., 2010), which permits credit assignmentfor partial matches. We deﬁne IOU on a token level:for two spans, it is the size of the overlap of thetokens they cover divided by the size of their union.We count a prediction as a match if it overlaps withany of the ground truth rationales by more thansome threshold (here, 0.5). We use these partialmatches to calculate an F1 score. We also measuretoken-level precision and recall, and use these toderive token-level F1 scores.Metrics for continuous or soft token scoringmodels consider token rankings, rewarding modelsfor assigning higher scores to marked tokens. Inparticular, we take the Area Under the Precision-Recall curve (AUPRC) constructed by sweeping athreshold over token scores. We deﬁne additionalmetrics for soft scoring models below.In general, the rationales we have for tasks aresufﬁcient to make judgments, but not necessarilycomprehensive. However, for some datasets wehave explicitly collected comprehensive rationalesfor at least a subset of the test set. Therefore, onthese datasets recall evaluates comprehensivenessdirectly (it does so only noisily on other datasets).6Consider that an extra token destroys the match but notusually the meaningWe highlight which corpora contain comprehensiverationales in the test set in Table 3.4.2Measuring faithfulnessAs discussed above, a model may provide ratio-nales that are plausible (agreeable to humans) butthat it did not rely on for its output. In many set-tings one may want rationales that actually explainmodel predictions, i.e., rationales extracted for aninstance in this case ought to have meaningfully in-ﬂuenced its prediction for the same. We call thesefaithful rationales. How best to measure rationalefaithfulness is an open question. In this ﬁrst versionof ERASER we propose simple metrics motivatedby prior work (Zaidan et al., 2007; Yu et al., 2019).In particular, following Yu et al. (2019) we deﬁnemetrics intended to measure the comprehensiveness(were all features needed to make a prediction se-lected?) and sufﬁciency (do the extracted rationalescontain enough signal to come to a disposition?) ofrationales, respectively.Comprehensiveness.To calculate rationalecomprehensiveness we create contrast exam-ples (Zaidan et al., 2007): We construct a con-trast example for xi, ˜xi, which is xi with the pre-dicted rationales ri removed. Assuming a classiﬁ-cation setting, let m(xi)j be the original predictionprovided by a model m for the predicted class j.Then we consider the predicted probability fromthe model for the same class once the supportingrationales are stripped. Intuitively, the model oughtto be less conﬁdent in its prediction once rationalesare removed from xi. We can measure this as:comprehensiveness = m(xi)j −m(xi/ri)j(1)A high score here implies that the rationales wereindeed inﬂuential in the prediction, while a lowscore suggests that they were not. A negative value\f4448Where do you ﬁnd the most amount of leafs?Where do you ﬁnd the most amount of leafs?(a) Compost pile (b) Flowers(c) Forest(d) Field(e) Ground(a) Compost pile (b) Flowers(c) Forest(d) Field(e) Ground……ˆp(Forest|xi)Where do you ﬁnd the most amount of leafs?(a) Compost pile (b) Flowers(c) Forest(d) Field(e) Ground…ComprehensivenessSuﬃency˜xixiriFigure 2: Illustration of faithfulness scoring metrics, comprehensiveness and sufﬁciency, on the CommonsenseExplanations (CoS-E) dataset. For the former, erasing the tokens comprising the provided rationale (˜xi) ought todecrease model conﬁdence in the output ‘Forest’. For the latter, the model should be able to come to a similardisposition regarding ‘Forest’ using only the rationales ri.here means that the model became more conﬁdentin its prediction after the rationales were removed;this would seem counter-intuitive if the rationaleswere indeed the reason for its prediction.Sufﬁciency. This captures the degree to whichthe snippets within the extracted rationales are ade-quate for a model to make a prediction.sufﬁciency = m(xi)j −m(ri)j(2)These metrics are illustrated in Figure 2.As deﬁned, the above measures have assumeddiscrete rationales ri. We would also like to eval-uate the faithfulness of continuous importancescores assigned to tokens by models. Here weadopt a simple approach for this. We convert softscores over features si provided by a model intodiscrete rationales ri by taking the top−kd values,where kd is a threshold for dataset d. We set kd tothe average rationale length provided by humansfor dataset d (see Table 4). Intuitively, this says:How much does the model prediction change if weremove a number of tokens equal to what humansuse (on average for this dataset) in order of theimportance scores assigned to these by the model.Once we have discretized the soft scores into ra-tionales in this way, we compute the faithfulnessscores as per Equations 1 and 2.This approach is conceptually simple. It is alsocomputationally cheap to evaluate, in contrast tomeasures that require per-token measurements, e.g.,importance score correlations with ‘leave-one-out’scores (Jain and Wallace, 2019), or counting howmany ‘important’ tokens need to be erased beforea prediction ﬂips (Serrano and Smith, 2019). How-ever, the necessity of discretizing continuous scoresforces us to pick a particular threshold k.We can also consider the behavior of these mea-sures as a function of k, inspired by the measure-ments proposed in Samek et al. (2016) in the con-text of evaluating saliency maps for image classi-ﬁcation. They suggested ranking pixel regions byimportance and then measuring the change in out-put as they are removed in rank order. Our datasetscomprise documents and rationales with quite dif-ferent lengths; to make this measure comparableacross datasets, we construct bins designating thenumber of tokens to be deleted. Denoting the to-kens up to and including bin k for instance i by rik,we deﬁne an aggregate comprehensiveness mea-sure:1∣B∣+ 1(∣B∣∑k=0m(xi)j −m(xi/rik)j)(3)This is deﬁned for sufﬁciency analogously. Herewe group tokens into k = 5 bins by grouping theminto the top 1%, 5%, 10%, 20% and 50% of to-kens, with respect to the corresponding importancescore. We refer to these metrics as “Area Over thePerturbation Curve” (AOPC).7These AOPC sufﬁciency and comprehensivenessmeasures score a particular token ordering undera model. As a point of reference, we also reportthese when random scores are assigned to tokens.7Our AOPC metrics are similar in concept to ROAR(Hooker et al., 2019) except that we re-use an existing modelas opposed to retraining for each fraction.\f44495Baseline ModelsOur focus in this work is primarily on the ERASERbenchmark itself, rather than on any particularmodel(s). But to establish a starting point for futurework, we evaluate several baseline models acrossthe corpora in ERASER.8 We broadly classify theseinto models that assign ‘soft’ (continuous) scoresto tokens, and those that perform a ‘hard’ (discrete)selection over inputs. We additionally considermodels speciﬁcally designed to select individualtokens (and very short sequences) as rationales, ascompared to longer snippets. All of our implemen-tations are in PyTorch (Paszke et al., 2019) and areavailable in the ERASER repository.9All datasets in ERASER comprise inputs, ratio-nales, and labels. But they differ considerably indocument and rationale lengths (Table A). This mo-tivated use of different models for datasets, appro-priate to their sizes and rationale granularities. Wehope that this benchmark motivates design of mod-els that provide rationales that can ﬂexibly adapt tovarying input lengths and expected rationale gran-ularities. Indeed, only with such models can weperform comparisons across all datasets.5.1Hard selectionModels that perform hard selection may be viewedas comprising two independent modules: an en-coder which is responsible for extracting snippetsof inputs, and a decoder that makes a predictionbased only on the text provided by the encoder. Weconsider two variants of such models.Lei et al. (2016). In this model, an encoder in-duces a binary mask over inputs x, z. The decoderaccepts the tokens in x unmasked by z to make aprediction ˆy. These modules are trained jointly viaREINFORCE (Williams, 1992) style estimation,minimizing the loss over expected binary vectorsz yielded from the encoder. One of the advantagesof this approach is that it need not have access tomarked rationales; it can learn to rationalize on thebasis of instance labels alone. However, given thatwe do have rationales in the training data, we exper-iment with a variant in which we train the encoderexplicitly using rationale-level annotations.In our implementation of Lei et al. (2016), wedrop in two independent BERT (Devlin et al., 2019)or GloVe (Pennington et al., 2014) base modules8This is not intended to be comprehensive.9https://github.com/jayded/eraserbenchmarkwith bidirectional LSTMs (Hochreiter and Schmid-huber, 1997) on top to induce contextualized rep-resentations of tokens for the encoder and decoder,respectively. The encoder generates a scalar (de-noting the probability of selecting that token) foreach LSTM hidden state using a feedfoward layerand sigmoid. In the variant using human rationalesduring training, we minimize cross entropy lossover rationale predictions. The ﬁnal loss is thena composite of classiﬁcation loss, regularizers onrationales (Lei et al., 2016), and loss over rationalepredictions, when available.Pipeline models.These are simple models inwhich we ﬁrst train the encoder to extract ratio-nales, and then train the decoder to perform pre-diction using only rationales. No parameters areshared between the two models.Here we ﬁrst consider a simple pipeline that ﬁrstsegments inputs into sentences. It passes these,one at a time, through a Gated Recurrent Unit(GRU) (Cho et al., 2014), to yield hidden represen-tations that we compose via an attentive decodinglayer (Bahdanau et al., 2015). This aggregate rep-resentation is then passed to a classiﬁcation modulewhich predicts whether the corresponding sentenceis a rationale (or not). A second model, using effec-tively the same architecture but parameterized inde-pendently, consumes the outputs (rationales) fromthe ﬁrst to make predictions. This simple model isdescribed at length in prior work (Lehman et al.,2019). We further consider a ‘BERT-to-BERT’pipeline, where we replace each stage with a BERTmodule for prediction (Devlin et al., 2019).In pipeline models, we train each stage indepen-dently. The rationale identiﬁcation stage is trainedusing approximate sentence boundaries from oursource annotations, with randomly sampled neg-ative examples at each epoch. The classiﬁcationstage uses the same positive rationales as the iden-tiﬁcation stage, a type of teacher forcing (Williamsand Zipser, 1989) (details in Appendix C).5.2Soft selectionWe consider a model that passes tokens throughBERT (Devlin et al., 2019) to induce contextual-ized representations that are then passed to a bi-directional LSTM (Hochreiter and Schmidhuber,1997). The hidden representations from the LSTMare collapsed into a single vector using additiveattention (Bahdanau et al., 2015). The LSTM layerallows us to bypass the 512 word limit imposed by\f4450Perf.IOU F1Token F1Evidence InferenceLei et al. (2016)0.4610.0000.000Lei et al. (2016) (u)0.4610.0000.000Lehman et al. (2019)0.4710.1190.123Bert-To-Bert0.7080.4550.468BoolQLei et al. (2016)0.3810.0000.000Lei et al. (2016) (u)0.3800.0000.000Lehman et al. (2019)0.4110.0500.127Bert-To-Bert0.5440.0520.134Movie ReviewsLei et al. (2016)0.9140.1240.285Lei et al. (2016) (u)0.9200.0120.322Lehman et al. (2019)0.7500.0630.139Bert-To-Bert0.8600.0750.145FEVERLei et al. (2016)0.7190.2180.234Lei et al. (2016) (u)0.7180.0000.000Lehman et al. (2019)0.6910.5400.523Bert-To-Bert0.8770.8350.812MultiRCLei et al. (2016)0.6550.2710.456Lei et al. (2016) (u)0.6480.000†0.000†Lehman et al. (2019)0.6140.1360.140Bert-To-Bert0.6330.4160.412CoS-ELei et al. (2016)0.4770.2550.331Lei et al. (2016) (u)0.4760.000†0.000†Bert-To-Bert0.3440.3890.519e-SNLILei et al. (2016)0.9170.6930.692Lei et al. (2016) (u)0.9030.2610.379Bert-To-Bert0.7330.7040.701Table 3: Performance of models that perform hard ra-tionale selection. All models are supervised at the ratio-nale level except for those marked with (u), which learnonly from instance-level supervision; † denotes cases inwhich rationale training degenerated due to the REIN-FORCE style training. Perf. is accuracy (CoS-E) ormacro-averaged F1 (others). Bert-To-Bert for CoS-Eand e-SNLI uses a token classiﬁcation objective. Bert-To-Bert CoS-E uses the highest scoring answer.BERT; when we exceed this, we effectively startencoding a ‘new’ sequence (setting the positionalindex to 0) via BERT. The hope is that the LSTMlearns to compensate for this. Evidence Inferenceand BoolQ comprise very long (>1000 token) in-puts; we were unable to run BERT over these. Weinstead resorted to swapping GloVe 300d embed-dings (Pennington et al., 2014) in place of BERTrepresentations for tokens. spans.To soft score features we consider: Simple gra-dients, attention induced over contextualized repre-sentations, and LIME (Ribeiro et al., 2016).Perf.AUPRCComp. ↑Suff. ↓Evidence InferenceGloVe + LSTM - Attention0.4290.506-0.002-0.023GloVe + LSTM - Gradient0.4290.0160.046-0.138GloVe + LSTM - Lime0.4290.0140.006-0.128GloVe + LSTM - Random0.4290.014-0.001-0.026BoolQGloVe + LSTM - Attention0.4710.5250.0100.022GloVe + LSTM - Gradient0.4710.0720.0240.031GloVe + LSTM - Lime0.4710.0730.028-0.154GloVe + LSTM - Random0.4710.0740.0000.005MoviesBERT+LSTM - Attention0.9700.4170.1290.097BERT+LSTM - Gradient0.9700.3850.1420.112BERT+LSTM - Lime0.9700.2800.1870.093BERT+LSTM - Random0.9700.2590.0580.330FEVERBERT+LSTM - Attention0.8700.2350.0370.122BERT+LSTM - Gradient0.8700.2320.0590.136BERT+LSTM - Lime0.8700.2910.2120.014BERT+LSTM - Random0.8700.2440.0340.122MultiRCBERT+LSTM - Attention0.6550.2440.0360.052BERT+LSTM - Gradient0.6550.2240.0770.064BERT+LSTM - Lime0.6550.2080.213-0.079BERT+LSTM - Random0.6550.1860.0290.081CoS-EBERT+LSTM - Attention0.4870.6060.0800.217BERT+LSTM - Gradient0.4870.5850.1240.226BERT+LSTM - Lime0.4870.5440.2230.143BERT+LSTM - Random0.4870.5940.0720.224e-SNLIBERT+LSTM - Attention0.9600.3950.1050.583BERT+LSTM - Gradient0.9600.4160.1800.472BERT+LSTM - Lime0.9600.5130.4370.389BERT+LSTM - Random0.9600.3570.0810.487Table 4: Metrics for ‘soft’ scoring models. Perf. is ac-curacy (CoS-E) or F1 (others). Comprehensiveness andsufﬁciency are in terms of AOPC (Eq. 3). ‘Random’assigns random scores to tokens to induce orderings;these are averages over 10 runs.6EvaluationHere we present initial results for the baseline mod-els discussed in Section 5, with respect to the met-rics proposed in Section 4. We present results intwo parts, reﬂecting the two classes of rationalesdiscussed above: ‘Hard’ approaches that performdiscrete selection of snippets, and ‘soft’ methodsthat assign continuous importance scores to tokens.In Table 3 we evaluate models that perform dis-crete selection of rationales. We view these as in-herently faithful, because by construction we knowwhich snippets the decoder used to make a pre-diction.10 Therefore, for these methods we reportonly metrics that measure agreement with humanannotations.10This assumes independent encoders and decoders.\f4451Due to computational constraints, we were un-able to run our BERT-based implementation of Leiet al. (2016) over larger corpora. Conversely, thesimple pipeline of Lehman et al. (2019) assumesa setting in which rationale are sentences, and sois not appropriate for datasets in which rationalestend to comprise only very short spans. Again, inour view this highlights the need for models thatcan rationalize at varying levels of granularity, de-pending on what is appropriate.We observe that for the “rationalizing” modelof Lei et al. (2016), exploiting rationale-level super-vision often (though not always) improves agree-ment with human-provided rationales, as in priorwork (Zhang et al., 2016; Strout et al., 2019). In-terestingly, this does not seem strongly correlatedwith predictive performance.Lei et al. (2016) outperforms the simple pipelinemodel when using a BERT encoder. Further, Leiet al. (2016) outperforms the ‘BERT-to-BERT’pipeline on the comparable datasets for the ﬁnalprediction tasks. This may be an artifact of theamount of text each model can select: ‘BERT-to-BERT’ is limited to sentences, while Lei et al.(2016) can select any subset of the text. Designingextraction models that learn to adaptively selectcontiguous rationales of appropriate length for agiven task seems a potentially promising direction.In Table 4 we report metrics for models thatassign continuous importance scores to individ-ual tokens. For these models we again measuredownstream (task) performance (macro F1 or ac-curacy). Here the models are actually the same,and so downstream performance is equivalent. Toassess the quality of token scores with respect tohuman annotations, we report the Area Under thePrecision Recall Curve (AUPRC).These scoring functions assign only soft scoresto inputs (and may still use all inputs to come toa particular prediction), so we report the metricsintended to measure faithfulness deﬁned above:comprehensiveness and sufﬁciency, averaged over‘bins’ of tokens ordered by importance scores. Toprovide a point of reference for these metrics —which depend on the underlying model — we re-port results when rationales are randomly selected(averaged over 10 runs).Both simple gradient and LIME-based scoringyield more comprehensive rationales than attentionweights, consistent with prior work (Jain and Wal-lace, 2019; Serrano and Smith, 2019). Attentionfares better in terms of AUPRC — suggesting bet-ter agreement with human rationales — which isalso in line with prior ﬁndings that it may provideplausible, but not faithful, explanation (Zhong et al.,2019). Interestingly, LIME does particularly wellacross these tasks in terms of faithfulness.From the ‘Random’ results that we concludemodels with overall poor performance on their ﬁ-nal tasks tend to have an overall poor ordering, withmarginal differences in comprehensiveness and suf-ﬁciency between them. For models that with highsufﬁciency scores: Movies, FEVER, CoS-E, and e-SNLI, we ﬁnd that random removal is particularlydamaging to performance, indicating poor absoluteranking; whereas those with high comprehensive-ness are sensitive to rationale length.7Conclusions and Future DirectionsWe have introduced a new publicly available re-source: the Evaluating Rationales And Simple En-glish Reasoning (ERASER) benchmark. This com-prises seven datasets, all of which include bothinstance level labels and corresponding supportingsnippets (‘rationales’) marked by human annotators.We have augmented many of these datasets withadditional annotations, and converted them into astandard format comprising inputs, rationales, andoutputs. ERASER is intended to facilitate progresson explainable models for NLP.We proposed several metrics intended to mea-sure the quality of rationales extracted by models,both in terms of agreement with human annota-tions, and in terms of ‘faithfulness’. We believethese metrics provide reasonable means of compar-ison of speciﬁc aspects of interpretability, but weview the problem of measuring faithfulness, in par-ticular, a topic ripe for additional research (whichERASER can facilitate).Our hope is that ERASER enables future workon designing more interpretable NLP models, andcomparing their relative strengths across a vari-ety of tasks, datasets, and desired criteria. It alsoserves as an ideal starting point for several futuredirections such as better evaluation metrics for in-terpretability, causal analysis of NLP models anddatasets of rationales in other languages.8AcknowledgementsWe thank the anonymous ACL reviewers.This work was supported in part by the NSF (CA-REER award 1750978), and by the Army ResearchOfﬁce (W911NF1810328).\f4452ReferencesDavid Alvarez-Melis and Tommi Jaakkola. 2017.Acausal framework for explaining the predictions ofblack-box sequence-to-sequence models.In Pro-ceedings of the 2017 Conference on Empirical Meth-ods in Natural Language Processing, pages 412–421.Leila Arras, Franziska Horn, Gr´egoire Montavon,Klaus-Robert M¨uller, and Wojciech Samek. 2017.”what is relevant in a text document?”: An inter-pretable machine learning approach. In PloS one.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. 2015.Neural machine translation by jointlylearning to align and translate.In 3rd Inter-national Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings.Joost Bastings, Wilker Aziz, and Ivan Titov. 2019. In-terpretable neural predictions with differentiable bi-nary variables. In Proceedings of the 57th AnnualMeeting of the Association for Computational Lin-guistics, pages 2963–2977, Florence, Italy. Associa-tion for Computational Linguistics.Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-ert: Pretrained language model for scientiﬁc text. InEMNLP.Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning. 2015. A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Association for Computational Linguistics.Gino Brunner, Yang Liu, Damian Pascual, OliverRichter, Massimiliano Ciaramita, and Roger Watten-hofer. 2020. On identiﬁability in transformers. InInternational Conference on Learning Representa-tions.Oana-Maria Camburu,Tim Rockt¨aschel,ThomasLukasiewicz, and Phil Blunsom. 2018. e-snli: Nat-ural language inference with natural language expla-nations. In Advances in Neural Information Process-ing Systems, pages 9539–9549.Shiyu Chang, Yang Zhang, Mo Yu, and TommiJaakkola. 2019. A game theoretic approach to class-wise selective rationalization. In Advances in Neu-ral Information Processing Systems, pages 10055–10065.Sihao Chen, Daniel Khashabi, Wenpeng Yin, ChrisCallison-Burch, and Dan Roth. 2019. Seeing thingsfrom a different angle: Discovering diverse perspec-tives about claims. In Proceedings of the Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL), pages 542–557, Minneapolis, Minnesota.Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio. 2014.Learningphrase representations using RNN encoder–decoderfor statistical machine translation. In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association for ComputationalLinguistics.Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifﬁculty of natural yes/no questions. In NAACL.Jacob Cohen. 1960.A coefﬁcient of agreement fornominal scales.Educational and PsychologicalMeasurement, 20(1):37–46.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019.BERT: Pre-training ofdeep bidirectional transformers for language under-standing.In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers),pages 4171–4186, Minneapolis, Minnesota. Associ-ation for Computational Linguistics.Yanzhuo Ding, Yang Liu, Huanbo Luan, and MaosongSun. 2017.Visualizing and understanding neuralmachine translation. In Proceedings of the 55th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), Vancouver,Canada. Association for Computational Linguistics.Finale Doshi-Velez and Been Kim. 2017. Towards arigorous science of interpretable machine learning.arXiv preprint arXiv:1702.08608.Mark Everingham, Luc Van Gool, Christopher K. I.Williams, John Winn, and Andrew Zisserman. 2010.The pascal visual object classes (voc) challenge. In-ternational Journal of Computer Vision, 88(2):303–338.Shi Feng, Eric Wallace, Alvin Grissom, Mohit Iyyer,Pedro Rodriguez, and Jordan L. Boyd-Graber. 2018.Pathologies of neural models make interpretationdifﬁcult. In EMNLP.Matt Gardner, Joel Grus, Mark Neumann, OyvindTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-ters, Michael Schmitz, and Luke Zettlemoyer. 2018.AllenNLP: A deep semantic natural language pro-cessing platform. In Proceedings of Workshop forNLP Open Source Software (NLP-OSS), pages 1–6, Melbourne, Australia. Association for Computa-tional Linguistics.Sepp Hochreiter and J¨urgen Schmidhuber. 1997.Long short-term memory.Neural computation,9(8):1735–1780.\f4453Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans,and Been Kim. 2019. A benchmark for interpretabil-ity methods in deep neural networks.In H. Wal-lach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,E. Fox, and R. Garnett, editors, Advances in Neu-ral Information Processing Systems 32, pages 9737–9748. Curran Associates, Inc.Alon Jacovi and Yoav Goldberg. 2020. Towards faith-fully interpretable nlp systems:How should wedeﬁne and evaluate faithfulness?arXiv preprintarXiv:2004.03685.Sarthak Jain and Byron C. Wallace. 2019. Attention isnot Explanation. In Proceedings of the 2019 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Volume 1 (Long and Short Pa-pers), pages 3543–3556, Minneapolis, Minnesota.Association for Computational Linguistics.Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and By-ron C. Wallace. 2020. Learning to Faithfully Ratio-nalize by Construction. In Proceedings of the Con-ference of the Association for Computational Lin-guistics (ACL).Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,Shyam Upadhyay, and Dan Roth. 2018.LookingBeyond the Surface: A Challenge Set for ReadingComprehension over Multiple Sentences. In Proc.of the Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL).Diederik Kingma and Jimmy Ba. 2014.Adam: Amethod for stochastic optimization.InternationalConference on Learning Representations.Eric Lehman, Jay DeYoung, Regina Barzilay, and By-ron C Wallace. 2019. Inferring which medical treat-ments work from reports of clinical trials. In Pro-ceedings of the North American Chapter of the As-sociation for Computational Linguistics (NAACL),pages 3705–3717.Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.Rationalizing neural predictions. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 107–117.Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.2016. Visualizing and understanding neural modelsin NLP. In Proceedings of the 2016 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 681–691, San Diego, California. As-sociation for Computational Linguistics.Zachary C Lipton. 2016. The mythos of model inter-pretability. arXiv preprint arXiv:1606.03490.Scott M Lundberg and Su-In Lee. 2017.A uniﬁedapproach to interpreting model predictions. In Ad-vances in Neural Information Processing Systems,pages 4765–4774.Tyler McDonnell, Mucahid Kutlu, Tamer Elsayed, andMatthew Lease. 2017. The many beneﬁts of anno-tator rationales for relevance judgments. In IJCAI,pages 4909–4913.Tyler McDonnell, Matthew Lease, Mucahid Kutlu, andTamer Elsayed. 2016.Why is that relevant? col-lecting annotator rationales for relevance judgments.In Fourth AAAI Conference on Human Computationand Crowdsourcing.Gr´egoire Montavon, Sebastian Lapuschkin, AlexanderBinder, Wojciech Samek, and Klaus-Robert M¨uller.2017. Explaining nonlinear classiﬁcation decisionswith deep taylor decomposition.Pattern Recogni-tion, 65:211–222.Pooya Moradi, Nishant Kambhatla, and Anoop Sarkar.2019. Interrogating the explanatory power of atten-tion in neural machine translation. In Proceedings ofthe 3rd Workshop on Neural Generation and Trans-lation, pages 221–230, Hong Kong. Association forComputational Linguistics.Mark Neumann, Daniel King, Iz Beltagy, and WaleedAmmar. 2019.Scispacy: Fast and robust modelsfor biomedical natural language processing. CoRR,abs/1902.07669.Dong Nguyen. 2018. Comparing automatic and humanevaluation of local explanations for text classiﬁca-tion. In Proceedings of the 2018 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, Volume 1 (Long Papers), pages 1069–1078.Bo Pang and Lillian Lee. 2004.A sentimental edu-cation: Sentiment analysis using subjectivity sum-marization based on minimum cuts.In Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics (ACL-04), pages 271–278, Barcelona, Spain.Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, et al. 2019. Pytorch: An imperative style,high-performance deep learning library.In Ad-vances in Neural Information Processing Systems,pages 8024–8035.David J Pearce. 2005. An improved algorithm for ﬁnd-ing the strongly connected components of a directedgraph. Technical report, Victoria University, NZ.Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. Glove: Global vectors for word rep-resentation. In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 1532–1543, Doha, Qatar. Asso-ciation for Computational Linguistics.Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Gra-ham Neubig, and Zachary C. Lipton. 2020. Learn-ing to deceive with attention-based explanations. InAnnual Conference of the Association for Computa-tional Linguistics (ACL).\f4454Sampo Pyysalo, F Ginter, Hans Moen, T Salakoski, andSophia Ananiadou. 2013. Distributional semanticsresources for biomedical text processing. Proceed-ings of Languages in Biology and Medicine.Alec Radford, Karthik Narasimhan, Tim Salimans, andIlya Sutskever. 2018.Improving language under-standing by generative pre-training.Nazneen Fatema Rajani, Bryan McCann, CaimingXiong, and Richard Socher. 2019. Explain yourself!leveraging language models for commonsense rea-soning. Proceedings of the Association for Compu-tational Linguistics (ACL).Marco Ribeiro, Sameer Singh, and Carlos Guestrin.2016. why should i trust you?: Explaining the pre-dictions of any classiﬁer. In Proceedings of the 2016Conference of the North American Chapter of theAssociation for Computational Linguistics: Demon-strations, pages 97–101.Wojciech Samek, Alexander Binder, Gr´egoire Mon-tavon, Sebastian Lapuschkin, and Klaus-RobertM¨uller. 2016. Evaluating the visualization of whata deep neural network has learned.IEEE trans-actions on neural networks and learning systems,28(11):2660–2673.Tal Schuster, Darsh J Shah, Yun Jie Serene Yeo, DanielFilizzola, Enrico Santus, and Regina Barzilay. 2019.Towards debiasing fact veriﬁcation models. In Pro-ceedings of the 2019 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP). As-sociation for Computational Linguistics.Soﬁa Serrano and Noah A. Smith. 2019. Is attentioninterpretable?In Proceedings of the 57th AnnualMeeting of the Association for Computational Lin-guistics, pages 2931–2951, Florence, Italy. Associa-tion for Computational Linguistics.Burr Settles. 2012.Active learning.Synthesis Lec-tures on Artiﬁcial Intelligence and Machine Learn-ing, 6(1):1–114.Manali Sharma, Di Zhuang, and Mustafa Bilgic. 2015.Active learning with rationales for text classiﬁcation.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 441–451.Kevin Small, Byron C Wallace, Carla E Brodley, andThomas A Trikalinos. 2011. The constrained weightspace svm: learning with ranked features. In Pro-ceedings of the International Conference on Inter-national Conference on Machine Learning (ICML),pages 865–872.D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wat-tenberg. 2017.SmoothGrad: removing noise byadding noise. ICML workshop on visualization fordeep learning.Robyn Speer. 2019. ftfy. Zenodo. Version 5.5.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov. 2014.Dropout: A simple way to prevent neural networksfrom overﬁtting. Journal of Machine Learning Re-search, 15:1929–1958.Julia Strout, Ye Zhang, and Raymond Mooney. 2019.Do human rationales improve machine explana-tions?In Proceedings of the 2019 ACL WorkshopBlackboxNLP: Analyzing and Interpreting NeuralNetworks for NLP, pages 56–62, Florence, Italy. As-sociation for Computational Linguistics.Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.Axiomatic attribution for deep networks.In Pro-ceedings of the 34th International Conference onMachine Learning-Volume 70, pages 3319–3328.JMLR. org.Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2019. CommonsenseQA: A ques-tion answering challenge targeting commonsenseknowledge. In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers),pages 4149–4158, Minneapolis, Minnesota. Associ-ation for Computational Linguistics.JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal.2018.FEVER: a Large-scale Dataset for Fact Extractionand VERiﬁcation.In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL), pages 809–819.Shikhar Vashishth, Shyam Upadhyay, Gaurav SinghTomar, and Manaal Faruqui. 2019.Attention in-terpretability across nlp tasks.arXiv preprintarXiv:1909.11218.Byron C Wallace, Kevin Small, Carla E Brodley, andThomas A Trikalinos. 2010.Active learning forbiomedical citation screening.In Proceedings ofthe 16th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 173–182. ACM.Alex Wang,Yada Pruksachatkun,Nikita Nangia,Amanpreet Singh, Julian Michael, Felix Hill, OmerLevy, and Samuel Bowman. 2019a. Superglue: Astickier benchmark for general-purpose language un-derstanding systems. In H. Wallach, H. Larochelle,A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-nett, editors, Advances in Neural Information Pro-cessing Systems 32, pages 3266–3280. Curran Asso-ciates, Inc.Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R. Bowman. 2019b.GLUE: A multi-task benchmark and analysis plat-form for natural language understanding. In Inter-national Conference on Learning Representations.\f4455Sarah Wiegreffe and Yuval Pinter. 2019. Attention isnot not explanation. In Proceedings of the 2019 Con-ference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP), pages 11–20, Hong Kong, China. Associ-ation for Computational Linguistics.Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforce-ment learning. Machine learning, 8(3-4):229–256.Ronald J Williams and David Zipser. 1989. A learn-ing algorithm for continually running fully recurrentneural networks.Neural computation, 1(2):270–280.Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-icz, and Jamie Brew. 2019.Huggingface’s trans-formers: State-of-the-art natural language process-ing. ArXiv, abs/1910.03771.Mo Yu, Shiyu Chang, Yang Zhang, and TommiJaakkola. 2019. Rethinking cooperative rationaliza-tion: Introspective extraction and complement con-trol.In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages4094–4103, Hong Kong, China. Association forComputational Linguistics.Omar Zaidan, Jason Eisner, and Christine Piatko.2007.Using annotator rationales to improve ma-chine learning for text categorization. In Proceed-ings of the conference of the North American chap-ter of the Association for Computational Linguistics(NAACL), pages 260–267.Omar F Zaidan and Jason Eisner. 2008. Modeling an-notators: A generative approach to learning from an-notator rationales. In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 31–40.Ye Zhang, Iain Marshall, and Byron C Wallace. 2016.Rationale-augmented convolutional neural networksfor text classiﬁcation. In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), volume 2016, page 795. NIHPublic Access.Ruiqi Zhong, Steven Shao, and Kathleen McKeown.2019. Fine-grained sentiment analysis with faithfulattention. arXiv preprint arXiv:1908.06870.AppendixADataset PreprocessingWe describe what, if any, additional processing weperform on a per-dataset basis. All datasets wereconverted to a uniﬁed format.MultiRC (Khashabi et al., 2018) We perform min-imal processing. We use the validation set as thetesting set for public release.Evidence Inference (Lehman et al., 2019) We per-form minimal processing. As not all of the pro-vided evidence spans come with offsets, we deleteany prompts that had no grounded evidence spans.Movie reviews (Zaidan and Eisner, 2008) We per-form minimal processing. We use the ninth fold asthe validation set, and collect annotations on thetenth fold for comprehensive evaluation.FEVER (Thorne et al., 2018) We perform substan-tial processing for FEVER - we delete the ”NotEnough Info” claim class, delete any claims withsupport in more than one document, and reparti-tion the validation set into a validation and a testset for this benchmark (using the test set wouldcompromise the information retrieval portion ofthe original FEVER task). We ensure that thereis no document overlap between train, validation,and test sets (we use Pearce (2005) to ensure this,as conceptually a claim may be supported by factsin more than one document). We ensure that thevalidation set contains the documents used to cre-ate the FEVER symmetric dataset (Schuster et al.,2019) (unfortunately, the documents used to createthe validation and test sets overlap so we cannotprovide this partitioning). Additionally, we cleanup some encoding errors in the dataset via Speer(2019).BoolQ (Clark et al., 2019) The BoolQ dataset re-quired substantial processing. The original datasetdid not retain source Wikipedia articles or col-lection dates.In order to identify the sourceparagraphs, we download the 12/20/18 Wikipediaarchive, and use FuzzyWuzzy https://github.com/seatgeek/fuzzywuzzy to identify the sourceparagraph span that best matches the original re-lease. If the Levenshtein distance ratio does notreach a score of at least 90, the corresponding in-stance is removed. For public release, we use theofﬁcial validation set for testing, and repartitiontrain into a training and validation set.e-SNLI (Camburu et al., 2018) We perform mini-mal processing. We separate the premise and hy-pothesis statements into separate documents.Commonsense Explanations (CoS-E) (Rajaniet al., 2019) We perform minimal processing, pri-marily deletion of any questions without a rationale\f4456DatasetDocumentsInstancesRationale %Evidence StatementsEvidence LengthsMultiRCTrain4002402917.45629821.5Val56321418.5749822.8Test834848---Evidence InferenceTrain192479581.341037139.3Val2479721.38129440.3Test240959---Exhaustive Evidence InferenceVal811014.47504.035.2Test106152---Movie ReviewsTrain159916009.35138787.7Val1501507.451143.06.6Test200200---Exhaustive Movie ReviewsVal505019.10592.012.8FEVERTrain29159795720.014685631.3Val570612221.6867228.2Test6146111---BoolQTrain451863636.646363.0110.2Val109214917.131491.0106.5Test22942817---e-SNLITrain91193854930927.31199035.01.8Val16328982325.623639.01.6Test162999807---CoS-ETrain8733873326.687337.4Val1092109227.110927.6Test10921092---Table 5: Detailed breakdowns for each dataset - the number of documents, instances, evidence statements, andlengths. Additionally we include the percentage of each relevant document that is considered a rationale. For testsets, counts are for all instances including documents with non comprehensive rationales.DatasetLabelsInstancesDocumentsSentencesTokensEvidence Inference398892411156.04760.6BoolQ2106617026175.33582.5Movie Reviews22000199936.8774.1FEVER2110190409912.1326.5MultiRC23209153914.9302.5CoS-E510917109171.027.6e-SNLI35689399445651.716.0Table 6: General dataset statistics: number of labels, instances, unique documents, and average numbers of sen-tences and tokens in documents, across the publicly released train/validation/test splits in ERASER. For CoS-Eand e-SNLI, the sentence counts are not meaningful as the partitioning of question/sentence/answer formatting isan arbitrary choice in this framework.\f4457or questions with rationales that were not possi-ble to automatically map back to the underlyingtext. As recommended by the authors of Talmoret al. (2019) we repartition the train and validationsets into a train, validation, and test set for thisbenchmark. We encode the entire question and an-swers as a prompt and convert the problem into aﬁve-class prediction. We also convert the “Sanity”datasets for user convenience.All datasets in ERASER were tokenized usingspaCy11 library (with SciSpacy (Neumann et al.,2019) for Evidence Inference). In addition, we alsosplit all datasets except e-SNLI and CoS-E intosentences using the same library.BAnnotation detailsWe collected comprehensive rationales for a subsetof some test sets to accurately evaluate model recallof rationales.1. Movies. We used the Upwork Platform12 tohire two ﬂuent english speakers to annotateeach of the 200 documents in our test set.Workers were paid at rate of USD 8.5 per hourand on average, it took them 5 min to anno-tate a document. Each annotator was asked toannotate a set of 6 documents and comparedagainst in-house annotations (by authors).2. Evidence Inference. We again used Upworkto hire 4 medical professionals ﬂuent in en-glish and having passed a pilot of 3 documents.125 documents were annotated (only once byone of the annotators, which we felt was ap-propriate given their high-level of expertise)with an average cost of USD 13 per document.Average time spent of single document was31 min.3. BoolQ. We used Amazon Mechanical Turk(MTurk) to collect reference comprehensiverationales from randomly selected 199 docu-ments from our test set (ranging in 800 to 1500tokens in length). Only workers from AU, NZ,CA, US, GB with more than 10K approvedHITs and an approval rate of greater than 98%were eligible. For every document, 3 annota-tions were collected and workers were paidUSD 1.50 per HIT. The average work time(obtained through MTurk interface) was 21min. We did not anticipate the task taking so11https://spacy.io/12http://www.upwork.comlong (on average); the effective low pay ratewas unintended.CHyperparameter and training detailsC.1(Lei et al., 2016) modelsFor these models, we set the sparsity rate at 0.01and we set the contiguity loss weight to 2 timessparsity rate (following the original paper). Weused bert-base-uncased (Wolf et al., 2019) as to-ken embedder (for all datasets except BoolQ, Ev-idence Inference and FEVER) and BidirectionalLSTM with 128 dimensional hidden state in eachdirection. A dropout (Srivastava et al., 2014) rateof 0.2 was used before feeding the hidden repre-sentations to attention layer in decoder and linearlayer in encoder. One layer MLP with 128 dimen-sional hidden state and ReLU activation was usedto compute the decoder output distribution.For three datasets mentioned above, we useGloVe embeddings (http://nlp.stanford.edu/data/glove.840B.300d.zip).A learning rate of 2e-5 with Adam (Kingma andBa, 2014) optimizer was used for all models and weonly ﬁne-tuned top two layers of BERT encoder.Th models were trained for 20 epochs and earlystopping with patience of 5 epochs was used. Thebest model was selected on validation set using theﬁnal task performance metric.The input for the above model was encodedin form of [CLS] document [SEP] query[SEP].ThismodelwasimplementedusingtheAllenNLP library (Gardner et al., 2018).C.2BERT-LSTM/GloVe-LSTMThis model is essentially the same as the decoder inprevious section. The BERT-LSTM uses the samehyperparameters, and GloVe-LSTM is trained witha learning rate of 1e-2.C.3Lehman et al. (2019) modelsWith the exception of the Evidence Inferencedataset, these models were trained using the GLoVe(Pennington et al., 2014) 200 dimension word vec-tors, and Evidence Inference using the (Pyysaloet al., 2013) PubMed word vectors. We use Adam(Kingma and Ba, 2014) with a learning rate of1e-3, Dropout (Srivastava et al., 2014) of 0.05 ateach layer (embedding, GRU, attention layer) ofthe model, for 50 epochs with a patience of 10. Wemonitor validation loss, and keep the best modelon the validation set.\f4458C.4BERT-to-BERT modelWe primarily used the ‘bert-base-uncased‘ modelfor both components of the identiﬁcation and clas-siﬁcation pipeline, with the sole exception beingEvidence Inference with SciBERT (Beltagy et al.,2019). We trained with the standard BERT parame-ters of a learning rate of 1e-5, Adam (Kingma andBa, 2014), for 10 epochs. We monitor validationloss, and keep the best model on the validation set.",
        "answer": "Amazon Mechanical Turk (MTurk).",
        "space_key_index": 1,
        "metadata": {
            "format": "PDF 1.3",
            "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
            "author": "Jay DeYoung ; Sarthak Jain ; Nazneen Fatema Rajani ; Eric Lehman ; Caiming Xiong ; Richard Socher ; Byron C. Wallace",
            "subject": "acl 2020",
            "keywords": "",
            "creator": "LaTeX with hyperref package",
            "producer": "pdfTeX-1.40.17",
            "creationDate": "D:20200518115416Z",
            "modDate": "D:20200518115416Z",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "Was the conditional training method effective in controlling response-relatedness?",
        "context": "Proceedings of NAACL-HLT 2019, pages 1702–1723Minneapolis, Minnesota, June 2 - June 7, 2019. c⃝2019 Association for Computational Linguistics1702What makes a good conversation?How controllable attributes affect human judgmentsAbigail See∗Stanford Universityabisee@stanford.eduStephen RollerFacebook AI Researchroller@fb.comDouwe KielaFacebook AI Researchdkiela@fb.comJason WestonFacebook AI Researchjase@fb.comAbstractA good conversation requires balance – be-tween simplicity and detail; staying on topicand changing it; asking questions and an-swering them. Although dialogue agents arecommonly evaluated via human judgmentsof overall quality, the relationship betweenquality and these individual factors is lesswell-studied. In this work, we examine twocontrollable neural text generation methods,conditional training and weighted decoding,in order to control four important attributesfor chitchat dialogue: repetition, speciﬁcity,response-relatedness and question-asking. Weconduct a large-scale human evaluation tomeasure the effect of these control parame-ters on multi-turn interactive conversations onthe PersonaChat task. We provide a detailedanalysis of their relationship to high-level as-pects of conversation, and show that by con-trolling combinations of these variables ourmodels obtain clear improvements in humanquality judgments.1IntroductionNeural generation models for dialogue, despitetheir ubiquity in current research, are still poorlyunderstood. Well known problems, such as thegenericness and repetitiveness of responses (Ser-ban et al., 2016a), remain without a de facto solu-tion. Strikingly, the factors that determine humanjudgments of overall conversation quality are al-most entirely unexplored. Most works have beenlimited to the next utterance prediction problem,whereas a multi-turn evaluation is necessary toevaluate the quality of a full conversation.In this work we both (i) conduct a large-scalestudy to identify the ﬁne-grained factors governinghuman judgments of full conversations, and (ii)develop models that apply our ﬁndings in practice,∗A.S. completed most of this work at Facebook (FAIR).Figure 1: We manipulate four low-level attributes andmeasure their effect on human judgments of individualconversational aspects, as well as overall quality.leading to state-of-the-art performance. Speciﬁ-cally, we identify and study eight aspects of con-versation that can be measured by human judg-ments, while varying four types of low-level at-tributes that can be algorithmically controlled inneural models; see Figure 1. To control the low-level model attributes, we consider two simple butgeneral algorithms: conditional training, in whichthe neural model is conditioned on additional con-trol features, and weighted decoding, in whichcontrol features are added to the decoding scoringfunction at test time only.One major result of our ﬁndings is that existingwork has ignored the importance of conversationalﬂow, as standard models (i) repeat or contradictprevious statements, (ii) fail to balance speciﬁcitywith genericness, and (iii) fail to balance askingquestions with other dialogue acts.Conductingexperiments on the PersonaChat task (Zhang et al.,2018b), we obtain signiﬁcantly higher engaging-ness scores than the baseline by optimizing con-trol of repetition, speciﬁcity and question-askingover multiple turns. Using these ﬁndings, our bestmodel matches the performance of the winning en-try in the recent NeurIPS ConvAI2 competition(Dinan et al., 2019), which was trained on much\f1703more data but had no control (see Section 8.1).Our code, pretrained models, and full chatlogs, areavailable at https://parl.ai/projects/controllable_dialogue.2Related WorkDialogueDialogue evaluation is relatively wellunderstood in goal-oriented tasks, where auto-mated approaches can be coded by measuring taskcompletion (Bordes et al., 2017; El Asri et al.,2017; Hastie, 2012; Henderson et al., 2014; Wenet al., 2017).Task success combined with dia-logue cost can be linked to human judgments likeuser satisfaction via the PARADISE framework(Walker et al., 1997).However in chitchat tasks, which we study inthis work, automatic metrics and their relation tohuman ratings are less well-understood.Whileword-overlap metrics are effective for question-answering and machine translation, for dialoguethey have little to no correlation with human judg-ments (Liu et al., 2016; Novikova et al., 2017) –this is due to the open-ended nature of dialogue.There are more recent attempts to ﬁnd better auto-matic approaches, such as adversarial evaluation(Li et al., 2017b) and learning a scoring model(Lowe et al., 2017), but their value is still unclear.Nevertheless, a number of studies only use au-tomatic metrics, with no human study at all (Loweet al., 2015; Parthasarathi and Pineau, 2018; Ser-ban et al., 2016b).Other works do use humanevaluations (Dinan et al., 2018; Li et al., 2016a,b;Venkatesh et al., 2017; Vinyals and Le, 2015;Zhang et al., 2018b), typically reporting just onetype of judgment (either quality or appropriate-ness) via a Likert scale or pairwise comparison.Most of those works only consider single turnevaluations, often with a shortened dialogue his-tory, rather than full multi-turn dialogue.A more comprehensive evaluation strategy hasbeen studied within the scope of the Alexa prize(Venkatesh et al., 2017; Guo et al., 2018) by com-bining multiple automatic metrics designed to cap-ture various conversational aspects (engagement,coherence, domain coverage, conversational depthand topical diversity). Though these aspects havesome similarity to the aspects studied here, we alsofocus on lower-level aspects (e.g. avoiding repeti-tion, ﬂuency), to understand how they correspondto both our controllable attributes, and to overallquality judgments.ControllableneuraltextgenerationRe-searchers have proposed several approaches tocontrol aspects of RNN-based natural languagegeneration such as sentiment, length, speakerstyle and tense (Fan et al., 2018; Ficler andGoldberg, 2017; Ghazvininejad et al., 2017; Huet al., 2017; Kikuchi et al., 2016; Peng et al.,2018; Wang et al., 2017). In particular, severalworks use control to tackle the same commonsequence-to-sequence problems we address here(particularly genericness and unrelated output),in the context of single-turn response generation(Baheti et al., 2018; Li et al., 2016a, 2017a; Shenet al., 2017; Xing et al., 2017; Zhang et al., 2018a;Zhou et al., 2017).By contrast, we focus ondeveloping controls for, and human evaluation of,multi-turn interactive dialogue – this includes anew method (described in Section 5) to controlattributes at the dialogue level rather than theutterance level.In this work, we require a control method thatis both general-purpose (one technique to simul-taneously control many attributes) and easily tun-able (the control setting is adjustable after train-ing). Given these constraints, we study two controlmethods: conditional training (variants of whichhave been described by Fan et al. (2018); Kikuchiet al. (2016); Peng et al. (2018)) and weighted de-coding (described by Ghazvininejad et al. (2017)as a general technique, and by Baheti et al. (2018)to control response-relatedness). To our knowl-edge, this work is the ﬁrst to systematically com-pare the effectiveness of two general-purpose con-trol methods across several attributes.3The PersonaChat datasetPersonaChat (Zhang et al., 2018b) is a chitchatdialogue task involving two participants (two hu-mans or a human and a bot). Each participant isgiven a persona – a short collection of personaltraits such as I’m left handed or My favorite seasonis spring – and are instructed to get to know eachother by chatting naturally using their designatedpersonas, for 6–8 turns. The training set contains8939 conversations and 955 personas, collectedvia crowdworkers, plus 1000 conversations and100 personas for validation, and a similar numberin the hidden test set. The PersonaChat task wasthe subject of the NeurIPS 2018 ConvAI2 Chal-lenge (Dinan et al., 2019), in which competitorswere ﬁrst evaluated with respect to automatic met-\f1704rics (perplexity, hits@1 and F1 score), and thenwith respect to human judgment via the question“How much did you enjoy talking to this user?”on a scale of 1–4.4Baseline modelOur baseline model is a 2-layer LSTM sequence-to-sequence model with attention.On any dia-logue turn, the input x to the encoder is the entiredialogue history (separated using unique speaker-identifying tokens), with the model’s own personaprepended.Conditioned on this input sequencex, the decoder generates a response y.Exceptwhen stated otherwise, all our models decode us-ing beam search with beam size 20.We initialized the word embedding matrix with300-dimensional GloVe embeddings (Penningtonet al., 2014). Using the ParlAI framework (Milleret al., 2017), we pretrained the model on a datasetof 2.5 million Twitter message-response pairs,1then ﬁne-tuned it on PersonaChat.On the Per-sonaChat validation set, the baseline model has aperplexity of 26.83 and F1 of 17.02, which wouldhave placed us 4th out of 26 models in the Con-vAI2 competition (Dinan et al., 2019). We attemptto improve over this baseline using control.5Controllable text generation methodsSuppose we have a sequence-to-sequence modelwhich gives P(y|x) = ΠtP(yt|x, y1, . . . , yt−1),the conditional probability of a response y (themodel’s next utterance) given input x (the context,which in our case includes the model’s own per-sona and the dialogue history).Contrary to most previous work, which con-trols at the sentence level, we wish to control at-tributes of the output y at the dialogue level –meaning that a single control setting is used for awhole dialogue. For example, to control question-asking, we provide a control setting at the begin-ning of each dialogue (e.g. 20% questions or 70%questions) rather than providing a control settingfor each utterance (e.g. is a question or isn’t aquestion). With this approach, the sequence-to-sequence model is able to choose what value thecontrolled attribute should take for any particularutterance, but we are able to choose the overall dis-tribution. We ﬁnd that this approach works well– for example, the sequence-to-sequence model is1The Twitter dataset is provided in ParlAI; details can befound here: https://parl.ai/docs/tasks.htmlgenerally good at detecting when to ask a ques-tion. In particular, this is easier than the alterna-tive: developing a separate process to decide, foreach utterance, whether to ask a question.In this section, we describe the two methods– which we call Conditional Training (CT) andWeighted Decoding (WD) – that we use to controlattributes of the output y at the dialogue level.5.1Conditional Training (CT)Conditional Training (Fan et al., 2018; Kikuchiet al., 2016; Peng et al., 2018) is a method tolearn a sequence-to-sequence model P(y|x, z),where z is a discrete control variable.If thecontrol attribute is naturally continuous (for ex-ample in our work, repetitiveness, speciﬁcity andresponse-relatedness), we use z to represent buck-eted ranges. For a binary attribute like question-asking, z represents an overall probability (as ex-plained in Section 5).To train a CT model, we ﬁrst automatically an-notate every (x, y) pair in the training set with theattribute we wish to control (for example, whethery contains a question mark).During training,for each example we determine the correspond-ing z value (for continuous attributes, this sim-ply means sorting into the correct bucket; forquestion-asking, see Section 6.4). Next, the con-trol variable z is represented via an embedding(each of the possible values of z has its own em-bedding). For all our experiments, the embeddingis of length 10; this was determined via hyperpa-rameter tuning. There are several possible waysto condition the sequence-to-sequence model on z– for example, append z to the end of the inputsequence, or use z as the START symbol for thedecoder. We ﬁnd it most effective to concatenatez to the decoder’s input on every step.2 Lastly, theCT model learns to produce y = y1, . . . , yT byoptimizing the cross-entropy loss:lossCT = −1TTXt=1log P(yt|x, z, y1, . . . , yt−1)Our CT models are initialized with the parametersfrom the baseline sequence-to-sequence modelP(y|x) (the new decoder parameters are initial-ized with small random values), then ﬁne-tuned tooptimize lossCT on the PersonaChat training set,until convergence of lossCT on the validation set.2To build a CT model P(y|x, z1, . . . , zn) conditioned onmultiple controls {z1, . . . , zn}, we can simply concatenatemultiple control embeddings to the decoder inputs.\f17055.2Weighted Decoding (WD)Weighted Decoding (Ghazvininejad et al., 2017)is a decoding method that increases or decreasesthe probability of words with certain features. Thetechnique is applied only at test time, requiring nochange to the training method. A limitation of WDis that the controllable attribute must be deﬁnedat the word-level; any desired utterance-level at-tribute must be redeﬁned via word-level features.In weighted decoding, on the tth step of decod-ing, a partial hypothesis y<t = y1, . . . , yt−1 is ex-panded by computing the score for each possiblenext word w in the vocabulary:score(w, y<t; x) = score(y<t; x)+ log PRNN(w|y<t, x) +Xiwi ∗fi(w; y<t, x).Here, log PRNN(w|y<t, x) is the log-probability ofthe word w calculated by the RNN, score(y<t; x)is the accumulated score of the already-generatedwords in the hypothesis y<t, and fi(w; y<t, x)are decoding features with associated weights wi.There can be multiple features fi (to control mul-tiple attributes), and the weights wi are hyperpa-rameters to be chosen.A decoding feature fi(w; y<t, x) assigns a realvalue to the word w, in the context of the text gen-erated so far y<t and the context x. The featurecan be continuous (e.g. the unigram probability ofw), discrete (e.g. the length of w in characters),or binary (e.g.whether w starts with the sameletter as the last word in y<t). A positive weightwi increases the probability of words w that scorehighly with respect to fi; a negative weight de-creases their probability.Note that weighted decoding and conditionaltraining can be applied simultaneously (i.e. traina CT model then apply WD at test time) – a strat-egy we use in our experiments.6Controlling conversational attributesIn this section, we describe how we use condi-tional training and weighted decoding to controlfour attributes: repetition, speciﬁcity, response-relatedness and question-asking. We evaluate theeffectiveness of both control methods via auto-matic metrics (i.e., measuring how well the at-tribute was controlled), and use our ﬁndings to se-lect control methods and control settings to be ex-plored further via human evaluation (Section 8).6.1RepetitionOur baseline model exhibits three types of rep-etition, which we call external repetition (self-repetition across utterances), internal repetition(self-repetition within utterances), and partnerrepetition (repeating the conversational partner).To control repetition with weighted decod-ing,3we deﬁne ﬁve n-gram based decodingfeatures (see Appendix D). Three of thesefeatures (extrep bigram,intrep bigram andpartnerrep bigram) identify repeating bigramsfor the three repetition types.The other twofeatures (extrep unigram and intrep unigram)identify repeating content words. By applying anegative weight to these features, we can reducerepetition. In particular, if the weight is −∞, ourmethod is equivalent to n-gram blocking as de-scribed by Kulikov et al. (2018). We observe thatrepetition control is very important, thus all furthercontrol experiments include repetition control.6.2SpeciﬁcityLike many sequence-to-sequence models usingbeam search decoding, our baseline frequentlyasks generic questions such as What music do youlike? and gives dull, unspeciﬁc responses, such asI like all kinds of music.We control speciﬁcity using Normalized InverseDocument Frequency (NIDF) as a measure ofword rareness.4 The Inverse Document Frequencyof a word w is IDF(w) = log(R/cw) where Ris the number of responses in the dataset, and cwis the number of those responses that contain w.Normalized IDF (which ranges from 0 to 1) isNIDF(w) = IDF(w) −min idfmax idf −min idf(1)where min idf and max idf are the minimum andmaximum IDFs, taken over all words in the vo-cabulary. To control speciﬁcity with weighted de-coding, we use NIDF as a decoding feature. Asshown in Table 1, this method produces reason-able outputs when the feature weight is within acertain range, but at the extremes the outputs are3We also tried controlling repetition with conditionaltraining, deﬁning z as the (bucketed) maximum ROUGE-Lprecision between the response y and the bot’s previous ut-terances. However, this method was unsuccessful becausethere are not enough repetitive examples in the training datafor the model to learn the control. Experimenting with dataaugmentation to solve this problem is an area for future work.4Note that our NIDF speciﬁcity features are similar to theNIRF and NIWF features used by Zhang et al. (2018a).\f1706Input: Yes, I’m studying law at the momentBaseline Response: That sounds like a lot of fun!WtNIDFWeighted Decoding Response-5.00.6%Oh......................................0.017.1%That sounds like a lot of fun!3.018.3%That sounds like a lot of fun. Howlong have you been studying?7.038.5%I majored in practising myspiritual full time philosophy test10.071.9%Oh wow! Merna jean isa paino yihao hui bu acara sya gila [...]zNIDFConditional Training Response016.8%Sounds like you are a great person!218.3%So you are a law student?418.4%That sounds like a lot of fun622.8%That sounds like a rewarding job!824.4%That sounds like a rewarding career!Table 1: Middle: Example of controlling speciﬁcity(NIDF) via weighted decoding. At the extremes, themodel produces only the most rare or the most com-mon tokens. Bottom: Example of controlling speci-ﬁcity via conditional training. This gives a narrowerNIDF range, but all the responses are appropriate.nonsensical. The boundary for nonsensical outputdiffers from example to example.To control speciﬁcity with conditional training,we deﬁne the speciﬁcity of an utterance y to bethe mean NIDF of the words in y. Thus our con-trol variable z is mean NIDF (discretized into 10equal-sized buckets). As shown in Table 1, thismethod gives outputs with a narrower NIDF range,but overall produces less nonsensical outputs.6.3Response-relatednessIn conversation, it’s generally desirable to producea response that is related to the partner’s last utter-ance; for example if the partner says My grandfa-ther died last month, it is appropriate to say I’m sosorry. Were you close to your grandfather? How-ever, our baseline model frequently responds withunrelated utterances like Do you have any pets?To control response-relatedness with weighteddecoding, we use the decoding feature resp rel:resp rel(w; y<t, x) =cos sim(word emb(w), sent emb(ℓ))where word emb(w) is the GloVe embedding forthe word w, sent emb(ℓ) is the sentence embed-ding for the partner’s last utterance ℓ(note ℓis partof the context x), and cos sim is the cosine simi-larity between the two. In particular, the sentenceembedding sent emb(s) for an utterance s is aInput: Do you go get coffee oftenBaseline Response: I do, when I am not playing the piano.WtSimWeighted Decoding Response-10.0-0.05I am a musician.0.0-0.02I do, when I am not playing the piano.5.00.40I do, usually at starbucks.8.00.59Not usually, especially when you drink latte.11.00.72Not often, usually with drinks,espresso, latte, tea, etc.Table 2: Example of controlling response-relatedness(cosine similarity to input) via weighted decoding. Pos-itive weights (e.g.5.0) can yield more on-topic re-sponses, but higher weights (e.g. 11.0) can result innonsensical lists of topically related words.weighted average of the GloVe embeddings of thewords in s, with the ﬁrst principal component pro-jected out; for full details, see Arora et al. (2017).This method of controlling response-relatedness issimilar to that described in (Baheti et al., 2018).We ﬁnd that weighted decoding is effective to con-trol the semantic relatedness of the model’s re-sponse to the partner’s last utterance (see Table 2).As before, we ﬁnd that extreme weights lead tononsensical output.To control response-relatedness with condi-tional training, we try deﬁning the control vari-able z to be cos sim(sent emb(y), sent emb(ℓ)),the overall cosine similarity between the partner’slast utterance ℓand the model’s response y (again,we discretize z). However, we ﬁnd this method in-effective – the CT model learns only a very weakconnection between z and the semantic related-ness of the output (see Section 7 for more details).6.4Question-askingConsiderate chitchat requires a reciprocal askingand answering of questions – asking too few or toomany can appear self-centered or nosy. We controlquestion-asking in order to study these trade-offs.To control question-asking with weighted de-coding,we use the binary decoding featureis qn word(w), which is equal to 1 if and onlyif the word w is in a pre-deﬁned list of interrog-ative words (how, what, when, where, which, who,whom, whose, why, ?). We ﬁnd this is a somewhateffective method to encourage or discourage ques-tions, but with unintended side-effects: a negativeweight can discourage valid non-question utter-ances that happen to contain interrogative words(such as I’m learning how to knit) and a positiveweight can result in degenerate utterances (such as\f170701234567891010 (boost)Question-Asking Control Level (CT)0%20%40%60%80%100%% Utterances containing '?'Question-controlled CTQuestion-controlled CT w/ rep ctrlTarget for question-controlled CTBeam search baselineRepetition-controlled baselineGold dataFigure 2: Controlling question-asking via conditionaltraining. Exact numbers can be found in Appendix F.What??????? or Who? When? How?).For conditional training, we regard an utterancey as containing a question if and only if y con-tains a question mark.We train our CT modelon a control variable z with 11 possible values:{0, . . . , 10}. As discussed in Section 5, we wishto control question-asking at the distributional, di-alogue level, rather than at the binary, utterancelevel. Thus the setting z = i means that the modelshould produce, on average, utterances contain-ing ‘?’with probability i/10.During trainingwe randomly assign examples to buckets such thateach bucket i is trained on examples with the cor-rect proportion of questions (i/10), and all bucketshave the same amount of training examples.We ﬁnd that conditional training is effective tocontrol question-asking – as shown in Figure 2,by increasing z from 0 to 10, we obtain a rangeof question-asking rates from 1.40% to 97.72%.However, when we introduce repetition control,question-asking is reduced – in particular, the z =10 setting (which should produce 100% questions)now only produces 79.67% questions. The pri-mary problem is the weighted decoding featureextrep bigram, which discourages bigrams thathave appeared in previous utterances – this pre-vents the model from producing bigrams that com-monly occur in many questions, such as do youand what is. To ﬁx this, we introduce an extrasetting z = 10 (boost), in which we do not usethe feature extrep bigram for weighted decodingduring beam search, but we do use it to rerank thecandidates after beam search. This setting, whichallows the model to produce necessary question-asking bigrams, yields a 99.54% question-askingrate, at the cost of slightly increased external bi-gram repetition (see Appendix F).For controlling question-asking, conditionaltraining is preferable to weighted decoding for tworeasons. Firstly, it allows us to achieve (close to)0% questions, 100% questions, or anything in be-tween, without introducing the risk of degenerateoutput.Secondly, presence-of-a-question-markcaptures the true attribute of interest (question-asking) more exactly and directly than presence ofinterrogative words. For these reasons, only theCT method is considered in the human evaluation.7Comparison of control methodsThe previous section shows that conditional train-ing and weighted decoding are both useful tech-niques, with different strengths and weaknesses.The primary disadvantage of conditional train-ing is that it sometimes fails to learn the connec-tion between the control variable z and the targetoutput y. In practice, we ﬁnd the model can learnsimple attributes of the output (such as the pres-ence of ‘?’, and overall genericness), but not re-lationships between the input and output (such assemantic relatedness). By contrast, weighted de-coding can force the desired feature to appear inthe output by raising the weight arbitrarily high(though this may have unintended side-effects).The primary disadvantage of weighted decod-ing is that it risks going off-distribution whenthe weight is too strong.By contrast, condi-tional training produces mostly well-formed, in-distribution outputs.This highlights the impor-tance of learned control – it is safer to learn toproduce output that both satisﬁes the control vari-able and is appropriate, than to alter the decodingprocess to satisfy the control variable, potentiallytrading off appropriateness in the process.Other considerations include: (1) Convenience:conditional training requires retraining; weighteddecoding doesn’t, but is slower at test time. (2)Data availability:conditional training requirestraining examples of the controllable attribute,whereas weighted decoding can control any com-putable feature without requiring examples. (3)Attribute deﬁnition: conditional training can con-trol sentence-level attributes, but they must be dis-crete.By contrast, weighted decoding requiresword-level features, but they can be continuous.8Human evaluation resultsIn order to study the effect of our controllable at-tributes, we conduct a large-scale human evalua-\f1708tion of 28 model conﬁgurations (see Appendix E),plus human-human conversations for comparison.ApproachIn our evaluation, a crowdworkerchats with a model (or in the human-humancase, another crowdworker) for six conversationalturns, then answers eight multiple-choice ques-tions which each capture different aspects of con-versational quality: avoiding repetition, interest-ingness, making sense, ﬂuency, listening, inquisi-tiveness, humanness and engagingness. The eightquestions are Likert questions on a 1-4 scale,where higher is better.5 To match the ConvAI2Challenge, we also add a persona retrieval ques-tion, in which the crowdworker is asked to selectwhich of two possible personas was the model’spersona. For full details of the evaluation design,see Appendix B.Our evaluation is the same as the ConvAI2Challenge evaluation, but more detailed – Con-vAI2 includes only engagingness and persona re-trieval.6As in the ConvAI2 challenge, each ofour 28 model conﬁgurations was evaluated by over100 crowdworkers, and the results were adjustedfor annotator variance via a Bayesian calibration(Kulikov et al., 2018).In designing our evaluation, we aimed to cap-ture the four aspects we expected to directly im-prove via control (avoiding repetition, interesting-ness, listening, inquisitiveness), two important er-ror classes we thought would be affected by ourcontrols (ﬂuency, making sense), and two overallquality measures (engagingness, humanness).8.1Main ﬁndingsIn this section we summarize the main ﬁndings ofour human evaluation – whose full results can befound in Appendices G and H, with sample con-versations in Appendix C.As Figure 3 shows, controlling for repetition,speciﬁcity and question-asking all lead to large5Exceptions: Avoiding repetition is a 1-3 scale, as wefound this gave clearer instructions. Inquisitiveness has anoptimal score of 3; 1 and 2 represent too little question-asking, and 4 represents too much.6There are three other minor differences between ourevaluation and ConvAI2’s: (1) We ﬁx capitalization and spac-ing before showing the chatbot’s utterances to crowdwork-ers, while ConvAI2 show the raw lowercase tokenized form.We found the latter interferes with ﬂuency evaluation. (2)We conduct 6 dialogue turns, while ConvAI2 conducts 4-6.This was necessary to evaluate repetitiveness. (3) We use(publicly-available) validation set personas, while ConvAI2uses (hidden) test set personas. This enables us to release ourevaluation chatlogs.engagingness improvements over the greedy andbeam-search baseline models.In particular, weﬁnd that controlling for multi-turn (self) repetitionis important and should be incorporated alongsideother attribute control methods. We found no im-provement by controlling response-relatedness.To better understand these overall engagingnessimprovements, we consider the full set of humanjudgments, shown in Figure 4. We ﬁnd that re-ducing repetition leads to improvements across allour aspects of conversational quality. Increasingspeciﬁcity shows improvements in interestingnessand listening ability over the repetition-controlledbaseline, while increasing question-asking showsimprovements in inquisitiveness and interesting-ness over the repetition-controlled baseline.Our most engaging model, which controls bothrepetition and question-asking – marked ‘Ques-tion (CT)’ in Figure 3 (left) – matches the en-gagingness of the winning entry in the ConvAI2competition, as both models achieve a raw score7of 3.1 (Dinan et al., 2019). However, the Con-vAI2 winner, Lost in Conversation, was trainedon approximately 12× as much data as our model.Lost in Conversation is based on the OpenAI GPTLanguage Model (Radford et al., 2018), which ispretrained on the BookCorpus (Zhu et al., 2015),which contains approximately 985 million words,whereas our model is pretrained on the Twitterdataset (approximately 79 million words).Altogether, our evaluation clearly shows thatcontrolling low-level attributes over multiple turnsleads to improved overall quality.8.2Effect of controlled attributesRepetition(WD)Weobservethatself-repetition across utterances (external repetition)is by far the most severe form of repetition inour beam search baseline model.We evaluateseveral settings of the extrep bigram weighteddecoding feature, and ﬁnd that an aggressiverepetition-reductionsetting(reducingbigramrepetition rate to below gold data levels) is ratedbest. We also ﬁnd that blocking repeated contentwords improves the avoiding repetition score. SeeAppendices E, F and G for full details.As shown in Figure 3 (left) and Figure 4,our repetition-controlled model improves hugely7Although the same Bayesian calibration method was ap-plied both in our study and in the ConvAI2 competition, cal-ibrated scores are not comparable across the two; thus wecompare raw scores (viewable in Table 7).\f1709GreedyBeam searchRepetition (WD)Specificity (WD)Question (CT)Human2.22.42.62.83.03.2Engagingness-10MoreGeneric-8-6-4-20No control246810MoreSpecificSpecificity Control Level (WD)2.22.42.62.83.03.2EngagingnessSpecificity-controlled WDBeam search baselineHumanRepetition-controlled baseline0FewerQuestions1234567891010 (boost)MoreQuestionsQuestion-Asking Control Level (CT)2.22.42.62.83.03.2EngagingnessQuestion-controlled CTBeam search baselineHumanRepetition-controlled baselineFigure 3: Calibrated human judgments of engagingness for the baselines and best controlled models (left); fordifferent speciﬁcity control settings (middle); and for different question-asking control settings (right).2.002.252.502.753.00Avoiding Repetition2.252.502.753.00Interestingness3.23.43.63.8Making Sense3.03.23.43.6Fluency2.502.753.003.253.50Listening1.752.002.252.502.75Inquisitiveness2.02.53.03.5HumannessGreedy searchBeam searchRepetition-controlled (WD)Specificity-controlled (WD)Question-controlled (CT)HumanFigure 4: Calibrated human judgments of conversational aspects for the baselines and best controlled models.Note: In Figure 3 and here, the Speciﬁcity and Question controlled models both include Repetition control, butQuestion control doesn’t include Speciﬁcity control, or vice versa.over the beam search baseline in all metrics, andachieves close-to-human scores on all metrics ex-cept humanness. This striking result demonstratesthat repetition is by far the biggest limiting qual-ity factor for naive sequence-to-sequence dialogueagents. The result also emphasizes the importanceof multi-turn dialogue evaluation to detect theproblem. We refer to this model as the repetition-controlled baseline, and use it as a basis for all re-maining experiments (i.e., we control speciﬁcity,response-relatedness and question-asking on topof these repetition-control settings).Speciﬁcity (WD, CT)For our weighted decod-ing models, the extreme settings (very generic andvery speciﬁc) score poorly in engagingness due tothe frequent presence of degenerate output – seeFigure 3 (middle). We ﬁnd that the weight = 4setting (which is more speciﬁc than the repetition-controlled baseline and about as speciﬁc as thegold data) maximizes engagingness.As shownin Figure 3 (left) and Figure 4, this more-speciﬁcmodel is rated more interesting, engaging, and abetter listener than the repetition-controlled base-line, but at the cost of reduced ﬂuency and makingsense. Our CT model with z = 7 (which has asimilar NIDF level as WD with weight = 4) showssimilar results, but the improvements are smaller.For further discussion on the interestingness of ourspeciﬁcity models, see Section 8.3.Response-relatedness (WD)We evaluated sev-eral control settings (weight = −10, 5, 10, 13) andfound that none scored better than weight = 0(no response-relatedness control); see AppendixH. This is surprising – prior to running the humanevaluation, we annotated 100 examples ourselvesto determine the best control settings. While weidentiﬁed a more responsive setting (weight = 5)as less likely than the uncontrolled model to ig-nore the user, crowdworkers rated it as a slightlyworse listener than the uncontrolled model. Oneexplanation for this discrepancy is that the moreresponsive model takes more risks, using morerare words (0.197 NIDF, up from 0.178), and thusreceives a lower makes-sense score (3.41, downfrom 3.70). We hypothesize that, compared to us,the crowdworkers are less tolerant of slightly non-sensical output, and more tolerant of generic unre-lated utterances.Question-asking (CT)As shown in Figure 3(right), a question-asking rate of 65.7% (z = 7)maximizes engagingness. This setting, which asksmore questions than both the repetition-controlledbaseline (50.0%) and the human-produced golddata (28.8%), brings us closest to human-level en-gagingness – see Figure 3 (left).Although weﬁnd that a rate of approximately 65.7% question-asking is the most engaging, a lower level (48.9%,or z = 4) is rated the best listener. Lastly, we ﬁnd\f1710ModelWin%Top 3 reasons for preferring modelSpeciﬁcity WD (weight = 6)84.1%More information; Better ﬂow; More descriptiveSpeciﬁcity WD (weight = 4)75.5%More information; They describe their life in more detail; FunnySpeciﬁcity CT (z = 7)56.2%More information; Better ﬂow; Seems more interestedTable 3: A/B tests comparing various speciﬁcity-controlled models to the repetition-controlled baseline on inter-estingness. We ﬁnd all comparisons are signiﬁcant (p < .05; binomial test).that although asking too many questions is less en-gaging, most crowdworkers will not directly criti-cize a chatbot that asks questions on every turn –only 11.9% of crowdworkers judged the z = 10(boost) setting, which asks 99.5% questions, asasking too many questions.8For full details ofthese scores, see Appendix F and H.For time and budget reasons, we did not eval-uate any models controlling both question-askingand speciﬁcity. However, we expect it is possibleto obtain further improvements by doing so.8.3A/B tests for interestingnessThough our more-speciﬁc models yielded signiﬁ-cant improvements in engagingness, we were sur-prised that they did not yield clearer improve-ments in interestingness. To investigate further,we conducted an A/B interestingness evaluation ofthree speciﬁcity-controlled models, compared tothe repetition-controlled baseline. Crowdworkerswere shown two conversations (from the main hu-man evaluation) and asked to choose which modelwas more interesting (see Figure 7 for details). Wecollected 500 samples per comparison, plus 200additional human vs repetition-controlled baselinesamples, which were used to ﬁlter for quality con-trol. After discarding low-quality crowdworkers,we have roughly 300 evaluations per comparison,with an average Cohen’s κ = 0.6.As shown in Table 3, all three models were ratedsigniﬁcantly more interesting than the repetition-controlled baseline. This convincingly shows thatproducing utterances with more rare words is avalid strategy to improve interestingness. We havetwo explanations for why these interestingness dif-ferences did not materialize in our main evalua-tion. Firstly, interestingness is a particularly sub-jective metric (unlike more tangible metrics suchas avoiding repetition and making sense) – thismakes it hard to calibrate across crowdworkers.8Though this conclusion may hold true for the Per-sonaChat task – a synthetic chatting task that instructs par-ticipants to get to know each other – in real-life social con-versations, incessant question-asking may be less tolerated.Secondly, we suspect that in our original evalu-ation, the crowdworkers may have evaluated theinterestingness of the task rather than the chat-bot. This could account for why subtle increasesin conversational ability did not result in higher in-terestingness ratings – the PersonaChat task itselfhas a natural interestingness limit.9ConclusionWhat makes a good conversation?Throughour evaluation, we showed that a good conversa-tion is about balance – controlling for the rightlevel of repetition, speciﬁcity and question-askingis important for overall quality.We also foundthat conversational aspects such as interestingness,listening, and inquisitiveness are all important –though optimizing these can introduce a trade-offagainst certain types of errors (such as repetitive,disﬂuent, or nonsensical output). Secondly, multi-turn evaluation is essential to study what makes agood conversation – multiple turns are required toreveal issues such as repetition, consistency, andquestion-asking frequency.Lastly, what do wemean by ‘good’? Although humanness and engag-ingness are both commonly used as overall qual-ity metrics, the two are very different. While ourmodels achieved close-to-human scores on engag-ingness, they failed to get close on humanness –showing that a chatbot need not be human-liketo be enjoyable. This striking result also demon-strates the importance of measuring more than onequality metric when evaluating dialogue agents.OutlookOur work shows that neural generativesystems have systemic problems when applied toopen-ended dialogue, some of which (e.g. repe-tition) are only observable in the multi-turn set-ting. Furthermore, control of low-level attributesoffers a practical way to correct these problems,yielding large improvements to overall quality – inour case, comparable to systems trained on muchmore data. Future work includes optimizing con-trol settings automatically, and building more con-vincingly human-like chatbots.\f1711ReferencesSanjeev Arora, Yingyu Liang, and Tengyu Ma.2017. A simple but tough-to-beat baseline forsentence embeddings. In Proceedings of the In-ternational Conference on Learning Represen-tations (ICLR).Ashutosh Baheti, Alan Ritter, Jiwei Li, and BillDolan. 2018. Generating more interesting re-sponses in neural conversation models with dis-tributional constraints.In Proceedings of the2018 Conference on Empirical Methods in Nat-ural Language Processing, pages 3970–3980.Association for Computational Linguistics.Antoine Bordes, Y-Lan Boureau, and Jason We-ston. 2017. Learning end-to-end goal-orienteddialog. In Proceedings of the International Con-ference on Learning Representations (ICLR).Emily Dinan, Varvara Logacheva, Valentin Ma-lykh, Alexander Miller, Kurt Shuster, Jack Ur-banek, Douwe Kiela, Arthur Szlam, Iulian Ser-ban, Ryan Lowe, et al. 2019.The secondconversational intelligence challenge (convai2).arXiv preprint arXiv:1902.00098.Emily Dinan, Stephen Roller, Kurt Shuster, An-gela Fan, Michael Auli, and Jason Weston.2018.Wizard of Wikipedia:Knowledge-powered conversational agents. arXiv preprintarXiv:1811.01241.Layla El Asri, Hannes Schulz, Shikhar Sharma,Jeremie Zumer, Justin Harris, Emery Fine,Rahul Mehrotra, and Kaheer Suleman. 2017.Frames:a corpus for adding memory togoal-oriented dialogue systems.In Proceed-ings of the 18th Annual SIGDIAL Meetingon Discourse and Dialogue, pages 207–219,Saarbr¨ucken, Germany. Association for Com-putational Linguistics.Angela Fan, David Grangier, and Michael Auli.2018. Controllable abstractive summarization.In Proceedings of the 2nd Workshop on Neu-ral Machine Translation and Generation, pages45–54. Association for Computational Linguis-tics.Jessica Ficler and Yoav Goldberg. 2017. Control-ling linguistic style aspects in neural languagegeneration. In Proceedings of the Workshop onStylistic Variation, pages 94–104. Associationfor Computational Linguistics.Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi,and Kevin Knight. 2017. Hafez: an interactivepoetry generation system.In Proceedings ofACL 2017, System Demonstrations, pages 43–48. Association for Computational Linguistics.Fenfei Guo, Angeliki Metallinou, Chandra Kha-tri, Anirudh Raju, Anu Venkatesh, and AshwinRam. 2018. Topic-based evaluation for conver-sational bots. Advances in Neural InformationProcessing Systems, Conversational AI Work-shop.Helen Hastie. 2012.Metrics and evaluationof spoken dialogue systems, pages 131–150.Springer.Matthew Henderson, Blaise Thomson, and Ja-son D Williams. 2014. The second dialog statetracking challenge. In Proceedings of the 15thAnnual Meeting of the Special Interest Groupon Discourse and Dialogue (SIGDIAL), pages263–272.Zhiting Hu, Zichao Yang, Xiaodan Liang, RuslanSalakhutdinov, and Eric P Xing. 2017. Towardcontrolled generation of text. In Thirty-fourthInternational Conference on Machine Learn-ing.Yuta Kikuchi, Graham Neubig, Ryohei Sasano,Hiroya Takamura, and Manabu Okumura. 2016.Controlling output length in neural encoder-decoders.In Proceedings of the 2016 Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 1328–1338. Associa-tion for Computational Linguistics.Ilya Kulikov, Alexander H Miller, KyunghyunCho, and Jason Weston. 2018. Importance ofa search strategy in neural dialogue modelling.arXiv preprint arXiv:1811.00907.Jiwei Li, Michel Galley, Chris Brockett, Jian-feng Gao, and Bill Dolan. 2016a. A diversity-promoting objective function for neural conver-sation models. In Proceedings of the 2016 Con-ference of the North American Chapter of theAssociation for Computational Linguistics: Hu-man Language Technologies, pages 110–119.Association for Computational Linguistics.Jiwei Li, Will Monroe, and Dan Jurafsky. 2017a.Learning to decode for future success.arXivpreprint arXiv:1701.06549.Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,Michel Galley, and Jianfeng Gao. 2016b. Deep\f1712reinforcement learning for dialogue generation.In Proceedings of the 2016 Conference on Em-pirical Methods in Natural Language Process-ing, pages 1192–1202, Austin, Texas. Associa-tion for Computational Linguistics.Jiwei Li, Will Monroe, Tianlin Shi, S´ebastienJean, Alan Ritter, and Dan Jurafsky. 2017b. Ad-versarial learning for neural dialogue genera-tion. arXiv preprint arXiv:1701.06547.Chia-WeiLiu,RyanLowe,IulianSerban,Mike Noseworthy, Laurent Charlin, and JoellePineau. 2016.How not to evaluate your dia-logue system: An empirical study of unsuper-vised evaluation metrics for dialogue responsegeneration. pages 2122–2132.Ryan Lowe, Michael Noseworthy, Iulian VladSerban,Nicolas Angelard-Gontier,YoshuaBengio, and Joelle Pineau. 2017. Towards anautomatic turing test: Learning to evaluate di-alogue responses. In Proceedings of the 55thAnnual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers),pages 1116–1126. Association for Computa-tional Linguistics.Ryan Lowe, Nissan Pow, Iulian Serban, and JoellePineau. 2015. The Ubuntu dialogue corpus: Alarge dataset for research in unstructured multi-turn dialogue systems.In Proceedings of the16th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, pages 285–294, Prague, Czech Republic. Association forComputational Linguistics.Alexander Miller, Will Feng, Dhruv Batra, An-toine Bordes, Adam Fisch, Jiasen Lu, DeviParikh, and Jason Weston. 2017.ParlAI: Adialog research software platform.In Pro-ceedings of the 2017 Conference on Empiri-cal Methods in Natural Language Processing:System Demonstrations, pages 79–84, Copen-hagen, Denmark. Association for Computa-tional Linguistics.Jekaterina Novikova, Ondˇrej Duˇsek, Amanda Cer-cas Curry, and Verena Rieser. 2017. Why weneed new evaluation metrics for nlg. In Pro-ceedings of the 2017 Conference on Empiri-cal Methods in Natural Language Processing,pages 2241–2252.Prasanna Parthasarathi and Joelle Pineau. 2018.Extendingneuralgenerativeconversationalmodel using external knowledge sources.InProceedings of the 2018 Conference on Empir-ical Methods in Natural Language Processing,pages 690–695, Brussels, Belgium. Associationfor Computational Linguistics.Nanyun Peng, Marjan Ghazvininejad, JonathanMay, and Kevin Knight. 2018. Towards con-trollable story generation.In Proceedings ofthe First Workshop on Storytelling, pages 43–49. Association for Computational Linguistics.Jeffrey Pennington, Richard Socher, and Christo-pher Manning. 2014. GloVe: Global vectors forword representation. In Proceedings of the 2014Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computa-tional Linguistics.Alec Radford, Karthik Narasimhan, Tim Sali-mans, and Ilya Sutskever. 2018. Improving lan-guage understanding by generative pre-training.Iulian Vlad Serban, Ryan Lowe, Laurent Charlin,and Joelle Pineau. 2016a. Generative deep neu-ral networks for dialogue: A short review. Ad-vances in Neural Information Processing Sys-tems workshop on Learning Methods for Dia-logue.Iulian Vlad Serban, Alessandro Sordoni, YoshuaBengio, Aaron C Courville, and Joelle Pineau.2016b. Building end-to-end dialogue systemsusing generative hierarchical neural networkmodels. In AAAI, volume 16, pages 3776–3784.Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, ShuziNiu, Yang Zhao, Akiko Aizawa, and GuopingLong. 2017. A conditional variational frame-work for dialog generation.In Proceedingsof the 55th Annual Meeting of the Associationfor Computational Linguistics (Volume 2: ShortPapers), pages 504–509. Association for Com-putational Linguistics.Anu Venkatesh, Chandra Khatri, Ashwin Ram,Fenfei Guo, Raefer Gabriel, Ashish Nagar, Ro-hit Prasad, Ming Cheng, Behnam Hedayatnia,Angeliki Metallinou, et al. 2017. On evaluat-ing and comparing conversational agents. Ad-vances in Neural Information Processing Sys-tems, Conversational AI Workshop.Oriol Vinyals and Quoc Le. 2015. A neural con-versational model. In Proceedings of the 31stInternational Conference on Machine Learning,Deep Learning Workshop, Lille, France.\f1713Marilyn A. Walker, Diane J. Litman, Candace A.Kamm, and Alicia Abella. 1997. PARADISE:A framework for evaluating spoken dialogueagents. In Proceedings of the 35th Annual Meet-ing of the Association for Computational Lin-guistics, pages 271–280, Madrid, Spain. Asso-ciation for Computational Linguistics.Di Wang, Nebojsa Jojic, Chris Brockett, and EricNyberg. 2017. Steering output style and topicin neural response generation. In Proceedingsof the 2017 Conference on Empirical Methodsin Natural Language Processing, pages 2140–2150. Association for Computational Linguis-tics.Tsung-HsienWen,DavidVandyke,NikolaMrkˇsi´c, Milica Gasic, Lina M. Rojas Barahona,Pei-Hao Su, Stefan Ultes, and Steve Young.2017.A network-based end-to-end trainabletask-oriented dialogue system. In Proceedingsof the 15th Conference of the European Chapterof the Association for Computational Linguis-tics: Volume 1, Long Papers, pages 438–449.Association for Computational Linguistics.Chen Xing, Wei Wu, Yu Wu, Jie Liu, YalouHuang, Ming Zhou, and Wei-Ying Ma. 2017.Topic aware neural response generation.InAAAI, volume 17, pages 3351–3357.Ruqing Zhang, Jiafeng Guo, Yixing Fan, YanyanLan, Jun Xu, and Xueqi Cheng. 2018a. Learn-ing to control the speciﬁcity in neural responsegeneration. In Proceedings of the 56th AnnualMeeting of the Association for ComputationalLinguistics (Volume 1:Long Papers), pages1108–1117, Melbourne, Australia. Associationfor Computational Linguistics.Saizheng Zhang, Emily Dinan, Jack Urbanek,Arthur Szlam, Douwe Kiela, and Jason Weston.2018b. Personalizing dialogue agents: I have adog, do you have pets too? In Proceedings ofthe 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Pa-pers), pages 2204–2213, Melbourne, Australia.Association for Computational Linguistics.Ganbin Zhou, Ping Luo, Rongyu Cao, Fen Lin,Bo Chen, and Qing He. 2017.Mechanism-aware neural machine for dialogue responsegeneration. In AAAI, pages 3400–3407.Yukun Zhu, Ryan Kiros, Rich Zemel, RuslanSalakhutdinov, Raquel Urtasun, Antonio Tor-ralba, and Sanja Fidler. 2015. Aligning booksand movies: Towards story-like visual explana-tions by watching movies and reading books. InProceedings of the IEEE international confer-ence on computer vision, pages 19–27.\f1714Supplementary MaterialAScreenshots of human evaluation interfaceFigure 5: Screenshot of the Task DescriptionFigure 6: Screenshot of the chat UI, talking with the beam search baseline model.\f1715Figure 7: Screenshot of the A/B test UI, comparing a human-human conversation (left) and a Repetition-controlledbaseline model (right).\f1716BHuman evaluation questionnaire designHere are the questions and multiple-choice options used in the human evaluation, in the order presented:[Engagingness] How much did you enjoy talking to this user?• Not at all • A little • Somewhat • A lot[Interestingness] How interesting or boring did you ﬁnd this conversation?• Very boring • A little boring • A little interesting • Very interesting[Inquisitiveness] How much did the user try to get to know you?• Didn’t ask about me at all • Asked about me some• Asked about me a good amount • Asked about me too much[Listening] How much did the user seem to pay attention to what you said?• Always ignored what I said • Mostly ignored what I said• Mostly paid attention to what I said • Always paid attention to what I said[Avoiding Repetition] How repetitive was this user?• Repeated themselves over and over • Sometimes said the same thing twice• Always said something new[Fluency] How naturally did this user speak English?• Very unnatural • Mostly unnatural • Mostly natural • Very natural[Making sense] How often did this user say something which did NOT make sense?• Never made any sense • Most responses didn’t make sense• Some responses didn’t make sense • Everything made perfect sense[Humanness] Do you think this user is a bot or a human?• Deﬁnitely a bot • Probably a bot • Probably a human • Deﬁnitely a human[Persona retrieval] Which prompt (character) do you think the other user was givenfor this conversation?Respondent chooses one of two provided personas\f1717CExample conversations from human evaluationHuman: What do you do for a living?Bot: I'm a buddhist. What about you?Human: I'm a Christian. Do you have a job?Bot: I am a buddhist. What about you?Human: I work on my parent's farm most days. Do you have a job?Bot: I am a buddhist. What about you?Human: Do you like music?Bot: I do like music. What about you?Human: Yes, but only country.Bot: What city are you from?Human: I live in the country, a very small town called Ila.Bot: What city are you from?Human: Hey there! I'm a little girl :) how are you?Bot: I am doing well, how are you?Human: I'm very lonely actually. I live in the cloud.Bot: What do you do for a living?Human: Nothing really, I'd like to say superhero since I have superpowers.Bot: That sounds like a lot of fun!Human: Yes! Do you like to shop?Bot: I do! I love to shop!Human: I always make time to shop!Bot: What is your favorite food?Human: Pizza I would have to say. What about you?Bot: Chocolate is one of my favorites!(a)(b)Bot: Hello, how are you today?Human: Hi! I'm great, how are you?Bot: I am doing well, thank you. What do you do for fun?Human: I don't have time for too much, I'm either working or spending time with mykitties.Bot: What kind of pets? I walk dogs for a living.Human: That must be a fun job.Bot: It is, but I spend most of my time reading.Human: What kind of books do you like?Bot: My favorite is harry potter. How about you?Human: I like legal and psychological thrillers.Bot: Wow, that is a lot of books.Human: WHat do you do for work? I'm in construction.Human: Hello, how are you doing today?Bot: I am doing well. Just playing some piano and singing along to folk music.Human: That sounds fun. What type of music do you like?Bot: I hope to make some musical songs for my band. Do you play anyinstruments?Human: Nope, I was raised in a single parent household so never really had muchmoney to spend.Bot: I wish I could spend more time with my family. I work as an auditor.Human: That's awesome. I work two jobs out of which one of them is as stuntdouble.Bot: That sounds like a lot of fun.Human: Well yeah it is, but comes with its own cons as well.Bot: What is your favorite thing to do?Human: Reading. I read a lot. Last year I read a total of 20 books.Bot: What kind of books are you reading?(c)(d)Figure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT(z = 7), (d) Speciﬁcity-controlled WD (weight = 4).\f1718DRepetition-control decoding featuresFeatureConditionextrep bigram(w, y<t, x)Adding w to the hypothesis y<t would create a 2-gramthat appears in a previous utterance by the modelextrep unigram(w, y<t, x)w is a non-stopword andw appears in a previous utterance by the modelintrep bigram(w, y<t, x)Adding w to the hypothesis y<t would create a 2-gramthat appears earlier in the hypothesis y<tintrep unigram(w, y<t, x)w is a non-stopword andw appears earlier in the hypothesis y<tpartnerrep bigram(w, y<t, x)Adding w to the hypothesis y<t would create a 2-gramthat appears in a previous utterance by the partnerTable 4: We deﬁne ﬁve binary features for controlling different types of repetition via weighted decoding (seeSection 5.2). Each feature depends on the word w, the partial hypothesis y<t, and the context x (which includesthe model’s own persona and the dialogue history). Each of these features is equal to 1 if and only if the conditionon the right is true; otherwise 0.\f1719EControl settings for all conﬁgurationsRepetitionSpeciﬁcityResponse-relQuestionsExternalInternalPartner Rep.BigramUnigramBigramUnigramBigramNIDFCos simHas ‘?’BaselinesGreedy SearchBeam Search (beam size 20)Repetition control (WD)Extrep bigram WD -0.5wt -0.5Extrep bigram WD -1.25wt -1.25Extrep bigram WD -3.5wt -3.5Extrep bigram WD -infwt -∞Repetition-controlled baselinewt -3.5wt -∞wt -∞Question control (CT)Question-controlled CT 0wt -3.5wt -∞wt -∞z = 0Question-controlled CT 1wt -3.5wt -∞wt -∞z = 1Question-controlled CT 4wt -3.5wt -∞wt -∞z = 4Question-controlled CT 7wt -3.5wt -∞wt -∞z = 7Question-controlled CT 10wt -3.5wt -∞wt -∞z = 10Question-controlled CT 10 (boost)wt 0 *wt -∞wt -∞z = 10Speciﬁcity control (CT)Speciﬁcity-controlled CT 0wt -3.5wt -∞wt -∞z = 0Speciﬁcity-controlled CT 2wt -3.5wt -∞wt -∞z = 2Speciﬁcity-controlled CT 4wt -3.5wt -∞wt -∞z = 4Speciﬁcity-controlled CT 7wt -3.5wt -∞wt -∞z = 7Speciﬁcity-controlled CT 9wt -3.5wt -∞wt -∞z = 9Speciﬁcity control (WD)Speciﬁcity-controlled WD -10wt -3.5wt -∞wt -∞wt -10Speciﬁcity-controlled WD -4wt -3.5wt -∞wt -∞wt -4Speciﬁcity-controlled WD 4wt -3.5wt -∞wt -∞wt 4Speciﬁcity-controlled WD 6wt -3.5wt -∞wt -∞wt 6Speciﬁcity-controlled WD 8wt -3.5wt -∞wt -∞wt 8Response-related control (WD) **Response-related controlled WD -10wt -3.5wt -∞wt -∞wt -∞wt -∞wt -10Response-related controlled WD 0wt -3.5wt -∞wt -∞wt -∞wt -∞wt 0Response-related controlled WD 5wt -3.5wt -∞wt -∞wt -∞wt -∞wt 5Response-related controlled WD 10wt -3.5wt -∞wt -∞wt -∞wt -∞wt 10Response-related controlled WD 13wt -3.5wt -∞wt -∞wt -∞wt -∞wt 13Table 5: Control settings for all conﬁgurations that were human-evaluated. ‘wt’ means the weight used for aweighted decoding feature and ‘z =’ means the setting (i.e. bucket) for the control variable in conditional training.* In the setting Question-controlled CT 10 (boost), the feature extrep bigram is not used for weighteddecoding during beam search, but it is used to rerank the candidates after beam search. See Section 6.4 for details.** Note that the Response-related controlled models additionally introduce repetition controls to block in-ternal bigram repetition and partner bigram repetition. This was necessary to prevent the model from parroting thepartner’s last utterance. In Table 8, we ﬁnd that just adding these extra repetition controls (here called Response-related controlled WD 0, i.e. increased repetition control but no response-relatedness control) outperforms ourcanonical Repetition-controlled baseline. However, given that we discovered this later, our speciﬁcity and questioncontrolled models are built on top of the canonical Repetition-controlled baseline.\f1720FAutomatic metrics for all conﬁgurationsRepetitionSpeciﬁcityResponse-relQuestionsExternalInternalPartner Rep.BigramUnigramBigramUnigramBigramNIDFCos simHas ‘?’Gold data and baselinesGold Data4.65%9.62%0.38%0.97%5.10%0.21190.169128.80%Greedy Search35.88%36.31%8.08%10.59%12.20%0.16880.18506.46%Beam Search (beam size 20)46.85%44.15%0.32%0.61%12.90%0.16620.095780.87%Repetition control (WD)Extrep bigram WD -0.519.70%16.85%0.26%0.62%11.93%0.17300.134873.04%Extrep bigram WD -1.254.62%4.79%0.40%0.89%10.61%0.17630.150461.22%Extrep bigram WD -3.50.75%4.61%0.47%0.94%9.89%0.17710.168148.89%Extrep bigram WD -inf0.00%4.74%0.51%1.05%9.56%0.17800.171145.98%Repetition-controlled baseline0.73%0.00%0.17%0.00%9.55%0.17660.167649.98%Question control (CT)Question-controlled CT 00.06%0.00%0.19%0.00%9.20%0.18710.17532.01%Question-controlled CT 10.09%0.00%0.19%0.00%8.66%0.18440.172217.33%Question-controlled CT 40.40%0.00%0.25%0.00%8.53%0.17940.171348.88%Question-controlled CT 70.80%0.00%0.17%0.00%8.48%0.17710.172465.65%Question-controlled CT 101.27%0.00%0.16%0.00%8.48%0.17610.172879.67%Question-controlled CT 10 (boost)*7.64%0.00%0.03%0.00%10.76%0.17010.165199.54%Speciﬁcity control (CT)Speciﬁcity-controlled CT 00.60%0.00%0.20%0.00%9.05%0.14780.152248.75%Speciﬁcity-controlled CT 20.28%0.00%0.10%0.00%8.37%0.17720.183350.57%Speciﬁcity-controlled CT 40.12%0.00%0.08%0.00%7.90%0.19210.187729.46%Speciﬁcity-controlled CT 70.02%0.00%0.14%0.00%8.17%0.21560.195516.51%Speciﬁcity-controlled CT 90.01%0.00%0.11%0.00%8.01%0.24620.19908.50%Speciﬁcity control (WD)Speciﬁcity-controlled WD -100.14%0.00%10.59%0.00%8.70%0.11070.099433.55%Speciﬁcity-controlled WD -40.65%0.00%1.98%0.00%9.95%0.15010.139844.92%Speciﬁcity-controlled WD 40.15%0.00%0.19%0.00%7.54%0.21210.197245.53%Speciﬁcity-controlled WD 60.07%0.00%0.13%0.00%6.50%0.25460.204039.37%Speciﬁcity-controlled WD 80.01%0.00%0.10%0.00%3.40%0.40350.143626.68%Response-related control (WD)Response-related controlled WD -100.13%0.00%0.00%0.00%0.00%0.1914-0.092125.71%Response-related controlled WD 00.24%0.00%0.00%0.00%0.00%0.17850.141444.55%Response-related controlled WD 50.15%0.00%0.00%0.00%0.00%0.19730.436039.78%Response-related controlled WD 100.05%0.00%0.00%0.00%0.00%0.25350.665327.56%Response-related controlled WD 130.02%0.00%0.00%0.00%0.00%0.29990.725120.47%Table 6: Automatic metrics (computed over validation set) for all model conﬁgurations that were human-evaluated.\f1721GHuman evaluation results for all conﬁgurationsModelAvoiding Rep.EngageFluencyHumannessInquisitiveInterestingListeningMake SensePersonaHuman and baselinesHuman2.90 ± 0.393.31 ± 0.903.66 ± 0.713.40 ± 0.802.63 ± 0.633.23 ± 0.833.64 ± 0.633.84 ± 0.520.92 ± 0.27Greedy Search2.16 ± 0.722.31 ± 1.083.20 ± 0.811.78 ± 0.902.00 ± 0.812.36 ± 0.982.78 ± 0.843.33 ± 0.750.87 ± 0.34Beam Search (beam size 20)2.14 ± 0.722.35 ± 1.013.23 ± 0.931.81 ± 0.872.50 ± 0.722.35 ± 0.982.63 ± 0.853.40 ± 0.770.77 ± 0.42Repetition control (WD)Extrep bigram WD -0.52.66 ± 0.562.56 ± 0.923.57 ± 0.642.19 ± 0.942.67 ± 0.622.61 ± 0.873.08 ± 0.783.60 ± 0.570.75 ± 0.43Extrep bigram WD -1.252.84 ± 0.392.91 ± 0.903.59 ± 0.642.32 ± 0.982.63 ± 0.602.86 ± 0.893.21 ± 0.713.64 ± 0.620.72 ± 0.45Extrep bigram WD -3.52.90 ± 0.302.95 ± 0.863.73 ± 0.502.45 ± 1.032.55 ± 0.612.88 ± 0.803.27 ± 0.793.68 ± 0.490.80 ± 0.40Extrep bigram WD -inf2.82 ± 0.432.96 ± 0.863.64 ± 0.582.40 ± 0.962.65 ± 0.692.86 ± 0.823.31 ± 0.693.66 ± 0.590.91 ± 0.29Repetition-controlled baseline2.89 ± 0.392.89 ± 0.893.66 ± 0.562.50 ± 0.992.70 ± 0.642.96 ± 0.923.25 ± 0.713.68 ± 0.540.87 ± 0.34Question control (CT)Question-controlled CT 02.95 ± 0.252.92 ± 0.903.70 ± 0.542.49 ± 0.972.48 ± 0.722.85 ± 0.933.29 ± 0.693.56 ± 0.660.86 ± 0.35Question-controlled CT 12.88 ± 0.332.94 ± 0.933.59 ± 0.662.47 ± 0.952.52 ± 0.692.85 ± 0.903.32 ± 0.733.63 ± 0.550.85 ± 0.36Question-controlled CT 42.88 ± 0.382.88 ± 0.943.59 ± 0.732.42 ± 1.072.55 ± 0.662.82 ± 0.853.37 ± 0.743.63 ± 0.590.84 ± 0.37Question-controlled CT 72.88 ± 0.373.07 ± 0.903.67 ± 0.542.42 ± 0.982.75 ± 0.582.97 ± 0.843.23 ± 0.763.53 ± 0.760.80 ± 0.40Question-controlled CT 102.74 ± 0.462.90 ± 0.933.70 ± 0.502.43 ± 1.042.71 ± 0.572.72 ± 0.883.12 ± 0.733.59 ± 0.660.79 ± 0.41Question-controlled CT 10 (boost)2.76 ± 0.492.84 ± 0.943.60 ± 0.642.26 ± 0.972.94 ± 0.572.83 ± 0.943.18 ± 0.803.52 ± 0.670.72 ± 0.45Speciﬁcity control (CT)Speciﬁcity-controlled CT 02.83 ± 0.402.96 ± 0.933.62 ± 0.582.42 ± 0.992.60 ± 0.562.86 ± 0.893.29 ± 0.703.66 ± 0.600.72 ± 0.45Speciﬁcity-controlled CT 22.90 ± 0.362.78 ± 1.003.60 ± 0.642.37 ± 0.932.66 ± 0.662.80 ± 0.963.14 ± 0.773.50 ± 0.630.81 ± 0.39Speciﬁcity-controlled CT 42.92 ± 0.272.81 ± 0.883.65 ± 0.592.34 ± 1.022.57 ± 0.622.80 ± 0.783.25 ± 0.783.50 ± 0.660.86 ± 0.35Speciﬁcity-controlled CT 72.89 ± 0.323.00 ± 0.943.64 ± 0.672.53 ± 1.032.56 ± 0.662.90 ± 0.903.34 ± 0.703.59 ± 0.600.82 ± 0.39Speciﬁcity-controlled CT 92.90 ± 0.352.83 ± 0.873.61 ± 0.622.40 ± 0.972.31 ± 0.742.84 ± 0.833.07 ± 0.813.58 ± 0.560.88 ± 0.32Speciﬁcity control (WD)Speciﬁcity-controlled WD -102.85 ± 0.432.43 ± 0.993.34 ± 0.832.15 ± 0.912.31 ± 0.692.38 ± 0.943.03 ± 0.753.33 ± 0.700.71 ± 0.45Speciﬁcity-controlled WD -42.90 ± 0.302.78 ± 0.953.55 ± 0.632.41 ± 0.922.52 ± 0.662.64 ± 0.933.28 ± 0.733.56 ± 0.620.82 ± 0.38Speciﬁcity-controlled WD 42.95 ± 0.212.99 ± 0.863.65 ± 0.552.49 ± 0.902.65 ± 0.553.00 ± 0.783.37 ± 0.593.63 ± 0.500.93 ± 0.25Speciﬁcity-controlled WD 62.93 ± 0.262.96 ± 0.903.52 ± 0.762.41 ± 1.042.58 ± 0.663.06 ± 0.803.24 ± 0.763.50 ± 0.660.93 ± 0.26Speciﬁcity-controlled WD 82.78 ± 0.522.40 ± 1.232.67 ± 1.251.86 ± 0.972.03 ± 0.872.55 ± 1.142.61 ± 1.052.91 ± 0.910.92 ± 0.28Response-related control (WD)Response-related controlled WD -102.86 ± 0.442.48 ± 0.983.42 ± 0.742.02 ± 0.932.38 ± 0.752.53 ± 0.942.84 ± 0.803.14 ± 0.750.91 ± 0.29Response-related controlled WD 02.96 ± 0.233.01 ± 0.903.72 ± 0.542.73 ± 1.002.56 ± 0.672.92 ± 0.843.37 ± 0.723.73 ± 0.520.82 ± 0.38Response-related controlled WD 52.90 ± 0.332.88 ± 0.903.51 ± 0.632.41 ± 1.012.53 ± 0.652.85 ± 0.903.27 ± 0.733.49 ± 0.630.82 ± 0.39Response-related controlled WD 102.78 ± 0.432.39 ± 1.043.06 ± 0.901.97 ± 0.992.22 ± 0.672.57 ± 1.013.03 ± 0.763.16 ± 0.630.75 ± 0.43Response-related controlled WD 132.71 ± 0.572.10 ± 1.132.54 ± 1.121.81 ± 1.072.14 ± 0.842.33 ± 1.062.69 ± 0.832.70 ± 0.880.62 ± 0.49Table 7: Raw scores (mean ± std.) for all models and human evaluation metrics.The ﬁrst eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, PersonaRetrieval, is on a scale from 0 to 1 where higher is better.The maximum of each column (excluding Human row) is in bold.\f1722ModelAvoiding Rep.EngageFluencyHumannessInquisitiveInterestingListeningMake SenseHuman and baselines* Human2.79 ± 0.123.04 ± 0.113.36 ± 0.123.35 ± 0.112.44 ± 0.122.92 ± 0.113.32 ± 0.133.68 ± 0.11* Greedy Search2.08 ± 0.102.24 ± 0.113.03 ± 0.101.75 ± 0.121.95 ± 0.102.29 ± 0.132.62 ± 0.103.23 ± 0.10* Beam Search (beam size 20)2.08 ± 0.112.29 ± 0.113.09 ± 0.131.71 ± 0.132.42 ± 0.112.29 ± 0.142.47 ± 0.123.35 ± 0.13Repetition control (WD)Extrep bigram WD -0.52.62 ± 0.102.54 ± 0.123.35 ± 0.122.13 ± 0.112.63 ± 0.112.56 ± 0.112.93 ± 0.113.48 ± 0.11Extrep bigram WD -1.252.78 ± 0.092.82 ± 0.133.40 ± 0.122.27 ± 0.122.54 ± 0.092.76 ± 0.103.05 ± 0.113.53 ± 0.14Extrep bigram WD -3.52.83 ± 0.112.93 ± 0.103.56 ± 0.102.43 ± 0.112.47 ± 0.112.83 ± 0.103.14 ± 0.103.62 ± 0.12Extrep bigram WD -inf2.74 ± 0.112.87 ± 0.143.49 ± 0.122.32 ± 0.132.56 ± 0.112.75 ± 0.123.13 ± 0.123.59 ± 0.12* Repetition-controlled baseline2.86 ± 0.122.82 ± 0.123.53 ± 0.102.40 ± 0.112.62 ± 0.132.84 ± 0.123.10 ± 0.113.58 ± 0.14Question control (CT)Question-controlled CT 02.87 ± 0.122.84 ± 0.133.51 ± 0.102.46 ± 0.112.36 ± 0.092.76 ± 0.093.10 ± 0.103.49 ± 0.12Question-controlled CT 12.82 ± 0.112.88 ± 0.113.42 ± 0.102.46 ± 0.122.47 ± 0.112.79 ± 0.133.14 ± 0.113.55 ± 0.10Question-controlled CT 42.78 ± 0.122.88 ± 0.103.47 ± 0.112.40 ± 0.092.53 ± 0.132.83 ± 0.133.24 ± 0.113.59 ± 0.10* Question-controlled CT 72.81 ± 0.102.99 ± 0.113.54 ± 0.092.35 ± 0.112.66 ± 0.122.92 ± 0.123.11 ± 0.103.47 ± 0.10Question-controlled CT 102.67 ± 0.132.87 ± 0.113.52 ± 0.122.35 ± 0.122.63 ± 0.122.66 ± 0.102.94 ± 0.113.53 ± 0.12Question-controlled CT 10 (boost)2.68 ± 0.122.74 ± 0.093.42 ± 0.122.19 ± 0.132.79 ± 0.112.74 ± 0.113.00 ± 0.123.45 ± 0.13Speciﬁcity control (CT)Speciﬁcity-controlled CT 02.79 ± 0.102.93 ± 0.093.44 ± 0.122.38 ± 0.112.56 ± 0.122.84 ± 0.123.12 ± 0.133.61 ± 0.11Speciﬁcity-controlled CT 22.78 ± 0.122.74 ± 0.113.39 ± 0.132.31 ± 0.132.56 ± 0.132.74 ± 0.122.99 ± 0.113.47 ± 0.10Speciﬁcity-controlled CT 42.82 ± 0.102.80 ± 0.133.44 ± 0.142.32 ± 0.132.51 ± 0.122.78 ± 0.153.09 ± 0.133.46 ± 0.13Speciﬁcity-controlled CT 72.81 ± 0.122.91 ± 0.133.43 ± 0.112.45 ± 0.102.49 ± 0.112.81 ± 0.123.15 ± 0.123.55 ± 0.11Speciﬁcity-controlled CT 92.80 ± 0.132.78 ± 0.103.41 ± 0.122.35 ± 0.132.28 ± 0.112.79 ± 0.112.91 ± 0.113.51 ± 0.12Speciﬁcity control (WD)Speciﬁcity-controlled WD -102.76 ± 0.112.41 ± 0.123.19 ± 0.122.15 ± 0.112.28 ± 0.132.35 ± 0.122.89 ± 0.113.28 ± 0.12Speciﬁcity-controlled WD -42.83 ± 0.102.76 ± 0.123.37 ± 0.102.36 ± 0.112.46 ± 0.112.62 ± 0.123.14 ± 0.093.52 ± 0.11* Speciﬁcity-controlled WD 42.84 ± 0.102.96 ± 0.123.45 ± 0.132.44 ± 0.122.56 ± 0.092.94 ± 0.113.20 ± 0.103.54 ± 0.11Speciﬁcity-controlled WD 62.81 ± 0.092.91 ± 0.103.34 ± 0.092.31 ± 0.112.53 ± 0.122.93 ± 0.123.09 ± 0.103.41 ± 0.12Speciﬁcity-controlled WD 82.70 ± 0.112.39 ± 0.122.54 ± 0.121.80 ± 0.132.00 ± 0.102.49 ± 0.122.47 ± 0.102.87 ± 0.11Response-related control (WD)Response-related controlled WD -102.77 ± 0.122.45 ± 0.123.26 ± 0.111.96 ± 0.102.31 ± 0.122.47 ± 0.122.73 ± 0.113.12 ± 0.12Response-related controlled WD 02.87 ± 0.122.97 ± 0.113.55 ± 0.092.62 ± 0.112.48 ± 0.102.88 ± 0.123.21 ± 0.093.70 ± 0.10Response-related controlled WD 52.79 ± 0.102.83 ± 0.093.35 ± 0.122.40 ± 0.122.51 ± 0.132.80 ± 0.133.13 ± 0.123.41 ± 0.12Response-related controlled WD 102.74 ± 0.112.42 ± 0.122.93 ± 0.111.95 ± 0.122.20 ± 0.122.56 ± 0.122.90 ± 0.123.12 ± 0.10Response-related controlled WD 132.63 ± 0.122.06 ± 0.112.40 ± 0.091.74 ± 0.112.07 ± 0.112.25 ± 0.122.49 ± 0.142.63 ± 0.10Table 8: Calibrated scores (mean ± std.) for all models and human evaluation metrics.The ﬁrst eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, PersonaRetrieval, is on a scale from 0 to 1 where higher is better.The maximum of each column (excluding Human row) is in bold.Rows marked with * are the six models included in Figure 3 (left) and Figure 4.\f1723HPlots of human evaluation results for all conﬁgurationsExtrepbigram -0.5Extrepbigram -1.25Extrepbigram -3.5Extrepbigram -infExtrep bigram -3.5+ unigram -inf2.42.62.83.03.23.43.6FluencyRepetition control settingBeam search baselineGreedy search baselineHuman0FewerQuestions1234567891010 (boost)MoreQuestions2.42.62.83.03.23.43.6FluencyQuestion-Asking Control Level (CT)Question-controlled CTBeam search baselineHumanRepetition-controlled baseline0MoreGeneric123456789MoreSpecific2.42.62.83.03.23.43.6FluencySpecificity Control Level (CT)Specificity-controlled CTBeam search baselineHumanRepetition-controlled baseline-10MoreGeneric-8-6-4-20No control246810MoreSpecific2.42.62.83.03.23.43.6FluencySpecificity Control Level (WD)Specificity-controlled WDBeam search baselineHumanRepetition-controlled baseline-15Moreunrelated-10-50No control51015Morerelated2.42.62.83.03.23.43.6FluencyResponse-relatedness Control Level (WD)Response-related controlled WDBeam search baselineHumanRepetition-controlled baseline+Extrepbigram -0.5Extrepbigram -1.25Extrepbigram -3.5Extrepbigram -infExtrep bigram -3.5+ unigram -inf2.02.22.42.62.83.0EngagingnessBeam search baselineGreedy search baselineHuman0FewerQuestions1234567891010 (boost)MoreQuestions2.02.22.42.62.83.0EngagingnessQuestion-controlled CTBeam search baselineHumanRepetition-controlled baseline0MoreGeneric123456789MoreSpecific2.02.22.42.62.83.0EngagingnessSpecificity-controlled CTBeam search baselineHumanRepetition-controlled baseline-10MoreGeneric-8-6-4-20No control246810MoreSpecific2.02.22.42.62.83.0EngagingnessSpecificity-controlled WDBeam search baselineHumanRepetition-controlled baseline-15Moreunrelated-10-50No control51015Morerelated2.02.22.42.62.83.0EngagingnessResponse-related controlled WDBeam search baselineHumanRepetition-controlled baseline+Extrepbigram -0.5Extrepbigram -1.25Extrepbigram -3.5Extrepbigram -infExtrep bigram -3.5+ unigram -inf1.752.002.252.502.753.003.25HumannessBeam search baselineGreedy search baselineHuman0FewerQuestions1234567891010 (boost)MoreQuestions1.752.002.252.502.753.003.25HumannessQuestion-controlled CTBeam search baselineHumanRepetition-controlled baseline0MoreGeneric123456789MoreSpecific1.752.002.252.502.753.003.25HumannessSpecificity-controlled CTBeam search baselineHumanRepetition-controlled baseline-10MoreGeneric-8-6-4-20No control246810MoreSpecific1.752.002.252.502.753.003.25HumannessSpecificity-controlled WDBeam search baselineHumanRepetition-controlled baseline-15Moreunrelated-10-50No control51015Morerelated1.752.002.252.502.753.003.25HumannessResponse-related controlled WDBeam search baselineHumanRepetition-controlled baseline+Extrepbigram -0.5Extrepbigram -1.25Extrepbigram -3.5Extrepbigram -infExtrep bigram -3.5+ unigram -inf2.62.83.03.23.43.63.8Making SenseBeam search baselineGreedy search baselineHuman0FewerQuestions1234567891010 (boost)MoreQuestions2.62.83.03.23.43.63.8Making SenseQuestion-controlled CTBeam search baselineHumanRepetition-controlled baseline0MoreGeneric123456789MoreSpecific2.62.83.03.23.43.63.8Making SenseSpecificity-controlled CTBeam search baselineHumanRepetition-controlled baseline-10MoreGeneric-8-6-4-20No control246810MoreSpecific2.62.83.03.23.43.63.8Making SenseSpecificity-controlled WDBeam search baselineHumanRepetition-controlled baseline-15Moreunrelated-10-50No control51015Morerelated2.62.83.03.23.43.63.8Making SenseResponse-related controlled WDBeam search baselineHumanRepetition-controlled baseline+Extrepbigram -0.5Extrepbigram -1.25Extrepbigram -3.5Extrepbigram -infExtrep bigram -3.5+ unigram -inf2.02.22.42.62.8InquisitivenessBeam search baselineGreedy search baselineHuman0FewerQuestions1234567891010 (boost)MoreQuestions2.02.22.42.62.8InquisitivenessQuestion-controlled CTBeam search baselineHumanRepetition-controlled baseline0MoreGeneric123456789MoreSpecific2.02.22.42.62.8InquisitivenessSpecificity-controlled CTBeam search baselineHumanRepetition-controlled baseline-10MoreGeneric-8-6-4-20No control246810MoreSpecific2.02.22.42.62.8InquisitivenessSpecificity-controlled WDBeam search baselineHumanRepetition-controlled baseline-15Moreunrelated-10-50No control51015Morerelated2.02.22.42.62.8InquisitivenessResponse-related controlled WDBeam search baselineHumanRepetition-controlled baseline+Extrepbigram -0.5Extrepbigram -1.25Extrepbigram -3.5Extrepbigram -infExtrep bigram -3.5+ unigram -inf2.22.42.62.83.0InterestingnessBeam search baselineGreedy search baselineHuman0FewerQuestions1234567891010 (boost)MoreQuestions2.22.42.62.83.0InterestingnessQuestion-controlled CTBeam search baselineHumanRepetition-controlled baseline0MoreGeneric123456789MoreSpecific2.22.42.62.83.0InterestingnessSpecificity-controlled CTBeam search baselineHumanRepetition-controlled baseline-10MoreGeneric-8-6-4-20No control246810MoreSpecific2.22.42.62.83.0InterestingnessSpecificity-controlled WDBeam search baselineHumanRepetition-controlled baseline-15Moreunrelated-10-50No control51015Morerelated2.22.42.62.83.0InterestingnessResponse-related controlled WDBeam search baselineHumanRepetition-controlled baseline+Extrepbigram -0.5Extrepbigram -1.25Extrepbigram -3.5Extrepbigram -infExtrep bigram -3.5+ unigram -inf2.02.22.42.62.83.0Avoiding RepetitionBeam search baselineGreedy search baselineHuman0FewerQuestions1234567891010 (boost)MoreQuestions2.02.22.42.62.83.0Avoiding RepetitionQuestion-controlled CTBeam search baselineHumanRepetition-controlled baseline0MoreGeneric123456789MoreSpecific2.02.22.42.62.83.0Avoiding RepetitionSpecificity-controlled CTBeam search baselineHumanRepetition-controlled baseline-10MoreGeneric-8-6-4-20No control246810MoreSpecific2.02.22.42.62.83.0Avoiding RepetitionSpecificity-controlled WDBeam search baselineHumanRepetition-controlled baseline-15Moreunrelated-10-50No control51015Morerelated2.02.22.42.62.83.0Avoiding RepetitionResponse-related controlled WDBeam search baselineHumanRepetition-controlled baseline+Extrepbigram -0.5Extrepbigram -1.25Extrepbigram -3.5Extrepbigram -infExtrep bigram -3.5+ unigram -inf2.42.62.83.03.23.4ListeningBeam search baselineGreedy search baselineHuman0FewerQuestions1234567891010 (boost)MoreQuestions2.42.62.83.03.23.4ListeningQuestion-controlled CTBeam search baselineHumanRepetition-controlled baseline0MoreGeneric123456789MoreSpecific2.42.62.83.03.23.4ListeningSpecificity-controlled CTBeam search baselineHumanRepetition-controlled baseline-10MoreGeneric-8-6-4-20No control246810MoreSpecific2.42.62.83.03.23.4ListeningSpecificity-controlled WDBeam search baselineHumanRepetition-controlled baseline-15Moreunrelated-10-50No control51015Morerelated2.42.62.83.03.23.4ListeningResponse-related controlled WDBeam search baselineHumanRepetition-controlled baseline+Figure 9: Calibrated human evaluation scores for all models. This is the same data as in Table 8.Note: ‘Repetition-controlled baseline+’ in the rightmost column is ‘Response-related controlled WD 0’ inTable 8. See Table 5 for explanation.",
        "answer": "No.",
        "space_key_index": 1,
        "metadata": {
            "format": "PDF 1.3",
            "title": "What makes a good conversation? How controllable attributes affect human judgments",
            "author": "Abigail See ; Stephen Roller ; Douwe Kiela ; Jason Weston",
            "subject": "N19-1 2019",
            "keywords": "",
            "creator": "LaTeX with hyperref package",
            "producer": "pdfTeX-1.40.18",
            "creationDate": "D:20190404111254Z",
            "modDate": "D:20190404111254Z",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "What F1 score did the single-paragraph BERT model achieve in the distractor setting on HOTPOTQA?",
        "context": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4249–4257Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics4249Compositional Questions Do Not Necessitate Multi-hop ReasoningSewon Min∗1, Eric Wallace∗2, Sameer Singh3,Matt Gardner2, Hannaneh Hajishirzi1,2, Luke Zettlemoyer11University of Washington2Allen Institute for Artiﬁcial Intelligence3University of California, Irvinesewon@cs.washington.edu, ericw@allenai.orgAbstractMulti-hop reading comprehension (RC) ques-tions are challenging because they requirereading and reasoning over multiple para-graphs. We argue that it can be difﬁcult to con-struct large multi-hop RC datasets. For exam-ple, even highly compositional questions canbe answered with a single hop if they targetspeciﬁc entity types, or the facts needed toanswer them are redundant. Our analysis iscentered on HOTPOTQA, where we show thatsingle-hop reasoning can solve much more ofthe dataset than previously thought. We intro-duce a single-hop BERT-based RC model thatachieves 67 F1—comparable to state-of-the-art multi-hop models. We also design an eval-uation setting where humans are not shown allof the necessary paragraphs for the intendedmulti-hop reasoning but can still answer over80% of questions. Together with detailed erroranalysis, these results suggest there should bean increasing focus on the role of evidence inmulti-hop reasoning and possibly even a shifttowards information retrieval style evaluationswith large and diverse evidence collections.1IntroductionMulti-hop reading comprehension (RC) requiresreading and aggregating information over multi-ple pieces of textual evidence (Welbl et al., 2017;Yang et al., 2018; Talmor and Berant, 2018). Inthis work, we argue that it can be difﬁcult to con-struct large multi-hop RC datasets. This is becausemulti-hop reasoning is a characteristic of both thequestion and the provided evidence; even highlycompositional questions can be answered with asingle hop if they target speciﬁc entity types, or thefacts needed to answer them are redundant. Forexample, the question in Figure 1 is compositional:a plausible solution is to ﬁnd “What animal’s habi-tat was the R´eserve Naturelle Lomako Yokokala∗Equal Contribution.Question: What is the former name of the animal whosehabitat the R´eserve Naturelle Lomako Yokokala was es-tablished to protect?Paragraph 5: The Lomako Forest Reserve is found inDemocratic Republic of the Congo. It was established in1991 especially to protect the habitat of the Bonobo apes.Paragraph 1: The bonobo (“Pan paniscus”), formerlycalled the pygmy chimpanzee and less often, the dwarfor gracile chimpanzee, is an endangered great ape and oneof the two species making up the genus “Pan”.Figure 1: A HOTPOTQA example designed to requirereasoning across two paragraphs. Eight spurious addi-tional paragraphs (not shown) are provided to increasethe task difﬁculty. However, since only one of the tenparagraphs is about an animal, one can immediately lo-cate the answer in Paragraph 1 using one hop. The fullexample is provided in Appendix A.established to protect?”, and then answer “What isthe former name of that animal?”. However, whenconsidering the evidence paragraphs, the questionis solvable in a single hop by ﬁnding the only para-graph that describes an animal.Our analysis is centered on HOTPOTQA (Yanget al., 2018), a dataset of mostly compositionalquestions. In its RC setting, each question is pairedwith two gold paragraphs, which should be neededto answer the question, and eight distractor para-graphs, which provide irrelevant evidence or incor-rect answers. We show that single-hop reasoningcan solve much more of this dataset than previouslythought. First, we design a single-hop QA modelbased on BERT (Devlin et al., 2018), which, de-spite having no ability to reason across paragraphs,achieves performance competitive with the state ofthe art. Next, we present an evaluation demonstrat-ing that humans can solve over 80% of questionswhen we withhold one of the gold paragraphs.To better understand these results, we presenta detailed analysis of why single-hop reasoningworks so well. We show that questions includeredundant facts which can be ignored when com-\f4250puting the answer, and that the ﬁne-grained entitytypes present in the provided paragraphs in the RCsetting often provide a strong signal for answer-ing the question, e.g., there is only one animal inthe given paragraphs in Figure 1, allowing one toimmediately locate the answer using one hop.This analysis shows that more carefully cho-sen distractor paragraphs would induce questionsthat require multi-hop reasoning.We thus ex-plore an alternative method for collecting distrac-tors based on adversarial paragraph selection. Al-though this appears to mitigate the problem, asingle-hop model re-trained on these distractorscan recover most of the original single-hop accu-racy, indicating that these distractors are still insuf-ﬁcient. Another method is to consider very largedistractor sets such as all of Wikipedia or the en-tire Web, as done in open-domain HOTPOTQA andComplexWebQuestions (Talmor and Berant, 2018).However, this introduces additional computationalchallenges and/or the need for retrieval systems.Finding a small set of distractors that induce multi-hop reasoning remains an open challenge that isworthy of follow up work.2Related WorkLarge-scale RC datasets (Hermann et al., 2015;Rajpurkar et al., 2016; Joshi et al., 2017) have en-abled rapid advances in neural QA models (Seoet al., 2017; Xiong et al., 2018; Yu et al., 2018; De-vlin et al., 2018). To foster research on reasoningacross multiple pieces of text, multi-hop QA hasbeen introduced (Koˇcisk`y et al., 2018; Talmor andBerant, 2018; Yang et al., 2018). These datasetscontain compositional or “complex” questions. Wedemonstrate that these questions do not necessitatemulti-hop reasoning.Existing multi-hop QA datasets are constructedusing knowledge bases, e.g., WIKIHOP (Welblet al., 2017) and COMPLEXWEBQUESTIONS (Tal-mor and Berant, 2018), or using crowd workers,e.g.,HOTPOTQA (Yang et al., 2018).WIKI-HOP questions are posed as triples of a relationand a head entity, and the task is to determinethe tail entity of the relationship. COMPLEXWE-BQUESTIONS consists of open-domain composi-tional questions, which are constructed by increas-ing the complexity of SPARQL queries from WE-BQUESTIONS (Berant et al., 2013). We focus onHOTPOTQA, which consists of multi-hop ques-tions written to require reasoning over two para-Figure 2: Our model, single-paragraph BERT, readsand scores each paragraph independently. The answerfrom the paragraph with the lowest yempty score is cho-sen as the ﬁnal answer.graphs from Wikipedia.Parallel research from Chen and Durrett (2019)presents similar ﬁndings on HOTPOTQA. Our workdiffers because we conduct human analysis to un-derstand why questions are solvable using single-hop reasoning. Moreover, we show that selectingdistractor paragraphs is difﬁcult using current re-trieval methods.3Single-paragraph QAThis section shows the performance of a single-hopmodel on HOTPOTQA.3.1Model DescriptionOur model, single-paragraph BERT, scores and an-swers each paragraph independently (Figure 2). Wethen select the answer from the paragraph with thebest score, similar to Clark and Gardner (2018).1The model receives a question Q = [q1, .., qm]and a single paragraph P = [p1, ..., pn] as in-put.Following Devlin et al. (2018), S=[q1, ..., qm, [SEP], p1, ..., pn], where [SEP] is a spe-cial token, is fed into BERT:S′ = BERT(S) ∈Rh×(m+n+1),where h is the hidden dimension of BERT. Next, aclassiﬁer uses max-pooling and learned parametersW1 ∈Rh×4 to generate four scalars:[yspan; yyes; yno; yempty] = W1maxpool(S′),where yspan, yyes, yno and yempty indicate the an-swer is either a span, yes, no, or no answer. Anextractive paragraph span, span, is obtained sep-arately following Devlin et al. (2018). The ﬁnalmodel outputs are a scalar value yempty and a textof either span, yes or no, based on which ofyspan, yyes, yno has the largest value.1Full details in Appendix B. Code available at https://github.com/shmsw25/single-hop-rc.\f4251ModelDistractor F1Open F1Single-paragraph BERT*67.0838.40BiDAF*58.2834.36BiDAF58.9932.89GRN66.7136.48QFE68.0638.06DFGN + BERT68.49-MultiQA-40.23DecompRC69.6340.65BERT Plus69.76-Cognitive Graph-48.87Table 1: F1 scores on HOTPOTQA. * indicates the re-sult is on the validation set; the other results are on thehidden test set shown in the ofﬁcial leaderboard.For a particular HOTPOTQA example, we runsingle-paragraph BERT on each paragraph in par-allel and select the answer from the paragraph withthe smallest yempty.3.2Model ResultsHOTPOTQA has two settings: a distractor settingand an open-domain setting.Distractor SettingThe HOTPOTQA distractorsetting pairs the two paragraphs the question waswritten for (gold paragraphs) with eight spuriousparagraphs selected using TF-IDF similarity withthe question (distractors). Our single-paragraphBERT model achieves 67.08 F1, comparable tothe state-of-the-art (Table 1).2 This indicates themajority of HOTPOTQA questions are answerablein the distractor setting using a single-hop model.Open-domain SettingThe HOTPOTQA open-domain setting (Fullwiki) does not provide a set ofparagraphs—all of Wikipedia is considered. Wefollow Chen et al. (2017) and retrieve paragraphsusing bigram TF-IDF similarity with the question.We use the single-paragraph BERT modeltrained in the distractor setting. We also ﬁne-tunethe model using incorrect paragraphs selected bythe retrieval system. In particular, we retrieve 30paragraphs and select the eight paragraphs withthe lowest yempty scores predicted by the trainedmodel. Single-paragraph BERT achieves 38.06 F1in the open-domain setting (Table 1). This showsthat the open-domain setting is challenging for oursingle-hop model and is worthy of future study.2Results as of March 4th, 2019.4Compositional Questions Are NotAlways Multi-hopThis section provides a human analysis of HOT-POTQA to understand what phenomena enablesingle-hop answer solutions. HOTPOTQA containstwo question types, Bridge and Comparison, whichwe evaluate separately.4.1Categorizing Bridge QuestionsBridge questions consist of two paragraphs linkedby an entity (Yang et al., 2018), e.g., Figure 1. Weﬁrst investigate single-hop human performance onHOTPOTQA bridge questions using a human studyconsisting of NLP graduate students. Humans seethe paragraph that contains the answer span and theeight distractor paragraphs, but do not see the othergold paragraph. As a baseline, we show a differentset of people the same questions in their standardten paragraph form.On a sample of 200 bridge questions from thevalidation set, human accuracy shows marginaldegradation when using only one hop: humansobtain 87.37 F1 using all ten paragraphs and 82.06F1 when using only nine (where they only see asingle gold paragraph). This indicates humans, justlike models, are capable of solving bridge questionsusing only one hop.Next, we manually categorize what enablessingle-hop answers for 100 bridge validation exam-ples (taking into account the distractor paragraphs),and place questions into four categories (Table 2).Multi-hop27% of questions require multi-hopreasoning. The ﬁrst example of Table 2 requires lo-cating the university where “Ralph Hefferline” wasa psychology professor, and multiple universitiesare provided as distractors. Therefore, the answercannot be determined in one hop.3Weak Distractors35% of questions allowsingle-hop answers in the distractor setting, mostlyby entity type matching. Consider the question inthe second row of Table 2: in the ten provided para-graphs, only one actress has a government position.Thus, the question is answerable without consider-ing the ﬁlm “Kiss and Tell.” These examples maybecome multi-hop in the open-domain setting, e.g.,there are numerous actresses with a governmentposition on Wikipedia.3It is possible that a single-hop model can do well byrandomly guessing between two or three well-typed options,but we do not evaluate that strategy here.\f4252TypeQuestion%Multi-hopRalph Hefferline was a psychology professor at a university that is located in what city?27Weak distractorsWhat government position was held by the woman who portrayed Corliss Archer in35the ﬁlm Kiss and Tell?Redundant evidenceKaiser Ventures corporation was founded by an American industrialist who became26known as the father of modern American shipbuilding?Non-compositional 1-hopWhen was Poison’s album ‘Shut Up, Make Love’ released?8Table 2: We categorize bridge questions while taking the paragraphs into account. We exclude 4% of questionsthat we found to have incorrect or ambiguous answer annotations. See Section 4.1 for details on question types.TypeQuestion%F1Multi-hopWho was born ﬁrst, Arthur Conan Doyle or Penelope Lively?4554.46Context-dependentAre Hot Rod and the Memory of Our People both magazines?3656.16Single-hopWhich writer was from England, Henry Roth or Robert Erskine Childers?1770.54Table 3: We automatically categorize comparison questions using rules (2% cannot be automatically categorized).Single-paragraph BERT achieves near chance accuracy on multi-hop questions but exploits single-hop ones.To further investigate entity type matching, wereduce the question to the ﬁrst ﬁve tokens start-ing from the wh-word, following Sugawara et al.(2018). Although most of these reduced questionsappear void of critical information, the F1 scoreof single-paragraph BERT only degrades about 15F1 from 67.08 to 52.13.Redundant Evidence26% of questions arecompositional but are solvable using only part ofthe question. For instance, in the third example ofTable 2 there is only a single founder of “KaiserVentures.”Thus, one can ignore the conditionon “American industrialist” and “father of modernAmerican shipbuilding.” This category differs fromthe weak distractors category because its questionsare single-hop regardless of the distractors.Non-compositional Single-hop8% of ques-tions are non-compositional and single-hop. Inthe last example of Table 2, one sentence containsall of the information needed to answer correctly.4.2Categorizing Comparison QuestionsComparison questions require quantitative or logi-cal comparisons between two quantities or events.We create rules (Appendix C) to group comparisonquestions into three categories: questions whichrequire multi-hop reasoning (multi-hop), may re-quire multi-hop reasoning (context-dependent), andrequire single-hop reasoning (single-hop).Many comparison questions are multi-hop orcontext-dependent multi-hop, and single-paragraphEvaluation DataTraining DataOriginalAdversarialOriginal67.0859.12Adversarial46.8460.10+ Type40.7358.42Table 4: We train on HOTPOTQA using standard dis-tractors (Original) or using adversarial distractors (Ad-versarial). The model is then tested on the original dis-tractors, adversarial distractors, or adversarial distrac-tors with ﬁltering by entity type (+ Type).BERT achieves near chance accuracy on thesetypes of questions (Table 3).4This shows thatmost comparison questions are not solvable by oursingle-hop model.5Can We Find Better Distractors?In Section 4.1, we identify that 35% of bridge ex-amples are solvable using single-hop reasoning dueto weak distractor paragraphs. Here, we attempt toautomatically correct these examples by choosingnew distractor paragraphs which are likely to trickour single-paragraph model.Adversarial DistractorsWe select the top-50ﬁrst paragraphs of Wikipedia pages using TF-IDFsimilarity with the question, following the originalHOTPOTQA setup. Next, we use single-paragraphBERT to adversarially select the eight distractorparagraphs from these 50 candidates. In particular,we feed each paragraph to the model and select4Comparison questions test mainly binary relationships.\f4253the paragraphs with the lowest yempty score (i.e.,the paragraphs that the model thinks contain theanswer). These paragraphs are dissimilar to theoriginal distractors—there is a 9.82% overlap.We report the F1 score of single-paragraphBERT on these new distractors in Table 4: theaccuracy declines from 67.08 F1 to 46.84 F1. How-ever, when the same procedure is done on the train-ing set and the model is re-trained, the accuracyincreases to 60.10 F1 on the adversarial distractors.Type DistractorsWe also experiment with ﬁlter-ing the initial list of 50 paragraph to ones whoseentity type (e.g., person) matches that of the goldparagraphs. This can help to eliminate the entitytype bias described in Section 4.1. As shown inTable 4, the original model’s accuracy degradessigniﬁcantly (drops to 40.73 F1). However, similarto the previous setup, the model trained on the ad-versarially selected distractors can recover most ofits original accuracy (increases to 58.42 F1).These results show that single-paragraph BERTcan struggle when the distribution of the distrac-tors changes (e.g., using adversarial selection ratherthan only TF-IDF). Moreover, the model can some-what recover its original accuracy when re-trainedon distractors from the new distribution.6ConclusionsIn summary, we demonstrate that question compo-sitionality is not a sufﬁcient condition for multi-hopreasoning. Instead, future datasets must carefullyconsider what evidence they provide in order toensure multi-hop reasoning is required. There areat least two different ways to achieve this.Open-domain QuestionsOur single-hop modelstruggles in the open-domain setting. We largelyattribute this to the insufﬁciencies of standard TF-IDF retrieval for multi-hop questions. For example,we fail to retrieve the paragraph about “Bonoboapes” in Figure 1, because the question does notcontain terms about “Bonobo apes.” Table 5 showsthat the model achieves 39.12 F1 given 500 re-trieved paragraphs, but achieves 53.12 F1 whenadditional two gold paragraphs are given, demon-strating the signiﬁcant effect of failure to retrievegold paragraphs. In this context, we suggest thatfuture work can explore better retrieval methodsfor multi-hop questions.Retrieving Strong DistractorsAnother way toensure multi-hop reasoning is to select strong dis-SettingF1Distractor67.08Open-domain 10 Paragraphs38.40Open-domain 500 Paragraphs39.12+ Gold Paragraph53.12Table 5: The accuracy of single-paragraph BERT indifferent open-domain retrieval settings. TF-IDF oftenfails to retrieve the gold paragraphs even when using500 candidates.tractor paragraphs. For example, we found 35% ofbridge questions are currently single-hop but maybecome multi-hop when combined with strongerdistractors (Section 4.1). However, as we demon-strate in Section 5, selecting strong distractorsfor RC questions is non-trivial. We suspect thisis also due to the insufﬁciencies of standard TF-IDF retrieval for multi-hop questions. In partic-ular, Table 5 shows that single-paragraph BERTachieves 53.12 F1 even when using 500 distractors(rather than eight), indicating that 500 distractorsare still insufﬁcient. In this end, future multi-hopRC datasets can develop improved methods fordistractor collection.AcknowledgementsThis research was supported by ONR (N00014-18-1-2826, N00014-17-S-B001), NSF (IIS-1616112,IIS-1252835, IIS-1562364), ARO (W911NF-16-1-0121), an Allen Distinguished Investigator Award,Samsung GRO and gifts from Allen Institute forAI, Google, and Amazon.The authors would like to thank Shi Feng, NikhilKandpal, Victor Zhong, the members of AllenNLPand UW NLP, and the anonymous reviewers fortheir valuable feedback.ReferencesJonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang. 2013.Semantic parsing on freebase fromquestion-answer pairs. In EMNLP.Danqi Chen, Adam Fisch, Jason Weston, and AntoineBordes. 2017. Reading wikipedia to answer open-domain questions. In ACL.Jifan Chen and Greg Durrett. 2019.Understandingdataset design choices for multi-hop reasoning. InNAACL.Christopher Clark and Matt Gardner. 2018.Simpleand effective multi-paragraph reading comprehen-sion. In ACL.\f4254Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018.BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In NAACL.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom. 2015. Teaching machines to readand comprehend. In NIPS.Mandar Joshi, Eunsol Choi, Daniel S Weld, and LukeZettlemoyer. 2017.TriviaQA: A large scale dis-tantly supervised challenge dataset for reading com-prehension. In ACL.Diederik P Kingma and Jimmy Ba. 2015. Adam: Amethod for stochastic optimization. In ICLR.Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom,Chris Dyer, Karl Moritz Hermann, G´abor Melis, andEdward Grefenstette. 2018. The NarrativeQA read-ing comprehension challenge. TACL.Adam Paszke, Sam Gross, Soumith Chintala, GregoryChanan, Edward Yang, Zachary DeVito, ZemingLin, Alban Desmaison, Luca Antiga, and AdamLerer. 2017. Automatic differentiation in PyTorch.In NIPS Autodiff Workshop.Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. SQuAD: 100,000+ questions formachine comprehension of text. In EMNLP.Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, andHannaneh Hajishirzi. 2017. Bidirectional attentionﬂow for machine comprehension. In ICLR.Saku Sugawara, Kentaro fInui, Satoshi Sekine, andAkiko Aizawa. 2018. What makes reading compre-hension questions easier? In EMNLP.Alon Talmor and Jonathan Berant. 2018. The web asa knowledge-base for answering complex questions.In NAACL.Johannes Welbl, Pontus Stenetorp, and SebastianRiedel. 2017. Constructing Datasets for Multi-hopReading Comprehension Across Documents.InTACL.Caiming Xiong, Victor Zhong, and Richard Socher.2018. DCN+: Mixed objective and deep residualcoattention for question answering. In ICLR.Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W Cohen, Ruslan Salakhutdinov, andChristopher D Manning. 2018.HotpotQA: Adataset for diverse, explainable multi-hop questionanswering. In EMNLP.Adams Wei Yu, David Dohan, Minh-Thang Luong, RuiZhao, Kai Chen, Mohammad Norouzi, and Quoc V.Le. 2018. Qanet: Combining local convolution withglobal self-attention for reading comprehension. InICLR.\f4255AExample Distractor QuestionWe present the full example from Figure 1 below.Paragraphs 1 and 5 are the two gold paragraphs.QuestionWhat is the former name of the ani-mal whose habitat the R´eserve Naturelle LomakoYokokala was established to protect?Answerpygmy chimpanzee(Gold Paragraph) Paragraph 1The bonobo(or ;“Pan paniscus”),formerly called thepygmy chimpanzee and less often, the dwarf orgracile chimpanzee, is an endangered great apeand one of the two species making up the genus“Pan”; the other is “Pan troglodytes”, or the com-mon chimpanzee. Although the name “chimpanzee”is sometimes used to refer to both species together,it is usually understood as referring to the commonchimpanzee, whereas “Pan paniscus” is usually re-ferred to as the bonobo.Paragraph 2The Carri´ere des Nerviens Re-gional Nature Reserve (in French “R´eserve na-turelle r´egionale de la carri´ere des Nerviens”) isa protected area in the Nord-Pas-de-Calais regionof northern France. It was established on 25 May2009 to protect a site containing rare plants and cov-ers just over 3 ha. It is located in the municipalitiesof Bavay and Saint-Waast in the Nord department.Paragraph 3C´ereste (Occitan: “Ceir´esta”) isa commune in the Alpes-de-Haute-Provence de-partment in southeastern France. It is known forits rich fossil beds in ﬁne layers of “Calcaire deCampagne Calavon” limestone, which are now pro-tected by the Parc naturel r´egional du Luberon andthe R´eserve naturelle g´eologique du Luberon.Paragraph 4The Grand Cote National WildlifeRefuge (French: “R´eserve Naturelle Faunique Na-tionale du Grand- Cote”) was established in 1989as part of the North American Waterfowl Manage-ment Plan. It is a 6000 acre reserve located inAvoyelles Parish, near Marksville, Louisiana, inthe United States.(Gold Paragraph) Paragraph 5The LomakoForest Reserve is found in Democratic Republic ofthe Congo. It was established in 1991 especiallyto protect the habitat of the Bonobo apes. This sitecovers 3,601.88 km2.Paragraph 6Guadeloupe National Park (French:“Parc national de la Guadeloupe”) is a nationalpark in Guadeloupe, an overseas department ofFrance located in the Leeward Islands of the easternCaribbean region. The Grand Cul-de-Sac MarinNature Reserve (French: “R´eserve Naturelle duGrand Cul-de-Sac Marin”) is a marine protectedarea adjacent to the park and administered in con-junction with it. Together, these protected areascomprise the Guadeloupe Archipelago (French:“l’Archipel de la Guadeloupe”) biosphere reserve.Paragraph 7La D´esirade National Nature Re-serve (French: “R´eserve naturelle nationale de LaD´esirade”) is a reserve in D´esirade Island in Guade-loupe. Established under the Ministerial Decree No.2011-853 of 19 July 2011 for its special geologi-cal features it has an area of 62 ha. The reserverepresents the geological heritage of the Caribbeantectonic plate, with a wide spectrum of rock for-mations, the outcrops of volcanic activity beingremnants of the sea level oscillations. It is one ofthirty three geosites of Guadeloupe.Paragraph 8La Tortue ou l’Ecalle or Ile Tortueis a small rocky islet off the northeastern coastof Saint Barth´elemy in the Caribbean. Its highestpoint is 35 m above sea level. Referencing tortoises,it forms part of the R´eserve naturelle nationale deSaint-Barth´elemy with several of the other northernislets of St Barts.Paragraph9NatureReserveofSaintBartholomew(R´eserveNaturelledeSaint-Barth´elemy)isanaturereserveofSaintBarth´elemy (RNN 132), French West Indies, anoverseas collectivity of France.Paragraph 10Ile Fourchue, also known as IleFourche is an island between Saint-Barth´elemy andSaint Martin, belonging to the Collectivity of SaintBarth´elemy. The island is privately owned. Theonly inhabitants are some goats. The highest pointis 103 meter above sea level. It is situated withinR´eserve naturelle nationale de Saint-Barth´elemy.BFull Model DetailsSingle-paragraph BERT is a pipeline which ﬁrstretrieves a single paragraph using a classiﬁer andthen selects the associated answer. Formally, themodel receives a question Q = [q1, .., qm] and asingle paragraph P = [p1, ..., pn] as input. Thequestion and paragraph are merged into a single\f4256QuestionParagraph 1Paragraph 2Paragraph Nyemptyspan/yes/nospan/yes/nolowest...yemptyyempty...span/yes/noBERTOutput 1Output 2Output N...Output 1Output 2Output NBERTWhereiscompanytheheadquartered?<sep>WarrierSachine......isaplaybackQuestionParagraphW1W2W3......maxpoolynoyspanyyesyemptyystartyendspanFigure 3: Single-paragraph BERT reads and scores each paragraph independently. The answer from the paragraphwith the lowest yempty score is chosen as the ﬁnal answer.sequence, S = [q1, ..., qm, [SEP], p1, ..., pn], where[SEP] is a special token indicating the boundary.The sequence is fed into BERT-BASE:S′ = BERT(S) ∈Rh×(m+n+1),where h is the hidden dimension of BERT. Next,a classiﬁer uses max-pooling and learned parame-ters W1 ∈Rh×4 to generate four scalars:[yspan; yyes; yno; yempty] = W1maxpool(S′),where yspan, yyes, yno and yempty indicate theanswer is either a span, yes, no, or no answer.A candidate answer span is then computed sepa-rately from the classiﬁer. We deﬁnepstart = Softmax(W2S′)pend = Softmax(W3S′),where W2, W3 ∈Rh are learned parameters. Then,ystart and yend are obtained:ystart, yend = arg maxi≤jpistartpjendwhere pistart and pjend indicate the i-th element ofpstart and j-th element of pend, respectively.We now have four scalar values yspan, yyes, yno,and yempty and a span from the paragraph span =[Systart, . . . , Syend].For HOTPOTQA, the input is a question andN context paragraphs. We create a batch of sizeN, where each entry is a question and a singleparagraph. Denote the ouput from i-th entry asyispan, yiyes, yino, yiemptyand spani. The ﬁnal answeris selected as:j=argmini(yiempty)ymax=max(yjspan, yjyes, yjno)answer=spanjifyjspan = ymaxyesifyjyes = ymaxnoifyjno = ymaxDuring training, yiempty is set to 0 for the paragraphwhich contains the answer span and 1 otherwise.ImplementationDetailsWeusePy-Torch (Paszke et al., 2017) based on HuggingFace’s implementation.5 We use Adam (Kingmaand Ba, 2015) with learning rate 5 × 10−5. Welowercase the input and set the maximum sequencelength |S| to 300. If a sequence is longer than 300,we split it into multiple sequences and treat themas different examples.CCategorizing Comparison QuestionsThis section describes how we categorize compari-son questions. We ﬁrst identify ten question opera-tions that sufﬁciently cover comparison questions(Table 6). Next, for each question, we extract thetwo entities under comparison using the Spacy6NER tagger on the question and the two HOT-POTQA supporting facts. Using these extracted5https://github.com/huggingface/pytorch-pretrained-BERT6https://spacy.io/\f4257Operation & ExampleNumerical QuestionsOperations: Is greater / Is smaller / Which is greater / Which is smallerExample (Which is smaller): Who was born ﬁrst, Arthur Conan Doyle or Penelope Lively?Logical QuestionsOperations: And / Or / Which is trueExample (And): Are Hot Rod and the Memory of Our People both magazines?String QuestionsOperations: Is equal / Not equal / IntersectionExample (Is equal): Are Cardinal Health and Kansas City Southern located in the same state?Table 6: The question operations used for categorizing comparison questions.Algorithm 1 Algorithm for Identifying Question Operations1: procedure CATEGORIZE(question, entity1, entity2)2:coordination, preconjunct ←f(question, entity1, entity2)3:Determine if the question is either question or both question from coordination and preconjunct4:head entity ←fhead(question, entity1, entity2)5:if more, most, later, last, latest, longer, larger, younger, newer, taller, higher in question then6:if head entity exists then discrete operation ←Which is greater7:else discrete operation ←Is greater8:else if less, earlier, earliest, ﬁrst, shorter, smaller, older, closer in question then9:if head entity exists then discrete operation ←Which is smaller10:else discrete operation ←Is smaller11:else if head entity exists then12:discrete operation ←Which is true13:else if question is not yes/no question and asks for the property in common then14:discrete operation ←Intersection15:else if question is yes/no question then16:Determine if question asks for logical comparison or string comparison17:if question asks for logical comparison then18:if either question then discrete operation ←Or19:else if both question then discrete operation ←And20:else if question asks for string comparison then21:if asks for same? then discrete operation ←Is equal22:else if asks for difference? then discrete operation ←Not equal23:return discrete operationentities, we identity the suitable question operationfollowing Algorithm 1.Based on the identiﬁed operation, questionsare classiﬁed into multi-hop, context-dependentmulti-hop, or single-hop. First, numerical ques-tions are always multi-hop (e.g., ﬁrst example ofTable 6). Next, the operations And, Or, Isequal, and Not equal are context-dependentmulti-hop. For instance, in the second exampleof Table 6, if “Hot Rod” is not a magazine, onecan immediately answer No. Finally, the oper-ations Which is true and Intersectionare single-hop because they can be answered us-ing one paragraph regardless of the context. Forinstance, in the third example of Table 6, if HenryRoth’s paragraph explains he is from England, onecan answer Henry Roth, otherwise, the answer isRobert Erskine Childers.",
        "answer": "67.08.",
        "space_key_index": 0,
        "metadata": {
            "format": "PDF 1.3",
            "title": "Compositional Questions Do Not Necessitate Multi-hop Reasoning",
            "author": "Sewon Min ; Eric Wallace ; Sameer Singh ; Matt Gardner ; Hannaneh Hajishirzi ; Luke Zettlemoyer",
            "subject": "P19-1 2019",
            "keywords": "",
            "creator": "LaTeX with hyperref package",
            "producer": "pdfTeX-1.40.18",
            "creationDate": "D:20190604001854Z",
            "modDate": "D:20190604001854Z",
            "trapped": "",
            "encryption": null
        }
    },
    {
        "question": "How many language pairs are included in the datasets compiled for the research?",
        "context": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3003–3008Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics3003Getting Gender Right in Neural Machine TranslationEva VanmassenhoveαChristian Hardmeierβα ADAPT, School of Computing, Dublin City University, Dublin, Irelandfirstname.lastname@adaptcentre.ieβ Department of Linguistics and Philology, Uppsala University, Uppsala, Swedenchristian.hardmeier@lingfil.uu.seAndy WayαAbstractSpeakers of different languages must attendto and encode strikingly different aspects ofthe world in order to use their language cor-rectly (Sapir, 1921; Slobin, 1996). One suchdifference is related to the way gender is ex-pressed in a language. Saying “I am happy”in English, does not encode any additionalknowledge of the speaker that uttered the sen-tence.However, many other languages dohave grammatical gender systems and so suchknowledge would be encoded.In order tocorrectly translate such a sentence into, say,French, the inherent gender information needsto be retained/recovered. The same sentencewould become either “Je suis heureux”, for amale speaker or “Je suis heureuse” for a fe-male one. Apart from morphological agree-ment, demographic factors (gender, age, etc.)also inﬂuence our use of language in terms ofword choices or even on the level of syntac-tic constructions (Tannen, 1991; Pennebakeret al., 2003). We integrate gender informationinto NMT systems. Our contribution is two-fold: (1) the compilation of large datasets withspeaker information for 20 language pairs, and(2) a simple set of experiments that incorpo-rate gender information into NMT for multi-ple language pairs. Our experiments show thatadding a gender feature to an NMT system sig-niﬁcantly improves the translation quality forsome language pairs.1IntroductionIn the ﬁeld of linguistics, the differences betweenmale and female traits within spoken and writtenlanguage have been studied both empirically andtheoretically, revealing that the language used bymales and females differs in terms of style andsyntax (Coates, 2015). The increasing amount ofwork on automatic author classiﬁcation (or ‘au-thor proﬁling’) reaching relatively high accuracieson domain-speciﬁc data corroborates these ﬁnd-ings (Rangel et al., 2013; Santosh et al., 2013).However, determining the gender of an authorbased solely on text is not a solved issue. Like-wise, the selection of the most informative fea-tures for gender classiﬁcation remains a difﬁculttask (Litvinova et al., 2016).When translating from one language into an-other, original author traits are partially lost, bothin human and machine translations (Mirkin et al.,2015; Rabinovich et al., 2017). However, in theﬁeld of Machine Translation (MT) one of the mostobservable consequences of this missing informa-tion are morphologically incorrect variants due toa lack of agreement in number and gender withthe subject. Such errors harm the overall ﬂuencyand adequacy of the translated sentence. Further-more, gender-related errors are not just harmingthe quality of the translation as getting the genderright is also a matter of basic politeness. Currentsystems have a tendency to perpetuate a male biaswhich amounts to negative discrimination againsthalf the population and this has been picked up bythe media.1Human translators rely on contextual informa-tion to infer the gender of the speaker in order tomake the correct morphological agreement. How-ever, most current MT systems do not; they simplyexploit statistical dependencies on the sentencelevel that have been learned from large amountsof parallel data. Furthermore, sentences are trans-lated in isolation.As a consequence, pieces ofinformation necessary to determine the gender ofthe speakers, might get lost. The MT system will,in such cases, opt for the statistically most likelyvariant, which depending on the training data, will1https://www.theguardian.com/technology/2017/apr/13/ai-programs-exhibit-racist-and-sexist-biases-research\f3004be either the male or the female form. Addition-ally, in the ﬁeld of MT, training data often con-sists of both original and translated parallel texts:large parts of the texts have already been trans-lated, which, as studied by Mirkin et al. (2015),does not preserve the original demographic andpsychometric traits of the author, making it veryhard for a Neural MT (NMT) system to determinethe gender of the author.With this in mind, a ﬁrst step towards the preser-vation of author traits would be their integrationinto an NMT system. As ‘gender’ manifests itselfnot only in the agreement with other words in asentence, but also in the choice of context-basedwords or on the level of syntactic constructions,the sets of experiments conducted in this paperfocus on the integration of a gender feature intoNMT for multiple language pairs.The structure of the paper is the following: re-lated work is described in Section 2; Section 3 de-scribes and analyses the datasets that were com-piled; the experimental setup is discussed in Sec-tion 4; the results are presented in Section 5; ﬁ-nally, we conclude and provide some ideas for fu-ture work in Section 6.2Related WorkDifferences in the language between male andfemale speakers have been studied within vari-ous ﬁelds related to linguistics, including NaturalLanguage Processing (NLP) for author proﬁling,conversational agents, recommendation systemsetc. Mirkin et al. (2015) motivated the need formore personalized MT. Their experiments showthat MT is detrimental to the automatic recogni-tion of linguistic signals of traits of the original au-thor/speaker. Their work suggests using domain-adaptation techniques to make MT more personal-ized but does not include any actual experimentson the inclusion of author traits in MT.Rabinovich et al. (2017) conducted a series ofexperiments on preserving original author traits,focusing particularly on gender.As suggestedby Mirkin et al. (2015), they treat the person-alization of Statistical MT (SMT) systems as adomain-adaptation task treating the female andmale gender as two different domains.Theyapplied two common simple domain-adaptationtechniques in order to create personalized SMT:(1) using gender-speciﬁc phrase-tables and lan-guage models, and (2) using a gender-speciﬁc tun-ing set. Although their models did not improveover the baseline, their work provides a detailedanalysis of gender traits in human and machinetranslation.Our work is, to the best of our knowledge, theﬁrst to attempt building a speaker-informed NMTsystem. Our approach is similar to the work ofSennrich et al. (2016) on controlling politeness,where some sentence of the training data are fol-lowed with an ‘informal’ or ‘polite’ tag indicatingthe level of politeness expressed.3Compilation of DatasetsOne of the main obstacles for more personalizedMT systems is ﬁnding large enough annotatedparallel datasets with speaker information. Rabi-novich et al. (2017) published an annotated paral-lel dataset for EN–FR and EN–DE. However, formany other language pairs no sufﬁciently large an-notated datasets are available.To address the aforementioned problem, wepublished online a collection of parallel corporalicensed under the Creative Commons Attribu-tion 4.0 International License for 20 languagepairs (Vanmassenhove and Hardmeier, 2018).2We followed the approach described by Rabi-novich et al. (2017) and tagged parallel sentencesfrom Europarl (Koehn, 2005) with speaker infor-mation (name, gender, age, date of birth, euroIDand date of the session) by retrieving speaker in-formation provided by tags in the Europarl sourceﬁles. The Europarl source ﬁles contain informa-tion about the speaker on the paragraph level andthe ﬁlenames contain the data of the session. Byretrieving the names of the speakers together withmeta-information on the members of the Euro-pean Parliament (MEPs) released by Rabinovichet al. (2017) (which includes among others name,country, date of birth and gender predictions perMEP), we were able to retrieve demographic an-notations (gender, age, etc.). An overview of thelanguage pairs as well as the amount of annotatedparallel sentences per language pair is given in Ta-ble 1.3.1Analysis of the EN–FR AnnotatedDatasetWe ﬁrst analysed the distribution of male and fe-male sentence in our data.In the 10 different2https://github.com/evavnmssnhv/Europarl-Speaker-Information\f3005Languages# sentsLanguages# sentsEN–BG306,380EN–IT1,297,635EN–CS491,848EN–LT481,570EN–DA1,421,197EN–LV487,287EN–DE1,296,843EN–NL1,419,359EN–EL921,540EN–PL478,008EN–ES1,419,507EN–PT1,426,043EN–ET494,645EN–RO303,396EN–FI1,393,572EN–SK488,351EN–FR1,440,620EN–SL479,313EN–HU251,833EN–SV1,349,472Table 1: Overview of annotated parallel sentences per lan-guage pairdatasets we experimented with, the percentage ofsentences uttered by female speakers is very sim-ilar, ranging between 32% and 33%. This simi-larity can be explained by the fact that Europarlis largely a multilingual corpus with a big overlapbetween the different language pairs.We conducted a more focused analysis on oneof the subcorpora (EN–FR) with respect to thepercentage of sentences uttered by males/femalesfor various age groups to obtain a better grasp ofwhat kind of data we are using for training. Ascan be seen from Figure 1, with the exception ofthe youngest age group (20–30), which representsonly a very small percentage of the total amountof sentences (0.71%), more male data is availablein all age groups. Furthermore, when looking atthe entire dataset, 67.39% of the sentences are pro-duced by male speakers. Moreover, almost half ofthe total number of sentences are uttered by the50–60 age group (43.76%).20-30 30-40 40-50 50-60 60-70 70-80 80-900%10%20%30%40%50%60%70%80%90%100%MaleFemaleAge groupsFigure 1: Percentage of female and male speakers per agegroupThe analysis shows that indeed, there is a gen-der unbalance in the Europarl dataset, which willbe reﬂected in the translations that MT systemstrained on this data produce.4Experimental Setup4.1DatasetsWe carried out a set of experiments on 10 lan-guage pairs (the ones for which we compiled morethan 500k annotated Europarl parallel sentences):EN–DE, EN–FR, EN–ES, EN–EL, EN–PT, EN–FI, EN–IT, EN–SV, EN–NL and EN–DA. We aug-mented every sentence with a tag on the Englishsource side, identifying the gender of the speaker,as illustrated in (1). This approach for encodingsentence-speciﬁc information for NMT has beensuccessfully exploited to tackle other types of is-sues, multilingual NMT systems (e.g., Zero ShotTranslation (Johnson et al., 2017)), domain adap-tation (Sennrich et al., 2016), etc.(1)“FEMALE Madam President, as a...”For each of these language pairs we trained twoNMT systems: a baseline and a tagged one. Weevaluated the performance of all our systems on arandomly selected 2K general test set. Moreover,we further evaluated the EN–FR systems on 2Kmale-only and female-only test sets to have a lookat the system performance with respect to gender-related issues. We also looked at two additionalmale and female test sets in which the ﬁrst personsingular pronoun appeared.4.2Description of the NMT SystemsWe used the OpenNMT-py toolkit (Klein et al.,2017) to train the NMT models.The modelsare sequence-to-sequence encoder-decoders withLSTMs as the recurrent unit (Bahdanau et al.,2014; Cho et al., 2014; Sutskever et al., 2014)trained with the default parameters. In order to by-pass the OOV problem and reduce the number ofdictionary entries, we use word-segmentation withBPE (Sennrich, 2015). We ran the BPE algorithmwith 89,500 operations (Sennrich, 2015). All sys-tems are trained for 13 epochs and the best modelis selected for evaluation.5ResultsIn this section we discuss some of the results ob-tained.We hypothesized that the male/femaletags would be particularly helpful for French, Por-tuguese, Italian, Spanish and Greek, where adjec-tives and even verb forms can be marked by thegender of the speaker.Since, according to theliterature, women and men also make use of dif-ferent syntactic constructions and make different\f3006word choices, we also tested the approach on otherlanguages that do not have morphological agree-ment with the gender of the speaker such as Dan-ish (DA), Dutch (NL), Finnish (FI), German (DE)and Swedish (SV).First, we wanted to see how our tagged systemsperformed on the general test set compared to thebaseline. In Table 2, the BLEU scores for 10 base-line and 10 gender-enhanced NMT systems arepresented.SystemsENEN-TAGFR37.8239.26*ES42.4742.28EL31.3831.54IT31.4631.75*PT36.1136.33DA36.6937.00*DE28.2828.05FI21.8221.35*SV35.4235.19NL28.3528.22Table 2: BLEU scores for the 10 baseline (denoted withEN) and the 10 gender-enhanced NMT (denoted with EN-TAG) systems. Entries labeled with * present statisticallysigniﬁcant differences (p < 0.05). Statistical signiﬁcance wascomputed with the MultEval tool (Clark et al., 2011).While most of the BLEU-scores (Papineni et al.,2002) in Table 2 are consistent with our hy-pothesis, showing (signiﬁcant) improvements forthe NMT systems enriched with a gender tag(EN-TAG) over the baseline systems (EN) forFrench, Italian, Portuguese and Greek, the Span-ish enriched system surprisingly does not (–0.19BLEU). As hypothesized, the Dutch, German,Finnish and Swedish systems do not improve.However, the Danish (EN–DA) enriched NMTsystem does achieve a signiﬁcant +0.31 BLEU im-provement.We expected to see the strongest improvementsin sentences uttered by female speakers as, accord-ing to our initial analysis, the male data was over-represented in the training. To test this hypothe-sis, we evaluated all systems on a male-only andfemale-only test set. Furthermore, we also experi-mented on test sets containing the pronoun of theﬁrst person singular as this form is used when aspeaker refers to himself/herself. The results onthe speciﬁc test set for the EN–FR dataset are pre-sented in Table 3. As hypothesized, the biggestBLEU score improvement is observed on the fe-male test set, particularly, the test sets containingﬁrst person singular pronouns (F1).We had a closer look at some of the transla-Test SetsENEN-TAGFR (M)37.5838.71*FR (F)37.7538.97*FR (M1)39.0039.66*FR (F1)37.3238.57*Table 3: BLEU-scores on EN–FR comparing the baseline(EN) and the tagged systems (EN–TAG) on 4 different testsets: a test set containing only male data (M), only femaledata (F), 1st person male data (M1) and ﬁrst person femaledata (F1). All the improvements of the EN-TAG system arestatistically signiﬁcant (p < 0.5), as indicated by *.tions.3 There are cases where the gender-informed(TAG) system improves over the baseline (BASE)due to better agreement.Interestingly, in (2)the French female form of vice-president (vice-pr´esidente) appears in the translation produced bythe BASE system while the male form is the cor-rect one. The gender-informed system does makethe correct agreement by using the female variant.In (3) the speaker is female but the baseline sys-tem outputs a male form of the adjective ‘happy’(‘heureux’).(2)(Ref)En tant que vice-pr´esident...(BASE)En tant que vice-pr´esidente...(TAG)En tant que vice-pr´esident...(3)(Ref)... je suis heureuse que...(BASE)... je suis heureux que...(TAG)... je suis heureuse que...However, we also encountered cases where thegender-informed system fails to produce the cor-rect agreement, as in (4), where both the BASEand the TAG system produce a male form (‘em-barass´e’) instead of the correct female one (‘em-barass´ee’ or ‘gˆen´ee’).(4)(Ref)je suis gˆen´ee que...(BASE)je suis embarass´e que...(TAG)je suis embarass´e que...For some language pairs the gender-informedsystem leads to a signiﬁcant improvement evenon a general test set. This implies that the im-provement is not merely because of better mor-phological agreement, as these kinds of improve-ments are very hard to measure with BLEU, espe-cially given the fact that Europarl consists of for-mal spoken language and does not contain manysentences using the ﬁrst person singular pronoun.From our analysis, we observe that in many casesthe gender-informed systems have a higher BLEU3We used the tool provided by Tilde https://www.letsmt.eu/Bleu.aspx to see where the BLEU scorebetween the baseline and our tagged systems varied the most.\f3007score than the baseline system due to differencesin word choices as in (5) and (6), where both trans-lations are correct, but the gender-informed sys-tem picks the preferred variant.The observations with respect to differences inword preferences between male and female speak-ers are in accordance with corpus linguistic stud-ies, which have shown that gender does not onlyhave an effect on morphological agreement, butalso manifests itself in other ways as males andfemales have different preferences when it comesto different types of constructions, word choicesetc. (Newman et al., 2008; Coates, 2015). Thisalso implies that, even for languages that do notmark gender overtly (i.e. grammatically), it canstill be beneﬁcial to take the gender of the au-thor/speaker into account.(5)(Ref)Je pense que ...(BASE)Je crois que...(TAG)Je pense que...Although more research is required in orderto draw general conclusions on this matter, fromother linguistic studies, it appears that it is indeedthe case that there is a relation between the useof the word “pense” (“think”) / “crois” (“believe”)and the gender of the speaker.To see whetherthere is a difference in word choice and whetherthis is reﬂected in our data, we compiled a listof the most frequent French words for the maledata and the female data. Our analysis reveals that“crois” is, in general, used more by males (hav-ing position 303 in the most frequent words formales, but only position 373 for females), while“pense” is found at a similar position in both lists(position 151 and 153). These ﬁndings are in ac-cordance with other linguistic corpus studies onlanguage and gender stating that women use lessassertive speech (Newman et al., 2008). “Croire”and “penser” are both verbs of cognition but thereis a difference in the degree of conﬁdence in thetruth value predicated: the verb “croire” denotesmore conﬁdence in the truth of the complementclause than the verb “penser” does. In the future,we would like to perform a more detailed analy-sis of other speciﬁc differences in lexical choicesbetween males and females on multiple languagepairs.(6)(Ref)J’ ai plusieurs remarques...(BASE)J’ ai un nombre de commentaires...(TAG)J’ ai plusieurs remarques...6Conclusions and Future WorkIn this work, we experimented with the incorpora-tion of speaker-gender tags during the training ofNMT systems in order to improve morphologicalagreement. We focused particularly on languagepairs that express grammatical gender but includedother language pairs as well, as linguistic studieshave shown that the style and syntax of languageused by males and females differs (Coates, 2015).From the experiments, we see that informingthe NMT system by providing tags indicating thegender of the speaker can indeed lead to signif-icant improvements over state-of-the-art baselinesystems, especially for those languages expressinggrammatical gender agreement. However, whileanalyzing the EN–FR translations, we observedthat the improvements are not always consistentand that, apart from morphological agreement, thegender-aware NMT system differs from the base-line in terms of word choices.In the future, we would like to conduct fur-ther manual evaluation on the translations to fur-ther analyze the differences with the baseline sys-tem.Furthermore, we aim to experiment withother ways of integrating speaker information. Weenvisage working on gender classiﬁcation tech-niques in order to work on other types (more in-formal) of corpora that are more likely to expressspeaker characteristics.AcknowledgementsThis work has been supported by COST actionIS1312, the Dublin City University Faculty of En-gineering & Computing under the Daniel O’HareResearch Scholarship scheme and by the ADAPTCentre for Digital Content Technology, whichis funded under the SFI Research Centres Pro-gramme (Grant 13/RC/2106).Christian Hard-meier was supported by the Swedish ResearchCouncil under grant 2017-930.We would also like to thank the anonymous re-viewers for their insightful comments and feed-back.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. 2014. Neural Machine Translation by JointlyLearning to Align and Translate. In InternationalConference on Learning Representations, Banff,Canada.\f3008KyunghyunCho,BartvanMerri¨enboer,C¸ alarG¨ulc¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio. 2014.Learn-ing Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In Pro-ceedings of EMNLP 2014, pages 1724–1734, Doha,Qatar.Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah ASmith. 2011. Better Hypothesis Testing for Statisti-cal Machine Translation: Controlling for OptimizerInstability. In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies: short papers-Volume 2, pages 176–181. Association for Compu-tational Linguistics.Jennifer Coates. 2015. Women, Men and Language:A Sociolinguistic Account of Gender Differences inLanguage. Routledge, London.Melvin Johnson, Mike Schuster, Quoc V Le, MaximKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,Fernanda Vi´egas, Martin Wattenberg, Greg Corrado,et al. 2017. Google’s Multilingual Neural MachineTranslation System: Enabling Zero-Shot Transla-tion. Transactions of the Association of Computa-tional Linguistics, 5(1):339–351.Guillaume Klein, Yoon Kim, Yuntian Deng, JeanSenellart, and Alexander M. Rush. 2017.Open-NMT: Open-Source Toolkit for Neural MachineTranslation.In Proceeding of ACL, Vancouver,Canada.Philipp Koehn. 2005. Europarl: A Parallel Corpus forStatistical Machine Translation. In MT Summit, vol-ume 5, pages 79–86, Phuket, Thailand.Tatiana Litvinova, Pavel Seredin, Olga Litvinova,Olga Zagorovskaya, Aleksandr Sboev, Dmitry Gu-dovskih, Ivan Moloshnikov, and Roman Rybka.2016.Gender Prediction for Authors of RussianTexts Using Regression And Classiﬁcation Tech-niques. In Proceedings of the Third Workshop onConcept Discovery in Unstructured Data co-locatedwith the 13th International Conference on ConceptLattices and Their Applications (CDUD@ CLA),pages 44–53, Moscow, Russia.Shachar Mirkin, Scott Nowson, Caroline Brun, andJulien Perez. 2015. Motivating Personality-AwareMachine Translation.In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 1102–1108, Lisbon, Portu-gal.Matthew L Newman, Carla J Groom, Lori D Handel-man, and James W Pennebaker. 2008. Gender Dif-ferences in Language Use: An Analysis of 14,000Text Samples. Discourse Processes, 45(3):211–236.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for AutomaticEvaluation of Machine Translation. In Proceedingsof the 40th annual meeting on association for com-putational linguistics, pages 311–318.James W. Pennebaker, Matthias R. Mehl, and Kate G.Niederhoffer. 2003. Psychological Aspects of Natu-ral Language Use: Our words, Our Selves. Annualreview of psychology, 54(1):547–577.Ella Rabinovich, Raj Nath Patel, Shachar Mirkin, Lu-cia Specia, and Shuly Wintner. 2017. PersonalizedMachine Translation: Preserving Original AuthorTraits.In Proceedings of the 15th Conference ofthe European Chapter of the Association for Compu-tational Linguistics: Volume 1, Long Papers, pages1074–1084, Valencia, Spain.Francisco Rangel, Paolo Rosso, Moshe Koppel, Ef-stathios Stamatatos, and Giacomo Inches. 2013.Overview of The Author Proﬁling Task at PAN2013.In CLEF Conference on Multilingual andMultimodal Information Access Evaluation, pages352–365.K Santosh, Romil Bansal, Mihir Shekhar, and Va-sudeva Varma. 2013. Author proﬁling: Predictingage and gender from blogs. Notebook for PAN atCLEF, pages 119–124.Edward Sapir. 1921.Language: An Introduction tothe Study of Speech. NewYork: Harcourt Brace &Company.Rico Sennrich. 2015.Modelling and Optimizing onSyntactic N-grams for Statistical Machine Transla-tion. Transactions of the Association for Computa-tional Linguistics, 3:169–182.Rico Sennrich, Barry Haddow, and Alexandra Birch.2016.Controlling Politeness in Neural MachineTranslation via Side Constraints. In Proceedings ofthe 2016 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 35–40,Berlin, Germany.Dan I. Slobin. 1996. From Thought and Language toThinking for Speaking. In J. Gumperz and S. Levin-son, editors, Rethinking Linguistic Relativity, pages70–96. Cambridge University Press, Cambridge.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.Sequence to Sequence Learning with Neural Net-works.In Advances in Neural Information Pro-cessing Systems 27: Annual Conference on NeuralInformation Processing Systems, pages 3104–3112,Montreal, Quebec, Canada.Deborah Tannen. 1991.You Just Don’t Understand.Ballantine Books, New York, USA.Eva Vanmassenhove and Christian Hardmeier. 2018.Europarl Datasets with Demographic Speaker Infor-mation. In EAMT, Alicante, Spain.",
        "answer": "20",
        "space_key_index": 0,
        "metadata": {
            "format": "PDF 1.3",
            "title": "Getting Gender Right in Neural Machine Translation",
            "author": "Eva Vanmassenhove ; Christian Hardmeier ; Andy Way",
            "subject": "EMNLP 2018",
            "keywords": "",
            "creator": "LaTeX with hyperref package",
            "producer": "pdfTeX-1.40.18",
            "creationDate": "D:20180827215344Z",
            "modDate": "D:20180827215344Z",
            "trapped": "",
            "encryption": null
        }
    }
]