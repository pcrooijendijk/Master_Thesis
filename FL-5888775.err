INFO:datasets:PyTorch version 2.5.1 available.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Traceback (most recent call last):
  File "/vol/csedu-nobackup/project/prooijendijk/Master_Thesis/FL_main.py", line 197, in <module>
    fire.Fire(federated_privacy_learning)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/vol/csedu-nobackup/project/prooijendijk/Master_Thesis/FL_main.py", line 148, in federated_privacy_learning
    model = get_peft_model(model, lora_config)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/mapping.py", line 136, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/peft_model.py", line 1094, in __init__
    super().__init__(model, peft_config, adapter_name)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/peft_model.py", line 129, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/tuners/lora/model.py", line 136, in __init__
    super().__init__(model, config, adapter_name)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 148, in __init__
    self.inject_adapter(self.model, adapter_name)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 325, in inject_adapter
    self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/tuners/lora/model.py", line 220, in _create_and_replace
    new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/tuners/lora/model.py", line 295, in _create_new_module
    new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)
  File "/vol/csedu-nobackup/project/prooijendijk/myenv/lib/python3.10/site-packages/peft/tuners/lora/bnb.py", line 262, in dispatch_bnb_8bit
    "memory_efficient_backward": target.state.memory_efficient_backward,
AttributeError: 'MatmulLtState' object has no attribute 'memory_efficient_backward'
