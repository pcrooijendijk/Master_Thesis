from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import torch
import json
from more_itertools import chunked

# Set up device and clear CUDA cache
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.cuda.empty_cache()

# Step 1: Load and parse the JSON
with open("utils/documents.json", "r", encoding="utf-8") as f:
    raw_json = json.load(f)

# Step 2: Extract only the "context" field and metadata
documents = [
    Document(
        page_content=entry["context"],
        metadata={"space_key_index": entry["space_key_index"]}
    )
    for entry in raw_json
]

# Step 3: Split documents into smaller chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# Step 4: Set up HuggingFace embeddings
model_kwargs = {'device': device}
encode_kwargs = {'normalize_embeddings': True, 'batch_size': 8}

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-en-v1.5", 
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# Step 5: Build FAISS index in batches
faiss_index = None
batch_size = 50  # You can tweak this depending on your RAM

for doc_batch in chunked(docs, batch_size):
    if faiss_index is None:
        faiss_index = FAISS.from_documents(doc_batch, embeddings)
    else:
        faiss_index.add_documents(doc_batch)

# Step 6: Run a similarity search
query = "Are LIME and Alvarez-Melis and Jaakkola (2017) methods dependent on model properties?"
result_docs = faiss_index.similarity_search(query)

text_splitter_recursive = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
)
result = """
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443–4458July 5 - 10, 2020. c⃝2020 Association for Computational Linguistics4443ERASER: A Benchmark to Evaluate Rationalized NLP ModelsJay DeYoung⋆Ψ, Sarthak Jain⋆Ψ, Nazneen Fatema Rajani⋆Φ, Eric LehmanΨ,Caiming XiongΦ, Richard SocherΦ, and Byron C. WallaceΨ⋆Equal contribution.ΨKhoury College of Computer Sciences, Northeastern UniversityΦSalesforce Research, Palo Alto, CA, 94301AbstractState-of-the-art models in NLP are now pre-dominantly based on deep neural networksthat are opaque in terms of how they cometo make predictions.This limitation hasincreased interest in designing more inter-pretable deep models for NLP that reveal the‘reasoning’ behind model outputs. But workin this direction has been conducted on dif-ferent datasets and tasks with correspondinglyunique aims and metrics; this makes it difﬁcultto track progress. We propose the EvaluatingRationales And Simple English Reasoning(ERASER) benchmark to advance researchon interpretable models in NLP. This bench-mark comprises multiple datasets and tasks forwhich human annotations of “rationales” (sup-porting evidence) have been collected. We pro-pose several metrics that aim to capture howwell the rationales provided by models alignwith human rationales, and also how faithfulthese rationales are (i.e., the degree to whichprovided rationales inﬂuenced the correspond-ing predictions). Our hope is that releasing thisbenchmark facilitates progress on designingmore interpretable NLP systems. The bench-mark, code, and documentation are availableat https://www.eraserbenchmark.com/1IntroductionInterest has recently grown in designing NLP sys-tems that can reveal why models make speciﬁcpredictions. But work in this direction has beenconducted on different datasets and using differentmetrics to quantify performance; this has made itdifﬁcult to compare methods and track progress.We aim to address this issue by releasing a stan-dardized benchmark of datasets — repurposed andaugmented from pre-existing corpora, spanning arange of NLP tasks — and associated metrics formeasuring different properties of rationales. We re-fer to this as the Evaluating Rationales And SimpleEnglish Reasoning (ERASER) benchmark.Commonsense Explanations (CoS-E)Where do you ﬁnd the most amount of leafs?(a) Compost pile  (b) Flowers  (c) Forest  (d) Field  (e) GroundMovie ReviewsIn this movie, … Plots to take over the world. The acting is great! The soundtrack is run-of-the-mill, but the action more than makes up for it(a) Positive  (b) NegativeEvidence InferenceArticle Patients for this trial were recruited … Compared with 0.9% saline, 120 mg of inhaled nebulized furosemide had no eﬀect on breathlessness during exercise. (a) Sig. decreased  (b) No sig. diﬀerence (c) Sig. increasedPrompt With respect to breathlessness, what is the reported diﬀerence between patients receiving placebo and those receiving furosemide?e-SNLIH A man in an orange vest leans over a pickup truckP A man is touching a truck (a) Entailment  (b) Contradiction  (c) NeutralFigure 1: Examples of instances, labels, and rationalesillustrative of four (out of seven) datasets included inERASER. The ‘erased’ snippets are rationales.In curating and releasing ERASER we take in-spiration from the stickiness of the GLUE (Wanget al., 2019b) and SuperGLUE (Wang et al., 2019a)benchmarks for evaluating progress in natural lan-guage understanding tasks, which have driven rapidprogress on models for general language repre-sentation learning. We believe the still somewhatnascent subﬁeld of interpretable NLP stands to ben-eﬁt similarly from an analogous collection of stan-dardized datasets and tasks; we hope these willaid the design of standardized metrics to measuredifferent properties of ‘interpretability’, and wepropose a set of such metrics as a starting point.Interpretability is a broad topic with many possi-ble realizations (Doshi-Velez and Kim, 2017; Lip-ton, 2016). In ERASER we focus speciﬁcally onrationales, i.e., snippets that support outputs. Alldatasets in ERASER include such rationales, ex-plicitly marked by human annotators. By deﬁnition,rationales should be sufﬁcient to make predictions,\f4444but they may not be comprehensive. Therefore, forsome datasets, we have also collected comprehen-sive rationales (in which all evidence supportingan output has been marked) on test instances.The ‘quality’ of extracted rationales will dependon their intended use. Therefore, we propose aninitial set of metrics to evaluate rationales thatare meant to measure different varieties of ‘inter-pretability’. Broadly, this includes measures ofagreement with human-provided rationales, and as-sessments of faithfulness. The latter aim to capturethe extent to which rationales provided by a modelin fact informed its predictions. We believe theseprovide a reasonable start, but view the problem ofdesigning metrics for evaluating rationales — espe-cially for measuring faithfulness — as a topic forfurther research that ERASER can facilitate. Andwhile we will provide a ‘leaderboard’, this is betterviewed as a ‘results board’; we do not privilegeany one metric. Instead, ERASER permits compar-ison between models that provide rationales withrespect to different criteria of interest.We implement baseline models and report theirperformance across the corpora in ERASER. Weﬁnd that no single ‘off-the-shelf’ architecture isreadily adaptable to datasets with very differentinstance lengths and associated rationale snippets(Section 3). This highlights a need for new modelsthat can consume potentially lengthy inputs andadaptively provide rationales at a task-appropriatelevel of granularity. ERASER provides a resourceto develop such models.In sum, we introduce the ERASER benchmark(www.eraserbenchmark.com), a uniﬁed set of di-verse NLP datasets (these are repurposed and aug-mented from existing corpora,1 including senti-ment analysis, Natural Language Inference, andQA tasks, among others) in a standardized for-mat featuring human rationales for decisions, alongwith starter code and tools, baseline models, andstandardized (initial) metrics for rationales.2Related WorkInterpretability in NLP is a large, fast-growingarea; we do not attempt to provide a comprehensiveoverview here. Instead we focus on directions par-ticularly relevant to ERASER, i.e., prior work onmodels that provide rationales for their predictions.Learning to explain. In ERASER we assume that1We ask users of the benchmark to cite all original papers,and provide a BibTeX entry for doing so on the website.rationales (marked by humans) are provided duringtraining. However, such direct supervision will notalways be available, motivating work on methodsthat can explain (or “rationalize”) model predic-tions using only instance-level supervision.In the context of modern neural models for textclassiﬁcation, one might use variants of attention(Bahdanau et al., 2015) to extract rationales. At-tention mechanisms learn to assign soft weights to(usually contextualized) token representations, andso one can extract highly weighted tokens as ratio-nales. However, attention weights do not in gen-eral provide faithful explanations for predictions(Jain and Wallace, 2019; Serrano and Smith, 2019;Wiegreffe and Pinter, 2019; Zhong et al., 2019;Pruthi et al., 2020; Brunner et al., 2020; Moradiet al., 2019; Vashishth et al., 2019). This likelyowes to encoders entangling inputs, complicatingthe interpretation of attention weights on inputsover contextualized representations of the same.2By contrast, hard attention mechanisms dis-cretely extract snippets from the input to pass to theclassiﬁer, by construction providing faithful expla-nations. Recent work has proposed hard attentionmechanisms as a means of providing explanations.Lei et al. (2016) proposed instantiating two modelswith their own parameters; one to extract rationales,and one that consumes these to make a prediction.They trained these models jointly via REINFORCE(Williams, 1992) style optimization.Recently, Jain et al. (2020) proposed a variantof this two-model setup that uses heuristic featurescores to derive pseudo-labels on tokens compris-ing rationales; one model can then be used to per-form hard extraction in this way, while a second(independent) model can make predictions on thebasis of these. Elsewhere, Chang et al. (2019)introduced the notion of classwise rationales thatexplains support for different output classes usinga game theoretic framework. Finally, other recentwork has proposed using a differentiable binarymask over inputs, which also avoids recourse toREINFORCE (Bastings et al., 2019).Post-hoc explanation. Another strand of inter-pretability work considers post-hoc explanationmethods, which seek to explain why a model madea speciﬁc prediction for a given input. Commonly2Interestingly, Zhong et al. (2019) ﬁnd that attention some-times provides plausible but not faithful rationales. Elsewhere,Pruthi et al. (2020) show that one can easily learn to deceivevia attention weights. These ﬁndings highlight that one shouldbe mindful of the criteria one wants rationales to fulﬁll.\f4445these take the form of token-level importancescores. Gradient-based explanations are a standardexample (Sundararajan et al., 2017; Smilkov et al.,2017). These enjoy a clear semantics (describinghow perturbing inputs locally affects outputs), butmay nonetheless exhibit counterintuitive behaviors(Feng et al., 2018).Gradients of course assume model differentia-bility. Other methods do not require any modelproperties. Examples include LIME (Ribeiro et al.,2016) and Alvarez-Melis and Jaakkola (2017);these methods approximate model behavior lo-cally by having it repeatedly make predictions overperturbed inputs and ﬁtting a simple, explainablemodel over the outputs.Acquiring rationales. Aside from interpretabilityconsiderations, collecting rationales from annota-tors may afford greater efﬁciency in terms of modelperformance realized given a ﬁxed amount of anno-tator effort (Zaidan and Eisner, 2008). In particular,recent work by McDonnell et al. (2017, 2016) hasobserved that at least for some tasks, asking anno-tators to provide rationales justifying their catego-rizations does not impose much additional effort.Combining rationale annotation with active learn-ing (Settles, 2012) is another promising direction(Wallace et al., 2010; Sharma et al., 2015).Learning from rationales. Work on learning fromrationales marked by annotators for text classiﬁca-tion dates back over a decade (Zaidan et al., 2007).Earlier efforts proposed extending standard dis-criminative models like Support Vector Machines(SVMs) with regularization terms that penalizedparameter estimates which disagreed with providedrationales (Zaidan et al., 2007; Small et al., 2011).Other efforts have attempted to specify generativemodels of rationales (Zaidan and Eisner, 2008).More recent work has aimed to exploit ratio-nales in training neural text classiﬁers. Zhang et al.(2016) proposed a rationale-augmented Convolu-tional Neural Network (CNN) for text classiﬁca-tion, explicitly trained to identify sentences support-ing categorizations. Strout et al. (2019) showed thatproviding this model with rationales during train-ing yields predicted rationales that are preferredby humans (compared to rationales produced with-out explicit supervision). Other work has proposed‘pipeline’ approaches in which independent mod-els are trained to perform rationale extraction andclassiﬁcation on the basis of these, respectively(Lehman et al., 2019; Chen et al., 2019), assumingNameSize (train/dev/test)TokensComp?Evidence Inference7958 / 972 / 9594761◇BoolQ6363 / 1491 / 28173583◇Movie Reviews1600 / 200 / 200774◆FEVER97957 / 6122 / 6111327!MultiRC24029 / 3214 / 4848303!CoS-E8733 / 1092 / 109228!e-SNLI911938 / 16449 / 1642916!Table 1: Overview of datasets in the ERASER bench-mark. Tokens is the average number of tokens in eachdocument. Comprehensive rationales mean that all sup-porting evidence is marked; !denotes cases where thisis (more or less) true by default; ◇, ◆are datasets forwhich we have collected comprehensive rationales foreither a subset or all of the test datasets, respectively.Additional information can be found in Appendix A..explicit training data is available for the former.Rajani et al. (2019) ﬁne-tuned a Transformer-based language model (Radford et al., 2018) onfree-text rationales provided by humans, with anobjective of generating open-ended explanations toimprove performance on downstream tasks.Evaluating rationales. Work on evaluating ratio-nales has often compared these to human judg-ments (Strout et al., 2019; Doshi-Velez and Kim,2017), or elicited other human evaluations of ex-planations (Ribeiro et al., 2016; Lundberg and Lee,2017; Nguyen, 2018). There has also been work onvisual evaluations of saliency maps (Li et al., 2016;Ding et al., 2017; Sundararajan et al., 2017).Measuring agreement between extracted andhuman rationales (or collecting subjective assess-ments of them) assesses the plausibility of ratio-nales, but such approaches do not establish whetherthe model actually relied on these particular ratio-nales to make a prediction. We refer to rationalesthat correspond to the inputs most relied upon tocome to a disposition as faithful.Most automatic evaluations of faithfulness mea-sure the impact of perturbing or erasing words ortokens identiﬁed as important on model output (Ar-ras et al., 2017; Montavon et al., 2017; Serrano andSmith, 2019; Samek et al., 2016; Jain and Wallace,2019). We build upon these methods in Section4. Finally, we note that a recent article urges thecommunity to evaluate faithfulness on a continuousscale of acceptability, rather than viewing this as abinary proposition (Jacovi and Goldberg, 2020).3Datasets in ERASERFor all datasets in ERASER we distribute both ref-erence labels and rationales marked by humansas supporting these in a standardized format. We\f4446delineate train, validation, and test splits for allcorpora (see Appendix A for processing details).We ensure that these splits comprise disjoint setsof source documents to avoid contamination.3 Wehave made the decision to distribute the test setspublicly,4 in part because we do not view the ‘cor-rect’ metrics to use as settled. We plan to acquireadditional human annotations on held-out portionsof some of the included corpora so as to offer hid-den test set evaluation opportunities in the future.Evidence inference (Lehman et al., 2019).Adataset of full-text articles describing randomizedcontrolled trials (RCTs).The task is to inferwhether a given intervention is reported to eithersigniﬁcantly increase, signiﬁcantly decrease, orhave no signiﬁcant effect on a speciﬁed outcome, ascompared to a comparator of interest. Rationaleshave been marked as supporting these inferences.As the original annotations are not necessarily ex-haustive, we collected exhaustive rationale annota-tions on a subset of the validation and test data.5BoolQ (Clark et al., 2019). This corpus consistsof passages selected from Wikipedia, and yes/noquestions generated from these passages. As theoriginal Wikipedia article versions used were notmaintained, we have made a best-effort attempt torecover these, and then ﬁnd within them the pas-sages answering the corresponding questions. Forpublic release, we acquired comprehensive annota-tions on a subset of documents in our test set.5Movie Reviews (Zaidan and Eisner, 2008). In-cludes positive/negative sentiment labels on moviereviews. Original rationale annotations were notnecessarily comprehensive; we thus collected com-prehensive rationales on the ﬁnal two folds of theoriginal dataset (Pang and Lee, 2004).5 In contrastto most other datasets, the rationale annotationshere are span level as opposed to sentence level.FEVER (Thorne et al., 2018). Short for Fact Ex-traction and VERiﬁcation; entails verifying claimsfrom textual sources. Speciﬁcally, each claim is tobe classiﬁed as supported, refuted or not enoughinformation with reference to a collection of source3Except for BoolQ, wherein source documents in the orig-inal train and validation set were not disjoint and we preservethis structure in our dataset. Questions, of course, are disjoint.4Consequently, for datasets that have been part of previ-ous benchmarks with other aims (namely, GLUE/superGLUE)but which we have re-purposed for work on rationales inERASER, e.g., BoolQ (Clark et al., 2019), we have carved outfor release test sets from the original validation sets.5Annotation details are in Appendix B.texts. We take a subset of this dataset, includingonly supported and refuted claims.MultiRC (Khashabi et al., 2018). A reading com-prehension dataset composed of questions withmultiple correct answers that by construction de-pend on information from multiple sentences. Hereeach rationale is associated with a question, whileanswers are independent of one another. We con-vert each rationale/question/answer triplet into aninstance within our dataset. Each answer candidatethen has a label of True or False.Commonsense Explanations (CoS-E) (Rajaniet al., 2019).This corpus comprises multiple-choice questions and answers from (Talmor et al.,2019) along with supporting rationales. The ratio-nales in this case come in the form both of high-lighted (extracted) supporting snippets and free-text, open-ended descriptions of reasoning. Givenour focus on extractive rationales, ERASER in-cludes only the former for now. Following Talmoret al. (2019), we repartition the training and valida-tion sets to provide a canonical test split.e-SNLI (Camburu et al., 2018). This dataset aug-ments the SNLI corpus (Bowman et al., 2015) withrationales marked in the premise and/or hypothesis(and natural language explanations, which we donot use). For entailment pairs, annotators were re-quired to highlight at least one word in the premise.For contradiction pairs, annotators had to highlightat least one word in both the premise and the hy-pothesis; for neutral pairs, they were only allowedto highlight words in the hypothesis.Human Agreement We report human agreementover extracted rationales for multiple annotatorsand documents in Table 2. All datasets have a highCohen κ (Cohen, 1960); with substantial or betteragreement.4MetricsIn ERASER models are evaluated both for theirpredictive performance and with respect to the ra-tionales that they extract. For the former, we relyon the established metrics for the respective tasks.Here we describe the metrics we propose to eval-uate the quality of extracted rationales. We donot claim that these are necessarily the best met-rics for evaluating rationales, however. Indeed, wehope the release of ERASER will spur additionalresearch into how best to measure the quality ofmodel explanations in the context of NLP.\f4447DatasetCohen κF1PR#Annotators/doc#DocumentsEvidence Inference------BoolQ0.618 ± 0.1940.617 ± 0.2270.647 ± 0.2600.726 ± 0.2173199Movie Reviews0.712 ± 0.1350.799 ± 0.1380.693 ± 0.1530.989 ± 0.102296FEVER0.854 ± 0.1960.871 ± 0.1970.931 ± 0.2050.855 ± 0.198224MultiRC0.728 ± 0.2680.749 ± 0.2650.695 ± 0.2840.910 ± 0.259299CoS-E0.619 ± 0.3080.654 ± 0.3170.626 ± 0.3190.792 ± 0.3712100e-SNLI0.743 ± 0.1620.799 ± 0.1300.812 ± 0.1540.853 ± 0.12439807Table 2: Human agreement with respect to rationales. For Movie Reviews and BoolQ we calculate the meanagreement of individual annotators with the majority vote per token, over the two-three annotators we hired viaUpwork and Amazon Turk, respectively. The e-SNLI dataset already comprised three annotators; for this wecalculate mean agreement between individuals and the majority. For CoS-E, MultiRC, and FEVER, members ofour team annotated a subset to use a comparison to the (majority of, where appropriate) existing rationales. Wecollected comprehensive rationales for Evidence Inference from Medical Doctors; as they have a high amount ofexpertise, we would expect agreement to be high, but have not collected redundant comprehensive annotations.4.1Agreement with human rationalesThe simplest means of evaluating extracted ratio-nales is to measure how well they agree with thosemarked by humans. We consider two classes ofmetrics, appropriate for models that perform dis-crete and ‘soft’ selection, respectively.For the discrete case, measuring exact matchesbetween predicted and reference rationales is likelytoo harsh.6 We thus consider more relaxed mea-sures.These include Intersection-Over-Union(IOU), borrowed from computer vision (Evering-ham et al., 2010), which permits credit assignmentfor partial matches. We deﬁne IOU on a token level:for two spans, it is the size of the overlap of thetokens they cover divided by the size of their union.We count a prediction as a match if it overlaps withany of the ground truth rationales by more thansome threshold (here, 0.5). We use these partialmatches to calculate an F1 score. We also measuretoken-level precision and recall, and use these toderive token-level F1 scores.Metrics for continuous or soft token scoringmodels consider token rankings, rewarding modelsfor assigning higher scores to marked tokens. Inparticular, we take the Area Under the Precision-Recall curve (AUPRC) constructed by sweeping athreshold over token scores. We deﬁne additionalmetrics for soft scoring models below.In general, the rationales we have for tasks aresufﬁcient to make judgments, but not necessarilycomprehensive. However, for some datasets wehave explicitly collected comprehensive rationalesfor at least a subset of the test set. Therefore, onthese datasets recall evaluates comprehensivenessdirectly (it does so only noisily on other datasets).6Consider that an extra token destroys the match but notusually the meaningWe highlight which corpora contain comprehensiverationales in the test set in Table 3.4.2Measuring faithfulnessAs discussed above, a model may provide ratio-nales that are plausible (agreeable to humans) butthat it did not rely on for its output. In many set-tings one may want rationales that actually explainmodel predictions, i.e., rationales extracted for aninstance in this case ought to have meaningfully in-ﬂuenced its prediction for the same. We call thesefaithful rationales. How best to measure rationalefaithfulness is an open question. In this ﬁrst versionof ERASER we propose simple metrics motivatedby prior work (Zaidan et al., 2007; Yu et al., 2019).In particular, following Yu et al. (2019) we deﬁnemetrics intended to measure the comprehensiveness(were all features needed to make a prediction se-lected?) and sufﬁciency (do the extracted rationalescontain enough signal to come to a disposition?) ofrationales, respectively.Comprehensiveness.To calculate rationalecomprehensiveness we create contrast exam-ples (Zaidan et al., 2007): We construct a con-trast example for xi, ˜xi, which is xi with the pre-dicted rationales ri removed. Assuming a classiﬁ-cation setting, let m(xi)j be the original predictionprovided by a model m for the predicted class j.Then we consider the predicted probability fromthe model for the same class once the supportingrationales are stripped. Intuitively, the model oughtto be less conﬁdent in its prediction once rationalesare removed from xi. We can measure this as:comprehensiveness = m(xi)j −m(xi/ri)j(1)A high score here implies that the rationales wereindeed inﬂuential in the prediction, while a lowscore suggests that they were not. A negative value\f4448Where do you ﬁnd the most amount of leafs?Where do you ﬁnd the most amount of leafs?(a) Compost pile (b) Flowers(c) Forest(d) Field(e) Ground(a) Compost pile (b) Flowers(c) Forest(d) Field(e) Ground……ˆp(Forest|xi)Where do you ﬁnd the most amount of leafs?(a) Compost pile (b) Flowers(c) Forest(d) Field(e) Ground…ComprehensivenessSuﬃency˜xixiriFigure 2: Illustration of faithfulness scoring metrics, comprehensiveness and sufﬁciency, on the CommonsenseExplanations (CoS-E) dataset. For the former, erasing the tokens comprising the provided rationale (˜xi) ought todecrease model conﬁdence in the output ‘Forest’. For the latter, the model should be able to come to a similardisposition regarding ‘Forest’ using only the rationales ri.here means that the model became more conﬁdentin its prediction after the rationales were removed;this would seem counter-intuitive if the rationaleswere indeed the reason for its prediction.Sufﬁciency. This captures the degree to whichthe snippets within the extracted rationales are ade-quate for a model to make a prediction.sufﬁciency = m(xi)j −m(ri)j(2)These metrics are illustrated in Figure 2.As deﬁned, the above measures have assumeddiscrete rationales ri. We would also like to eval-uate the faithfulness of continuous importancescores assigned to tokens by models. Here weadopt a simple approach for this. We convert softscores over features si provided by a model intodiscrete rationales ri by taking the top−kd values,where kd is a threshold for dataset d. We set kd tothe average rationale length provided by humansfor dataset d (see Table 4). Intuitively, this says:How much does the model prediction change if weremove a number of tokens equal to what humansuse (on average for this dataset) in order of theimportance scores assigned to these by the model.Once we have discretized the soft scores into ra-tionales in this way, we compute the faithfulnessscores as per Equations 1 and 2.This approach is conceptually simple. It is alsocomputationally cheap to evaluate, in contrast tomeasures that require per-token measurements, e.g.,importance score correlations with ‘leave-one-out’scores (Jain and Wallace, 2019), or counting howmany ‘important’ tokens need to be erased beforea prediction ﬂips (Serrano and Smith, 2019). How-ever, the necessity of discretizing continuous scoresforces us to pick a particular threshold k.We can also consider the behavior of these mea-sures as a function of k, inspired by the measure-ments proposed in Samek et al. (2016) in the con-text of evaluating saliency maps for image classi-ﬁcation. They suggested ranking pixel regions byimportance and then measuring the change in out-put as they are removed in rank order. Our datasetscomprise documents and rationales with quite dif-ferent lengths; to make this measure comparableacross datasets, we construct bins designating thenumber of tokens to be deleted. Denoting the to-kens up to and including bin k for instance i by rik,we deﬁne an aggregate comprehensiveness mea-sure:1∣B∣+ 1(∣B∣∑k=0m(xi)j −m(xi/rik)j)(3)This is deﬁned for sufﬁciency analogously. Herewe group tokens into k = 5 bins by grouping theminto the top 1%, 5%, 10%, 20% and 50% of to-kens, with respect to the corresponding importancescore. We refer to these metrics as “Area Over thePerturbation Curve” (AOPC).7These AOPC sufﬁciency and comprehensivenessmeasures score a particular token ordering undera model. As a point of reference, we also reportthese when random scores are assigned to tokens.7Our AOPC metrics are similar in concept to ROAR(Hooker et al., 2019) except that we re-use an existing modelas opposed to retraining for each fraction.\f44495Baseline ModelsOur focus in this work is primarily on the ERASERbenchmark itself, rather than on any particularmodel(s). But to establish a starting point for futurework, we evaluate several baseline models acrossthe corpora in ERASER.8 We broadly classify theseinto models that assign ‘soft’ (continuous) scoresto tokens, and those that perform a ‘hard’ (discrete)selection over inputs. We additionally considermodels speciﬁcally designed to select individualtokens (and very short sequences) as rationales, ascompared to longer snippets. All of our implemen-tations are in PyTorch (Paszke et al., 2019) and areavailable in the ERASER repository.9All datasets in ERASER comprise inputs, ratio-nales, and labels. But they differ considerably indocument and rationale lengths (Table A). This mo-tivated use of different models for datasets, appro-priate to their sizes and rationale granularities. Wehope that this benchmark motivates design of mod-els that provide rationales that can ﬂexibly adapt tovarying input lengths and expected rationale gran-ularities. Indeed, only with such models can weperform comparisons across all datasets.5.1Hard selectionModels that perform hard selection may be viewedas comprising two independent modules: an en-coder which is responsible for extracting snippetsof inputs, and a decoder that makes a predictionbased only on the text provided by the encoder. Weconsider two variants of such models.Lei et al. (2016). In this model, an encoder in-duces a binary mask over inputs x, z. The decoderaccepts the tokens in x unmasked by z to make aprediction ˆy. These modules are trained jointly viaREINFORCE (Williams, 1992) style estimation,minimizing the loss over expected binary vectorsz yielded from the encoder. One of the advantagesof this approach is that it need not have access tomarked rationales; it can learn to rationalize on thebasis of instance labels alone. However, given thatwe do have rationales in the training data, we exper-iment with a variant in which we train the encoderexplicitly using rationale-level annotations.In our implementation of Lei et al. (2016), wedrop in two independent BERT (Devlin et al., 2019)or GloVe (Pennington et al., 2014) base modules8This is not intended to be comprehensive.9https://github.com/jayded/eraserbenchmarkwith bidirectional LSTMs (Hochreiter and Schmid-huber, 1997) on top to induce contextualized rep-resentations of tokens for the encoder and decoder,respectively. The encoder generates a scalar (de-noting the probability of selecting that token) foreach LSTM hidden state using a feedfoward layerand sigmoid. In the variant using human rationalesduring training, we minimize cross entropy lossover rationale predictions. The ﬁnal loss is thena composite of classiﬁcation loss, regularizers onrationales (Lei et al., 2016), and loss over rationalepredictions, when available.Pipeline models.These are simple models inwhich we ﬁrst train the encoder to extract ratio-nales, and then train the decoder to perform pre-diction using only rationales. No parameters areshared between the two models.Here we ﬁrst consider a simple pipeline that ﬁrstsegments inputs into sentences. It passes these,one at a time, through a Gated Recurrent Unit(GRU) (Cho et al., 2014), to yield hidden represen-tations that we compose via an attentive decodinglayer (Bahdanau et al., 2015). This aggregate rep-resentation is then passed to a classiﬁcation modulewhich predicts whether the corresponding sentenceis a rationale (or not). A second model, using effec-tively the same architecture but parameterized inde-pendently, consumes the outputs (rationales) fromthe ﬁrst to make predictions. This simple model isdescribed at length in prior work (Lehman et al.,2019). We further consider a ‘BERT-to-BERT’pipeline, where we replace each stage with a BERTmodule for prediction (Devlin et al., 2019).In pipeline models, we train each stage indepen-dently. The rationale identiﬁcation stage is trainedusing approximate sentence boundaries from oursource annotations, with randomly sampled neg-ative examples at each epoch. The classiﬁcationstage uses the same positive rationales as the iden-tiﬁcation stage, a type of teacher forcing (Williamsand Zipser, 1989) (details in Appendix C).5.2Soft selectionWe consider a model that passes tokens throughBERT (Devlin et al., 2019) to induce contextual-ized representations that are then passed to a bi-directional LSTM (Hochreiter and Schmidhuber,1997). The hidden representations from the LSTMare collapsed into a single vector using additiveattention (Bahdanau et al., 2015). The LSTM layerallows us to bypass the 512 word limit imposed by\f4450Perf.IOU F1Token F1Evidence InferenceLei et al. (2016)0.4610.0000.000Lei et al. (2016) (u)0.4610.0000.000Lehman et al. (2019)0.4710.1190.123Bert-To-Bert0.7080.4550.468BoolQLei et al. (2016)0.3810.0000.000Lei et al. (2016) (u)0.3800.0000.000Lehman et al. (2019)0.4110.0500.127Bert-To-Bert0.5440.0520.134Movie ReviewsLei et al. (2016)0.9140.1240.285Lei et al. (2016) (u)0.9200.0120.322Lehman et al. (2019)0.7500.0630.139Bert-To-Bert0.8600.0750.145FEVERLei et al. (2016)0.7190.2180.234Lei et al. (2016) (u)0.7180.0000.000Lehman et al. (2019)0.6910.5400.523Bert-To-Bert0.8770.8350.812MultiRCLei et al. (2016)0.6550.2710.456Lei et al. (2016) (u)0.6480.000†0.000†Lehman et al. (2019)0.6140.1360.140Bert-To-Bert0.6330.4160.412CoS-ELei et al. (2016)0.4770.2550.331Lei et al. (2016) (u)0.4760.000†0.000†Bert-To-Bert0.3440.3890.519e-SNLILei et al. (2016)0.9170.6930.692Lei et al. (2016) (u)0.9030.2610.379Bert-To-Bert0.7330.7040.701Table 3: Performance of models that perform hard ra-tionale selection. All models are supervised at the ratio-nale level except for those marked with (u), which learnonly from instance-level supervision; † denotes cases inwhich rationale training degenerated due to the REIN-FORCE style training. Perf. is accuracy (CoS-E) ormacro-averaged F1 (others). Bert-To-Bert for CoS-Eand e-SNLI uses a token classiﬁcation objective. Bert-To-Bert CoS-E uses the highest scoring answer.BERT; when we exceed this, we effectively startencoding a ‘new’ sequence (setting the positionalindex to 0) via BERT. The hope is that the LSTMlearns to compensate for this. Evidence Inferenceand BoolQ comprise very long (>1000 token) in-puts; we were unable to run BERT over these. Weinstead resorted to swapping GloVe 300d embed-dings (Pennington et al., 2014) in place of BERTrepresentations for tokens. spans.To soft score features we consider: Simple gra-dients, attention induced over contextualized repre-sentations, and LIME (Ribeiro et al., 2016).Perf.AUPRCComp. ↑Suff. ↓Evidence InferenceGloVe + LSTM - Attention0.4290.506-0.002-0.023GloVe + LSTM - Gradient0.4290.0160.046-0.138GloVe + LSTM - Lime0.4290.0140.006-0.128GloVe + LSTM - Random0.4290.014-0.001-0.026BoolQGloVe + LSTM - Attention0.4710.5250.0100.022GloVe + LSTM - Gradient0.4710.0720.0240.031GloVe + LSTM - Lime0.4710.0730.028-0.154GloVe + LSTM - Random0.4710.0740.0000.005MoviesBERT+LSTM - Attention0.9700.4170.1290.097BERT+LSTM - Gradient0.9700.3850.1420.112BERT+LSTM - Lime0.9700.2800.1870.093BERT+LSTM - Random0.9700.2590.0580.330FEVERBERT+LSTM - Attention0.8700.2350.0370.122BERT+LSTM - Gradient0.8700.2320.0590.136BERT+LSTM - Lime0.8700.2910.2120.014BERT+LSTM - Random0.8700.2440.0340.122MultiRCBERT+LSTM - Attention0.6550.2440.0360.052BERT+LSTM - Gradient0.6550.2240.0770.064BERT+LSTM - Lime0.6550.2080.213-0.079BERT+LSTM - Random0.6550.1860.0290.081CoS-EBERT+LSTM - Attention0.4870.6060.0800.217BERT+LSTM - Gradient0.4870.5850.1240.226BERT+LSTM - Lime0.4870.5440.2230.143BERT+LSTM - Random0.4870.5940.0720.224e-SNLIBERT+LSTM - Attention0.9600.3950.1050.583BERT+LSTM - Gradient0.9600.4160.1800.472BERT+LSTM - Lime0.9600.5130.4370.389BERT+LSTM - Random0.9600.3570.0810.487Table 4: Metrics for ‘soft’ scoring models. Perf. is ac-curacy (CoS-E) or F1 (others). Comprehensiveness andsufﬁciency are in terms of AOPC (Eq. 3). ‘Random’assigns random scores to tokens to induce orderings;these are averages over 10 runs.6EvaluationHere we present initial results for the baseline mod-els discussed in Section 5, with respect to the met-rics proposed in Section 4. We present results intwo parts, reﬂecting the two classes of rationalesdiscussed above: ‘Hard’ approaches that performdiscrete selection of snippets, and ‘soft’ methodsthat assign continuous importance scores to tokens.In Table 3 we evaluate models that perform dis-crete selection of rationales. We view these as in-herently faithful, because by construction we knowwhich snippets the decoder used to make a pre-diction.10 Therefore, for these methods we reportonly metrics that measure agreement with humanannotations.10This assumes independent encoders and decoders.\f4451Due to computational constraints, we were un-able to run our BERT-based implementation of Leiet al. (2016) over larger corpora. Conversely, thesimple pipeline of Lehman et al. (2019) assumesa setting in which rationale are sentences, and sois not appropriate for datasets in which rationalestend to comprise only very short spans. Again, inour view this highlights the need for models thatcan rationalize at varying levels of granularity, de-pending on what is appropriate.We observe that for the “rationalizing” modelof Lei et al. (2016), exploiting rationale-level super-vision often (though not always) improves agree-ment with human-provided rationales, as in priorwork (Zhang et al., 2016; Strout et al., 2019). In-terestingly, this does not seem strongly correlatedwith predictive performance.Lei et al. (2016) outperforms the simple pipelinemodel when using a BERT encoder. Further, Leiet al. (2016) outperforms the ‘BERT-to-BERT’pipeline on the comparable datasets for the ﬁnalprediction tasks. This may be an artifact of theamount of text each model can select: ‘BERT-to-BERT’ is limited to sentences, while Lei et al.(2016) can select any subset of the text. Designingextraction models that learn to adaptively selectcontiguous rationales of appropriate length for agiven task seems a potentially promising direction.In Table 4 we report metrics for models thatassign continuous importance scores to individ-ual tokens. For these models we again measuredownstream (task) performance (macro F1 or ac-curacy). Here the models are actually the same,and so downstream performance is equivalent. Toassess the quality of token scores with respect tohuman annotations, we report the Area Under thePrecision Recall Curve (AUPRC).These scoring functions assign only soft scoresto inputs (and may still use all inputs to come toa particular prediction), so we report the metricsintended to measure faithfulness deﬁned above:comprehensiveness and sufﬁciency, averaged over‘bins’ of tokens ordered by importance scores. Toprovide a point of reference for these metrics —which depend on the underlying model — we re-port results when rationales are randomly selected(averaged over 10 runs).Both simple gradient and LIME-based scoringyield more comprehensive rationales than attentionweights, consistent with prior work (Jain and Wal-lace, 2019; Serrano and Smith, 2019). Attentionfares better in terms of AUPRC — suggesting bet-ter agreement with human rationales — which isalso in line with prior ﬁndings that it may provideplausible, but not faithful, explanation (Zhong et al.,2019). Interestingly, LIME does particularly wellacross these tasks in terms of faithfulness.From the ‘Random’ results that we concludemodels with overall poor performance on their ﬁ-nal tasks tend to have an overall poor ordering, withmarginal differences in comprehensiveness and suf-ﬁciency between them. For models that with highsufﬁciency scores: Movies, FEVER, CoS-E, and e-SNLI, we ﬁnd that random removal is particularlydamaging to performance, indicating poor absoluteranking; whereas those with high comprehensive-ness are sensitive to rationale length.7Conclusions and Future DirectionsWe have introduced a new publicly available re-source: the Evaluating Rationales And Simple En-glish Reasoning (ERASER) benchmark. This com-prises seven datasets, all of which include bothinstance level labels and corresponding supportingsnippets (‘rationales’) marked by human annotators.We have augmented many of these datasets withadditional annotations, and converted them into astandard format comprising inputs, rationales, andoutputs. ERASER is intended to facilitate progresson explainable models for NLP.We proposed several metrics intended to mea-sure the quality of rationales extracted by models,both in terms of agreement with human annota-tions, and in terms of ‘faithfulness’. We believethese metrics provide reasonable means of compar-ison of speciﬁc aspects of interpretability, but weview the problem of measuring faithfulness, in par-ticular, a topic ripe for additional research (whichERASER can facilitate).Our hope is that ERASER enables future workon designing more interpretable NLP models, andcomparing their relative strengths across a vari-ety of tasks, datasets, and desired criteria. It alsoserves as an ideal starting point for several futuredirections such as better evaluation metrics for in-terpretability, causal analysis of NLP models anddatasets of rationales in other languages.8AcknowledgementsWe thank the anonymous ACL reviewers.This work was supported in part by the NSF (CA-REER award 1750978), and by the Army ResearchOfﬁce (W911NF1810328).\f4452ReferencesDavid Alvarez-Melis and Tommi Jaakkola. 2017.Acausal framework for explaining the predictions ofblack-box sequence-to-sequence models.In Pro-ceedings of the 2017 Conference on Empirical Meth-ods in Natural Language Processing, pages 412–421.Leila Arras, Franziska Horn, Gr´egoire Montavon,Klaus-Robert M¨uller, and Wojciech Samek. 2017.”what is relevant in a text document?”: An inter-pretable machine learning approach. In PloS one.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. 2015.Neural machine translation by jointlylearning to align and translate.In 3rd Inter-national Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings.Joost Bastings, Wilker Aziz, and Ivan Titov. 2019. In-terpretable neural predictions with differentiable bi-nary variables. In Proceedings of the 57th AnnualMeeting of the Association for Computational Lin-guistics, pages 2963–2977, Florence, Italy. Associa-tion for Computational Linguistics.Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-ert: Pretrained language model for scientiﬁc text. InEMNLP.Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning. 2015. A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Association for Computational Linguistics.Gino Brunner, Yang Liu, Damian Pascual, OliverRichter, Massimiliano Ciaramita, and Roger Watten-hofer. 2020. On identiﬁability in transformers. InInternational Conference on Learning Representa-tions.Oana-Maria Camburu,Tim Rockt¨aschel,ThomasLukasiewicz, and Phil Blunsom. 2018. e-snli: Nat-ural language inference with natural language expla-nations. In Advances in Neural Information Process-ing Systems, pages 9539–9549.Shiyu Chang, Yang Zhang, Mo Yu, and TommiJaakkola. 2019. A game theoretic approach to class-wise selective rationalization. In Advances in Neu-ral Information Processing Systems, pages 10055–10065.Sihao Chen, Daniel Khashabi, Wenpeng Yin, ChrisCallison-Burch, and Dan Roth. 2019. Seeing thingsfrom a different angle: Discovering diverse perspec-tives about claims. In Proceedings of the Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL), pages 542–557, Minneapolis, Minnesota.Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio. 2014.Learningphrase representations using RNN encoder–decoderfor statistical machine translation. In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association for ComputationalLinguistics.Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifﬁculty of natural yes/no questions. In NAACL.Jacob Cohen. 1960.A coefﬁcient of agreement fornominal scales.Educational and PsychologicalMeasurement, 20(1):37–46.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019.BERT: Pre-training ofdeep bidirectional transformers for language under-standing.In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers),pages 4171–4186, Minneapolis, Minnesota. Associ-ation for Computational Linguistics.Yanzhuo Ding, Yang Liu, Huanbo Luan, and MaosongSun. 2017.Visualizing and understanding neuralmachine translation. In Proceedings of the 55th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), Vancouver,Canada. Association for Computational Linguistics.Finale Doshi-Velez and Been Kim. 2017. Towards arigorous science of interpretable machine learning.arXiv preprint arXiv:1702.08608.Mark Everingham, Luc Van Gool, Christopher K. I.Williams, John Winn, and Andrew Zisserman. 2010.The pascal visual object classes (voc) challenge. In-ternational Journal of Computer Vision, 88(2):303–338.Shi Feng, Eric Wallace, Alvin Grissom, Mohit Iyyer,Pedro Rodriguez, and Jordan L. Boyd-Graber. 2018.Pathologies of neural models make interpretationdifﬁcult. In EMNLP.Matt Gardner, Joel Grus, Mark Neumann, OyvindTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-ters, Michael Schmitz, and Luke Zettlemoyer. 2018.AllenNLP: A deep semantic natural language pro-cessing platform. In Proceedings of Workshop forNLP Open Source Software (NLP-OSS), pages 1–6, Melbourne, Australia. Association for Computa-tional Linguistics.Sepp Hochreiter and J¨urgen Schmidhuber. 1997.Long short-term memory.Neural computation,9(8):1735–1780.\f4453Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans,and Been Kim. 2019. A benchmark for interpretabil-ity methods in deep neural networks.In H. Wal-lach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,E. Fox, and R. Garnett, editors, Advances in Neu-ral Information Processing Systems 32, pages 9737–9748. Curran Associates, Inc.Alon Jacovi and Yoav Goldberg. 2020. Towards faith-fully interpretable nlp systems:How should wedeﬁne and evaluate faithfulness?arXiv preprintarXiv:2004.03685.Sarthak Jain and Byron C. Wallace. 2019. Attention isnot Explanation. In Proceedings of the 2019 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Volume 1 (Long and Short Pa-pers), pages 3543–3556, Minneapolis, Minnesota.Association for Computational Linguistics.Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and By-ron C. Wallace. 2020. Learning to Faithfully Ratio-nalize by Construction. In Proceedings of the Con-ference of the Association for Computational Lin-guistics (ACL).Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,Shyam Upadhyay, and Dan Roth. 2018.LookingBeyond the Surface: A Challenge Set for ReadingComprehension over Multiple Sentences. In Proc.of the Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL).Diederik Kingma and Jimmy Ba. 2014.Adam: Amethod for stochastic optimization.InternationalConference on Learning Representations.Eric Lehman, Jay DeYoung, Regina Barzilay, and By-ron C Wallace. 2019. Inferring which medical treat-ments work from reports of clinical trials. In Pro-ceedings of the North American Chapter of the As-sociation for Computational Linguistics (NAACL),pages 3705–3717.Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.Rationalizing neural predictions. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 107–117.Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.2016. Visualizing and understanding neural modelsin NLP. In Proceedings of the 2016 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 681–691, San Diego, California. As-sociation for Computational Linguistics.Zachary C Lipton. 2016. The mythos of model inter-pretability. arXiv preprint arXiv:1606.03490.Scott M Lundberg and Su-In Lee. 2017.A uniﬁedapproach to interpreting model predictions. In Ad-vances in Neural Information Processing Systems,pages 4765–4774.Tyler McDonnell, Mucahid Kutlu, Tamer Elsayed, andMatthew Lease. 2017. The many beneﬁts of anno-tator rationales for relevance judgments. In IJCAI,pages 4909–4913.Tyler McDonnell, Matthew Lease, Mucahid Kutlu, andTamer Elsayed. 2016.Why is that relevant? col-lecting annotator rationales for relevance judgments.In Fourth AAAI Conference on Human Computationand Crowdsourcing.Gr´egoire Montavon, Sebastian Lapuschkin, AlexanderBinder, Wojciech Samek, and Klaus-Robert M¨uller.2017. Explaining nonlinear classiﬁcation decisionswith deep taylor decomposition.Pattern Recogni-tion, 65:211–222.Pooya Moradi, Nishant Kambhatla, and Anoop Sarkar.2019. Interrogating the explanatory power of atten-tion in neural machine translation. In Proceedings ofthe 3rd Workshop on Neural Generation and Trans-lation, pages 221–230, Hong Kong. Association forComputational Linguistics.Mark Neumann, Daniel King, Iz Beltagy, and WaleedAmmar. 2019.Scispacy: Fast and robust modelsfor biomedical natural language processing. CoRR,abs/1902.07669.Dong Nguyen. 2018. Comparing automatic and humanevaluation of local explanations for text classiﬁca-tion. In Proceedings of the 2018 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, Volume 1 (Long Papers), pages 1069–1078.Bo Pang and Lillian Lee. 2004.A sentimental edu-cation: Sentiment analysis using subjectivity sum-marization based on minimum cuts.In Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics (ACL-04), pages 271–278, Barcelona, Spain.Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, et al. 2019. Pytorch: An imperative style,high-performance deep learning library.In Ad-vances in Neural Information Processing Systems,pages 8024–8035.David J Pearce. 2005. An improved algorithm for ﬁnd-ing the strongly connected components of a directedgraph. Technical report, Victoria University, NZ.Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. Glove: Global vectors for word rep-resentation. In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 1532–1543, Doha, Qatar. Asso-ciation for Computational Linguistics.Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Gra-ham Neubig, and Zachary C. Lipton. 2020. Learn-ing to deceive with attention-based explanations. InAnnual Conference of the Association for Computa-tional Linguistics (ACL).\f4454Sampo Pyysalo, F Ginter, Hans Moen, T Salakoski, andSophia Ananiadou. 2013. Distributional semanticsresources for biomedical text processing. Proceed-ings of Languages in Biology and Medicine.Alec Radford, Karthik Narasimhan, Tim Salimans, andIlya Sutskever. 2018.Improving language under-standing by generative pre-training.Nazneen Fatema Rajani, Bryan McCann, CaimingXiong, and Richard Socher. 2019. Explain yourself!leveraging language models for commonsense rea-soning. Proceedings of the Association for Compu-tational Linguistics (ACL).Marco Ribeiro, Sameer Singh, and Carlos Guestrin.2016. why should i trust you?: Explaining the pre-dictions of any classiﬁer. In Proceedings of the 2016Conference of the North American Chapter of theAssociation for Computational Linguistics: Demon-strations, pages 97–101.Wojciech Samek, Alexander Binder, Gr´egoire Mon-tavon, Sebastian Lapuschkin, and Klaus-RobertM¨uller. 2016. Evaluating the visualization of whata deep neural network has learned.IEEE trans-actions on neural networks and learning systems,28(11):2660–2673.Tal Schuster, Darsh J Shah, Yun Jie Serene Yeo, DanielFilizzola, Enrico Santus, and Regina Barzilay. 2019.Towards debiasing fact veriﬁcation models. In Pro-ceedings of the 2019 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP). As-sociation for Computational Linguistics.Soﬁa Serrano and Noah A. Smith. 2019. Is attentioninterpretable?In Proceedings of the 57th AnnualMeeting of the Association for Computational Lin-guistics, pages 2931–2951, Florence, Italy. Associa-tion for Computational Linguistics.Burr Settles. 2012.Active learning.Synthesis Lec-tures on Artiﬁcial Intelligence and Machine Learn-ing, 6(1):1–114.Manali Sharma, Di Zhuang, and Mustafa Bilgic. 2015.Active learning with rationales for text classiﬁcation.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 441–451.Kevin Small, Byron C Wallace, Carla E Brodley, andThomas A Trikalinos. 2011. The constrained weightspace svm: learning with ranked features. In Pro-ceedings of the International Conference on Inter-national Conference on Machine Learning (ICML),pages 865–872.D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wat-tenberg. 2017.SmoothGrad: removing noise byadding noise. ICML workshop on visualization fordeep learning.Robyn Speer. 2019. ftfy. Zenodo. Version 5.5.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov. 2014.Dropout: A simple way to prevent neural networksfrom overﬁtting. Journal of Machine Learning Re-search, 15:1929–1958.Julia Strout, Ye Zhang, and Raymond Mooney. 2019.Do human rationales improve machine explana-tions?In Proceedings of the 2019 ACL WorkshopBlackboxNLP: Analyzing and Interpreting NeuralNetworks for NLP, pages 56–62, Florence, Italy. As-sociation for Computational Linguistics.Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.Axiomatic attribution for deep networks.In Pro-ceedings of the 34th International Conference onMachine Learning-Volume 70, pages 3319–3328.JMLR. org.Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2019. CommonsenseQA: A ques-tion answering challenge targeting commonsenseknowledge. In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers),pages 4149–4158, Minneapolis, Minnesota. Associ-ation for Computational Linguistics.JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal.2018.FEVER: a Large-scale Dataset for Fact Extractionand VERiﬁcation.In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL), pages 809–819.Shikhar Vashishth, Shyam Upadhyay, Gaurav SinghTomar, and Manaal Faruqui. 2019.Attention in-terpretability across nlp tasks.arXiv preprintarXiv:1909.11218.Byron C Wallace, Kevin Small, Carla E Brodley, andThomas A Trikalinos. 2010.Active learning forbiomedical citation screening.In Proceedings ofthe 16th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 173–182. ACM.Alex Wang,Yada Pruksachatkun,Nikita Nangia,Amanpreet Singh, Julian Michael, Felix Hill, OmerLevy, and Samuel Bowman. 2019a. Superglue: Astickier benchmark for general-purpose language un-derstanding systems. In H. Wallach, H. Larochelle,A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-nett, editors, Advances in Neural Information Pro-cessing Systems 32, pages 3266–3280. Curran Asso-ciates, Inc.Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel R. Bowman. 2019b.GLUE: A multi-task benchmark and analysis plat-form for natural language understanding. In Inter-national Conference on Learning Representations.\f4455Sarah Wiegreffe and Yuval Pinter. 2019. Attention isnot not explanation. In Proceedings of the 2019 Con-ference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP), pages 11–20, Hong Kong, China. Associ-ation for Computational Linguistics.Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforce-ment learning. Machine learning, 8(3-4):229–256.Ronald J Williams and David Zipser. 1989. A learn-ing algorithm for continually running fully recurrentneural networks.Neural computation, 1(2):270–280.Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-icz, and Jamie Brew. 2019.Huggingface’s trans-formers: State-of-the-art natural language process-ing. ArXiv, abs/1910.03771.Mo Yu, Shiyu Chang, Yang Zhang, and TommiJaakkola. 2019. Rethinking cooperative rationaliza-tion: Introspective extraction and complement con-trol.In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages4094–4103, Hong Kong, China. Association forComputational Linguistics.Omar Zaidan, Jason Eisner, and Christine Piatko.2007.Using annotator rationales to improve ma-chine learning for text categorization. In Proceed-ings of the conference of the North American chap-ter of the Association for Computational Linguistics(NAACL), pages 260–267.Omar F Zaidan and Jason Eisner. 2008. Modeling an-notators: A generative approach to learning from an-notator rationales. In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 31–40.Ye Zhang, Iain Marshall, and Byron C Wallace. 2016.Rationale-augmented convolutional neural networksfor text classiﬁcation. In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), volume 2016, page 795. NIHPublic Access.Ruiqi Zhong, Steven Shao, and Kathleen McKeown.2019. Fine-grained sentiment analysis with faithfulattention. arXiv preprint arXiv:1908.06870.AppendixADataset PreprocessingWe describe what, if any, additional processing weperform on a per-dataset basis. All datasets wereconverted to a uniﬁed format.MultiRC (Khashabi et al., 2018) We perform min-imal processing. We use the validation set as thetesting set for public release.Evidence Inference (Lehman et al., 2019) We per-form minimal processing. As not all of the pro-vided evidence spans come with offsets, we deleteany prompts that had no grounded evidence spans.Movie reviews (Zaidan and Eisner, 2008) We per-form minimal processing. We use the ninth fold asthe validation set, and collect annotations on thetenth fold for comprehensive evaluation.FEVER (Thorne et al., 2018) We perform substan-tial processing for FEVER - we delete the ”NotEnough Info” claim class, delete any claims withsupport in more than one document, and reparti-tion the validation set into a validation and a testset for this benchmark (using the test set wouldcompromise the information retrieval portion ofthe original FEVER task). We ensure that thereis no document overlap between train, validation,and test sets (we use Pearce (2005) to ensure this,as conceptually a claim may be supported by factsin more than one document). We ensure that thevalidation set contains the documents used to cre-ate the FEVER symmetric dataset (Schuster et al.,2019) (unfortunately, the documents used to createthe validation and test sets overlap so we cannotprovide this partitioning). Additionally, we cleanup some encoding errors in the dataset via Speer(2019).BoolQ (Clark et al., 2019) The BoolQ dataset re-quired substantial processing. The original datasetdid not retain source Wikipedia articles or col-lection dates.In order to identify the sourceparagraphs, we download the 12/20/18 Wikipediaarchive, and use FuzzyWuzzy https://github.com/seatgeek/fuzzywuzzy to identify the sourceparagraph span that best matches the original re-lease. If the Levenshtein distance ratio does notreach a score of at least 90, the corresponding in-stance is removed. For public release, we use theofﬁcial validation set for testing, and repartitiontrain into a training and validation set.e-SNLI (Camburu et al., 2018) We perform mini-mal processing. We separate the premise and hy-pothesis statements into separate documents.Commonsense Explanations (CoS-E) (Rajaniet al., 2019) We perform minimal processing, pri-marily deletion of any questions without a rationale\f4456DatasetDocumentsInstancesRationale %Evidence StatementsEvidence LengthsMultiRCTrain4002402917.45629821.5Val56321418.5749822.8Test834848---Evidence InferenceTrain192479581.341037139.3Val2479721.38129440.3Test240959---Exhaustive Evidence InferenceVal811014.47504.035.2Test106152---Movie ReviewsTrain159916009.35138787.7Val1501507.451143.06.6Test200200---Exhaustive Movie ReviewsVal505019.10592.012.8FEVERTrain29159795720.014685631.3Val570612221.6867228.2Test6146111---BoolQTrain451863636.646363.0110.2Val109214917.131491.0106.5Test22942817---e-SNLITrain91193854930927.31199035.01.8Val16328982325.623639.01.6Test162999807---CoS-ETrain8733873326.687337.4Val1092109227.110927.6Test10921092---Table 5: Detailed breakdowns for each dataset - the number of documents, instances, evidence statements, andlengths. Additionally we include the percentage of each relevant document that is considered a rationale. For testsets, counts are for all instances including documents with non comprehensive rationales.DatasetLabelsInstancesDocumentsSentencesTokensEvidence Inference398892411156.04760.6BoolQ2106617026175.33582.5Movie Reviews22000199936.8774.1FEVER2110190409912.1326.5MultiRC23209153914.9302.5CoS-E510917109171.027.6e-SNLI35689399445651.716.0Table 6: General dataset statistics: number of labels, instances, unique documents, and average numbers of sen-tences and tokens in documents, across the publicly released train/validation/test splits in ERASER. For CoS-Eand e-SNLI, the sentence counts are not meaningful as the partitioning of question/sentence/answer formatting isan arbitrary choice in this framework.\f4457or questions with rationales that were not possi-ble to automatically map back to the underlyingtext. As recommended by the authors of Talmoret al. (2019) we repartition the train and validationsets into a train, validation, and test set for thisbenchmark. We encode the entire question and an-swers as a prompt and convert the problem into aﬁve-class prediction. We also convert the “Sanity”datasets for user convenience.All datasets in ERASER were tokenized usingspaCy11 library (with SciSpacy (Neumann et al.,2019) for Evidence Inference). In addition, we alsosplit all datasets except e-SNLI and CoS-E intosentences using the same library.BAnnotation detailsWe collected comprehensive rationales for a subsetof some test sets to accurately evaluate model recallof rationales.1. Movies. We used the Upwork Platform12 tohire two ﬂuent english speakers to annotateeach of the 200 documents in our test set.Workers were paid at rate of USD 8.5 per hourand on average, it took them 5 min to anno-tate a document. Each annotator was asked toannotate a set of 6 documents and comparedagainst in-house annotations (by authors).2. Evidence Inference. We again used Upworkto hire 4 medical professionals ﬂuent in en-glish and having passed a pilot of 3 documents.125 documents were annotated (only once byone of the annotators, which we felt was ap-propriate given their high-level of expertise)with an average cost of USD 13 per document.Average time spent of single document was31 min.3. BoolQ. We used Amazon Mechanical Turk(MTurk) to collect reference comprehensiverationales from randomly selected 199 docu-ments from our test set (ranging in 800 to 1500tokens in length). Only workers from AU, NZ,CA, US, GB with more than 10K approvedHITs and an approval rate of greater than 98%were eligible. For every document, 3 annota-tions were collected and workers were paidUSD 1.50 per HIT. The average work time(obtained through MTurk interface) was 21min. We did not anticipate the task taking so11https://spacy.io/12http://www.upwork.comlong (on average); the effective low pay ratewas unintended.CHyperparameter and training detailsC.1(Lei et al., 2016) modelsFor these models, we set the sparsity rate at 0.01and we set the contiguity loss weight to 2 timessparsity rate (following the original paper). Weused bert-base-uncased (Wolf et al., 2019) as to-ken embedder (for all datasets except BoolQ, Ev-idence Inference and FEVER) and BidirectionalLSTM with 128 dimensional hidden state in eachdirection. A dropout (Srivastava et al., 2014) rateof 0.2 was used before feeding the hidden repre-sentations to attention layer in decoder and linearlayer in encoder. One layer MLP with 128 dimen-sional hidden state and ReLU activation was usedto compute the decoder output distribution.For three datasets mentioned above, we useGloVe embeddings (http://nlp.stanford.edu/data/glove.840B.300d.zip).A learning rate of 2e-5 with Adam (Kingma andBa, 2014) optimizer was used for all models and weonly ﬁne-tuned top two layers of BERT encoder.Th models were trained for 20 epochs and earlystopping with patience of 5 epochs was used. Thebest model was selected on validation set using theﬁnal task performance metric.The input for the above model was encodedin form of [CLS] document [SEP] query[SEP].ThismodelwasimplementedusingtheAllenNLP library (Gardner et al., 2018).C.2BERT-LSTM/GloVe-LSTMThis model is essentially the same as the decoder inprevious section. The BERT-LSTM uses the samehyperparameters, and GloVe-LSTM is trained witha learning rate of 1e-2.C.3Lehman et al. (2019) modelsWith the exception of the Evidence Inferencedataset, these models were trained using the GLoVe(Pennington et al., 2014) 200 dimension word vec-tors, and Evidence Inference using the (Pyysaloet al., 2013) PubMed word vectors. We use Adam(Kingma and Ba, 2014) with a learning rate of1e-3, Dropout (Srivastava et al., 2014) of 0.05 ateach layer (embedding, GRU, attention layer) ofthe model, for 50 epochs with a patience of 10. Wemonitor validation loss, and keep the best modelon the validation set.\f4458C.4BERT-to-BERT modelWe primarily used the ‘bert-base-uncased‘ modelfor both components of the identiﬁcation and clas-siﬁcation pipeline, with the sole exception beingEvidence Inference with SciBERT (Beltagy et al.,2019). We trained with the standard BERT parame-ters of a learning rate of 1e-5, Adam (Kingma andBa, 2014), for 10 epochs. We monitor validationloss, and keep the best model on the validation set.
"""

splitted_text = text_splitter_recursive.split_text(result) # Splitting the text
text_store = FAISS.from_texts(
                        texts=splitted_text,
                        embedding=embeddings,
                        normalize_L2=True,
                        ) # Getting the FAISS vector store

scores = text_store.similarity_search_with_score(
                query=query, 
                k=20
            )
sim_threshold = 0.5
relevant_chunks = [
        document.page_content for document, score in scores if score >= sim_threshold
        ]
